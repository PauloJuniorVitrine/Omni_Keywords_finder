"""
üìä Metrics Collector System - Omni Keywords Finder

Sistema de coleta de m√©tricas avan√ßadas para monitoramento de qualidade dos testes.

Tracing ID: metrics-collector-system-2025-01-27-001
"""

import asyncio
import json
import logging
import time
from dataclasses import dataclass, field
from datetime import datetime, timedelta
from typing import Any, Dict, List, Optional, Tuple
from collections import defaultdict, Counter
import statistics

import aiohttp
import pytest
from dataclasses_json import dataclass_json

from tests.utils.risk_calculator import calculate_risk_score
from tests.utils.semantic_validator import SemanticValidator
from tests.decorators.critical_decorators import (
    critical_risk, 
    semantic_validation, 
    real_data_validation,
    production_scenario,
    side_effects_monitoring,
    performance_monitoring
)

# Configura√ß√£o de logging estruturado
logger = logging.getLogger(__name__)

@dataclass_json
@dataclass
class TestMetrics:
    """M√©tricas de um teste individual"""
    test_id: str
    test_name: str
    test_file: str
    test_class: Optional[str]
    execution_time: float
    risk_score: int
    semantic_similarity: float
    data_source: str  # "real", "synthetic", "mock"
    test_type: str  # "unit", "integration", "e2e"
    status: str  # "passed", "failed", "skipped"
    error_message: Optional[str] = None
    side_effects_covered: bool = True
    performance_impact: float = 0.0
    timestamp: float = field(default_factory=time.time)

@dataclass_json
@dataclass
class CoverageMetrics:
    """M√©tricas de cobertura por camada"""
    layer: str  # "domain", "gateway", "infrastructure"
    total_functions: int
    covered_functions: int
    coverage_percentage: float
    risk_score: int
    critical_functions: int
    critical_covered: int
    critical_coverage_percentage: float
    timestamp: float = field(default_factory=time.time)

@dataclass_json
@dataclass
class RiskMetrics:
    """M√©tricas de risco por fluxo"""
    flow_name: str
    total_tests: int
    avg_risk_score: float
    max_risk_score: int
    min_risk_score: int
    high_risk_tests: int  # RISK_SCORE >= 70
    medium_risk_tests: int  # RISK_SCORE 40-69
    low_risk_tests: int  # RISK_SCORE < 40
    risk_distribution: Dict[str, int] = field(default_factory=dict)
    timestamp: float = field(default_factory=time.time)

@dataclass_json
@dataclass
class MutationMetrics:
    """M√©tricas de mutation testing"""
    total_mutants: int
    killed_mutants: int
    survived_mutants: int
    mutation_score: float
    critical_mutants: int
    critical_killed: int
    critical_survived: int
    critical_mutation_score: float
    mutation_types: Dict[str, int] = field(default_factory=dict)
    timestamp: float = field(default_factory=time.time)

@dataclass_json
@dataclass
class SemanticMetrics:
    """M√©tricas de valida√ß√£o sem√¢ntica"""
    total_validations: int
    avg_similarity: float
    min_similarity: float
    max_similarity: float
    similarity_distribution: Dict[str, int] = field(default_factory=dict)
    false_positives: int
    false_negatives: int
    accuracy: float
    timestamp: float = field(default_factory=time.time)

@dataclass_json
@dataclass
class PerformanceMetrics:
    """M√©tricas de performance"""
    total_executions: int
    avg_execution_time: float
    max_execution_time: float
    min_execution_time: float
    slow_tests: int  # > 2x tempo m√©dio
    memory_usage: float
    cpu_usage: float
    network_requests: int
    avg_response_time: float
    timestamp: float = field(default_factory=time.time)

class MetricsCollector:
    """
    Coletor de m√©tricas avan√ßadas para monitoramento de qualidade
    
    Caracter√≠sticas:
    - Coleta de m√©tricas em tempo real
    - An√°lise de cobertura por camada
    - M√©tricas de risco por fluxo
    - M√©tricas de mutation testing
    - M√©tricas de valida√ß√£o sem√¢ntica
    - M√©tricas de performance
    """
    
    def __init__(self):
        """Inicializa o coletor de m√©tricas"""
        self.test_metrics: List[TestMetrics] = []
        self.coverage_metrics: List[CoverageMetrics] = []
        self.risk_metrics: List[RiskMetrics] = []
        self.mutation_metrics: List[MutationMetrics] = []
        self.semantic_metrics: List[SemanticMetrics] = []
        self.performance_metrics: List[PerformanceMetrics] = []
        
        self.semantic_validator = SemanticValidator()
        self.logger = logging.getLogger(f"{__name__}.MetricsCollector")
        
        # Contadores para m√©tricas agregadas
        self.test_counter = Counter()
        self.risk_counter = Counter()
        self.semantic_counter = Counter()
        self.performance_counter = Counter()
        
        # Hist√≥rico de m√©tricas
        self.metrics_history: Dict[str, List[Any]] = {
            'test_metrics': [],
            'coverage_metrics': [],
            'risk_metrics': [],
            'mutation_metrics': [],
            'semantic_metrics': [],
            'performance_metrics': []
        }
    
    def collect_test_metrics(self, 
                           test_id: str,
                           test_name: str,
                           test_file: str,
                           test_class: Optional[str],
                           execution_time: float,
                           risk_score: int,
                           semantic_similarity: float,
                           data_source: str,
                           test_type: str,
                           status: str,
                           error_message: Optional[str] = None,
                           side_effects_covered: bool = True,
                           performance_impact: float = 0.0) -> TestMetrics:
        """
        Coleta m√©tricas de um teste individual
        
        Args:
            test_id: ID √∫nico do teste
            test_name: Nome do teste
            test_file: Arquivo do teste
            test_class: Classe do teste (se aplic√°vel)
            execution_time: Tempo de execu√ß√£o
            risk_score: RISK_SCORE do teste
            semantic_similarity: Similaridade sem√¢ntica
            data_source: Origem dos dados
            test_type: Tipo do teste
            status: Status do teste
            error_message: Mensagem de erro (se houver)
            side_effects_covered: Se side effects foram cobertos
            performance_impact: Impacto na performance
            
        Returns:
            M√©tricas do teste
        """
        metrics = TestMetrics(
            test_id=test_id,
            test_name=test_name,
            test_file=test_file,
            test_class=test_class,
            execution_time=execution_time,
            risk_score=risk_score,
            semantic_similarity=semantic_similarity,
            data_source=data_source,
            test_type=test_type,
            status=status,
            error_message=error_message,
            side_effects_covered=side_effects_covered,
            performance_impact=performance_impact
        )
        
        self.test_metrics.append(metrics)
        self.test_counter[test_type] += 1
        self.test_counter[data_source] += 1
        self.test_counter[status] += 1
        
        # Log estruturado
        test_log = {
            "event": "test_metrics_collected",
            "test_id": test_id,
            "test_name": test_name,
            "risk_score": risk_score,
            "semantic_similarity": semantic_similarity,
            "execution_time": execution_time,
            "status": status,
            "timestamp": datetime.utcnow().isoformat()
        }
        
        self.logger.info(f"TEST_METRICS_LOG: {json.dumps(test_log)}")
        
        return metrics
    
    def collect_coverage_metrics(self, 
                               layer: str,
                               total_functions: int,
                               covered_functions: int,
                               critical_functions: int,
                               critical_covered: int) -> CoverageMetrics:
        """
        Coleta m√©tricas de cobertura por camada
        
        Args:
            layer: Camada do sistema
            total_functions: Total de fun√ß√µes
            covered_functions: Fun√ß√µes cobertas
            critical_functions: Fun√ß√µes cr√≠ticas
            critical_covered: Fun√ß√µes cr√≠ticas cobertas
            
        Returns:
            M√©tricas de cobertura
        """
        coverage_percentage = (covered_functions / max(total_functions, 1)) * 100
        critical_coverage_percentage = (critical_covered / max(critical_functions, 1)) * 100
        
        # Calcula RISK_SCORE baseado na cobertura
        risk_score = calculate_risk_score(
            component=f"coverage_{layer}",
            operation="coverage_analysis",
            data_sensitivity="medium",
            external_dependencies=0,
            error_rate=1.0 - (coverage_percentage / 100)
        )
        
        metrics = CoverageMetrics(
            layer=layer,
            total_functions=total_functions,
            covered_functions=covered_functions,
            coverage_percentage=coverage_percentage,
            risk_score=risk_score,
            critical_functions=critical_functions,
            critical_covered=critical_covered,
            critical_coverage_percentage=critical_coverage_percentage
        )
        
        self.coverage_metrics.append(metrics)
        
        # Log estruturado
        coverage_log = {
            "event": "coverage_metrics_collected",
            "layer": layer,
            "coverage_percentage": coverage_percentage,
            "critical_coverage_percentage": critical_coverage_percentage,
            "risk_score": risk_score,
            "timestamp": datetime.utcnow().isoformat()
        }
        
        self.logger.info(f"COVERAGE_METRICS_LOG: {json.dumps(coverage_log)}")
        
        return metrics
    
    def collect_risk_metrics(self, 
                           flow_name: str,
                           test_metrics: List[TestMetrics]) -> RiskMetrics:
        """
        Coleta m√©tricas de risco por fluxo
        
        Args:
            flow_name: Nome do fluxo
            test_metrics: Lista de m√©tricas de teste do fluxo
            
        Returns:
            M√©tricas de risco
        """
        if not test_metrics:
            return RiskMetrics(
                flow_name=flow_name,
                total_tests=0,
                avg_risk_score=0.0,
                max_risk_score=0,
                min_risk_score=0,
                high_risk_tests=0,
                medium_risk_tests=0,
                low_risk_tests=0
            )
        
        risk_scores = [t.risk_score for t in test_metrics]
        avg_risk_score = statistics.mean(risk_scores)
        max_risk_score = max(risk_scores)
        min_risk_score = min(risk_scores)
        
        high_risk_tests = len([r for r in risk_scores if r >= 70])
        medium_risk_tests = len([r for r in risk_scores if 40 <= r < 70])
        low_risk_tests = len([r for r in risk_scores if r < 40])
        
        risk_distribution = {
            "high": high_risk_tests,
            "medium": medium_risk_tests,
            "low": low_risk_tests
        }
        
        metrics = RiskMetrics(
            flow_name=flow_name,
            total_tests=len(test_metrics),
            avg_risk_score=avg_risk_score,
            max_risk_score=max_risk_score,
            min_risk_score=min_risk_score,
            high_risk_tests=high_risk_tests,
            medium_risk_tests=medium_risk_tests,
            low_risk_tests=low_risk_tests,
            risk_distribution=risk_distribution
        )
        
        self.risk_metrics.append(metrics)
        
        # Log estruturado
        risk_log = {
            "event": "risk_metrics_collected",
            "flow_name": flow_name,
            "total_tests": len(test_metrics),
            "avg_risk_score": avg_risk_score,
            "high_risk_tests": high_risk_tests,
            "timestamp": datetime.utcnow().isoformat()
        }
        
        self.logger.info(f"RISK_METRICS_LOG: {json.dumps(risk_log)}")
        
        return metrics
    
    def collect_mutation_metrics(self,
                               total_mutants: int,
                               killed_mutants: int,
                               critical_mutants: int,
                               critical_killed: int,
                               mutation_types: Dict[str, int]) -> MutationMetrics:
        """
        Coleta m√©tricas de mutation testing
        
        Args:
            total_mutants: Total de mutantes
            killed_mutants: Mutantes mortos
            critical_mutants: Mutantes cr√≠ticos
            critical_killed: Mutantes cr√≠ticos mortos
            mutation_types: Tipos de muta√ß√£o
            
        Returns:
            M√©tricas de mutation testing
        """
        survived_mutants = total_mutants - killed_mutants
        mutation_score = (killed_mutants / max(total_mutants, 1)) * 100
        
        critical_survived = critical_mutants - critical_killed
        critical_mutation_score = (critical_killed / max(critical_mutants, 1)) * 100
        
        metrics = MutationMetrics(
            total_mutants=total_mutants,
            killed_mutants=killed_mutants,
            survived_mutants=survived_mutants,
            mutation_score=mutation_score,
            critical_mutants=critical_mutants,
            critical_killed=critical_killed,
            critical_survived=critical_survived,
            critical_mutation_score=critical_mutation_score,
            mutation_types=mutation_types
        )
        
        self.mutation_metrics.append(metrics)
        
        # Log estruturado
        mutation_log = {
            "event": "mutation_metrics_collected",
            "total_mutants": total_mutants,
            "mutation_score": mutation_score,
            "critical_mutation_score": critical_mutation_score,
            "survived_mutants": survived_mutants,
            "timestamp": datetime.utcnow().isoformat()
        }
        
        self.logger.info(f"MUTATION_METRICS_LOG: {json.dumps(mutation_log)}")
        
        return metrics
    
    def collect_semantic_metrics(self,
                               total_validations: int,
                               similarity_scores: List[float],
                               false_positives: int,
                               false_negatives: int) -> SemanticMetrics:
        """
        Coleta m√©tricas de valida√ß√£o sem√¢ntica
        
        Args:
            total_validations: Total de valida√ß√µes
            similarity_scores: Scores de similaridade
            false_positives: Falsos positivos
            false_negatives: Falsos negativos
            
        Returns:
            M√©tricas de valida√ß√£o sem√¢ntica
        """
        if not similarity_scores:
            return SemanticMetrics(
                total_validations=total_validations,
                avg_similarity=0.0,
                min_similarity=0.0,
                max_similarity=0.0,
                false_positives=false_positives,
                false_negatives=false_negatives,
                accuracy=0.0
            )
        
        avg_similarity = statistics.mean(similarity_scores)
        min_similarity = min(similarity_scores)
        max_similarity = max(similarity_scores)
        
        # Distribui√ß√£o de similaridade
        similarity_distribution = {
            "0.9-1.0": len([s for s in similarity_scores if s >= 0.9]),
            "0.8-0.9": len([s for s in similarity_scores if 0.8 <= s < 0.9]),
            "0.7-0.8": len([s for s in similarity_scores if 0.7 <= s < 0.8]),
            "0.6-0.7": len([s for s in similarity_scores if 0.6 <= s < 0.7]),
            "<0.6": len([s for s in similarity_scores if s < 0.6])
        }
        
        # Calcula acur√°cia
        total_errors = false_positives + false_negatives
        accuracy = ((total_validations - total_errors) / max(total_validations, 1)) * 100
        
        metrics = SemanticMetrics(
            total_validations=total_validations,
            avg_similarity=avg_similarity,
            min_similarity=min_similarity,
            max_similarity=max_similarity,
            similarity_distribution=similarity_distribution,
            false_positives=false_positives,
            false_negatives=false_negatives,
            accuracy=accuracy
        )
        
        self.semantic_metrics.append(metrics)
        
        # Log estruturado
        semantic_log = {
            "event": "semantic_metrics_collected",
            "total_validations": total_validations,
            "avg_similarity": avg_similarity,
            "accuracy": accuracy,
            "false_positives": false_positives,
            "false_negatives": false_negatives,
            "timestamp": datetime.utcnow().isoformat()
        }
        
        self.logger.info(f"SEMANTIC_METRICS_LOG: {json.dumps(semantic_log)}")
        
        return metrics
    
    def collect_performance_metrics(self,
                                  total_executions: int,
                                  execution_times: List[float],
                                  memory_usage: float,
                                  cpu_usage: float,
                                  network_requests: int,
                                  response_times: List[float]) -> PerformanceMetrics:
        """
        Coleta m√©tricas de performance
        
        Args:
            total_executions: Total de execu√ß√µes
            execution_times: Tempos de execu√ß√£o
            memory_usage: Uso de mem√≥ria
            cpu_usage: Uso de CPU
            network_requests: Requisi√ß√µes de rede
            response_times: Tempos de resposta
            
        Returns:
            M√©tricas de performance
        """
        if not execution_times:
            return PerformanceMetrics(
                total_executions=total_executions,
                avg_execution_time=0.0,
                max_execution_time=0.0,
                min_execution_time=0.0,
                slow_tests=0,
                memory_usage=memory_usage,
                cpu_usage=cpu_usage,
                network_requests=network_requests,
                avg_response_time=0.0
            )
        
        avg_execution_time = statistics.mean(execution_times)
        max_execution_time = max(execution_times)
        min_execution_time = min(execution_times)
        
        # Testes lentos (> 2x tempo m√©dio)
        slow_threshold = avg_execution_time * 2
        slow_tests = len([t for t in execution_times if t > slow_threshold])
        
        avg_response_time = statistics.mean(response_times) if response_times else 0.0
        
        metrics = PerformanceMetrics(
            total_executions=total_executions,
            avg_execution_time=avg_execution_time,
            max_execution_time=max_execution_time,
            min_execution_time=min_execution_time,
            slow_tests=slow_tests,
            memory_usage=memory_usage,
            cpu_usage=cpu_usage,
            network_requests=network_requests,
            avg_response_time=avg_response_time
        )
        
        self.performance_metrics.append(metrics)
        
        # Log estruturado
        performance_log = {
            "event": "performance_metrics_collected",
            "total_executions": total_executions,
            "avg_execution_time": avg_execution_time,
            "slow_tests": slow_tests,
            "memory_usage": memory_usage,
            "cpu_usage": cpu_usage,
            "timestamp": datetime.utcnow().isoformat()
        }
        
        self.logger.info(f"PERFORMANCE_METRICS_LOG: {json.dumps(performance_log)}")
        
        return metrics
    
    def get_comprehensive_report(self) -> Dict[str, Any]:
        """Gera relat√≥rio abrangente de todas as m√©tricas"""
        return {
            "timestamp": datetime.utcnow().isoformat(),
            "summary": {
                "total_tests": len(self.test_metrics),
                "total_flows": len(set(m.flow_name for m in self.risk_metrics)),
                "total_layers": len(set(m.layer for m in self.coverage_metrics)),
                "avg_risk_score": statistics.mean([t.risk_score for t in self.test_metrics]) if self.test_metrics else 0.0,
                "avg_semantic_similarity": statistics.mean([t.semantic_similarity for t in self.test_metrics]) if self.test_metrics else 0.0,
                "avg_execution_time": statistics.mean([t.execution_time for t in self.test_metrics]) if self.test_metrics else 0.0
            },
            "test_metrics": {
                "by_type": dict(self.test_counter),
                "by_status": dict(Counter(t.status for t in self.test_metrics)),
                "by_data_source": dict(Counter(t.data_source for t in self.test_metrics)),
                "recent_tests": [
                    {
                        "test_id": t.test_id,
                        "test_name": t.test_name,
                        "risk_score": t.risk_score,
                        "semantic_similarity": t.semantic_similarity,
                        "execution_time": t.execution_time,
                        "status": t.status,
                        "timestamp": datetime.fromtimestamp(t.timestamp).isoformat()
                    }
                    for t in self.test_metrics[-10:]  # √öltimos 10 testes
                ]
            },
            "coverage_metrics": {
                "by_layer": [
                    {
                        "layer": c.layer,
                        "coverage_percentage": c.coverage_percentage,
                        "critical_coverage_percentage": c.critical_coverage_percentage,
                        "risk_score": c.risk_score,
                        "timestamp": datetime.fromtimestamp(c.timestamp).isoformat()
                    }
                    for c in self.coverage_metrics
                ]
            },
            "risk_metrics": {
                "by_flow": [
                    {
                        "flow_name": r.flow_name,
                        "total_tests": r.total_tests,
                        "avg_risk_score": r.avg_risk_score,
                        "high_risk_tests": r.high_risk_tests,
                        "risk_distribution": r.risk_distribution,
                        "timestamp": datetime.fromtimestamp(r.timestamp).isoformat()
                    }
                    for r in self.risk_metrics
                ]
            },
            "mutation_metrics": [
                {
                    "total_mutants": m.total_mutants,
                    "mutation_score": m.mutation_score,
                    "critical_mutation_score": m.critical_mutation_score,
                    "survived_mutants": m.survived_mutants,
                    "mutation_types": m.mutation_types,
                    "timestamp": datetime.fromtimestamp(m.timestamp).isoformat()
                }
                for m in self.mutation_metrics
            ],
            "semantic_metrics": [
                {
                    "total_validations": s.total_validations,
                    "avg_similarity": s.avg_similarity,
                    "accuracy": s.accuracy,
                    "false_positives": s.false_positives,
                    "false_negatives": s.false_negatives,
                    "similarity_distribution": s.similarity_distribution,
                    "timestamp": datetime.fromtimestamp(s.timestamp).isoformat()
                }
                for s in self.semantic_metrics
            ],
            "performance_metrics": [
                {
                    "total_executions": p.total_executions,
                    "avg_execution_time": p.avg_execution_time,
                    "slow_tests": p.slow_tests,
                    "memory_usage": p.memory_usage,
                    "cpu_usage": p.cpu_usage,
                    "network_requests": p.network_requests,
                    "avg_response_time": p.avg_response_time,
                    "timestamp": datetime.fromtimestamp(p.timestamp).isoformat()
                }
                for p in self.performance_metrics
            ]
        }

# Decorators para testes de m√©tricas
@critical_risk
@semantic_validation
@real_data_validation
@production_scenario
@side_effects_monitoring
@performance_monitoring
class TestMetricsCollector:
    """Testes de integra√ß√£o para o coletor de m√©tricas"""
    
    @pytest.fixture
    def metrics_collector(self):
        """Fixture para o coletor de m√©tricas"""
        return MetricsCollector()
    
    def test_test_metrics_collection(self, metrics_collector):
        """
        Testa coleta de m√©tricas de teste
        
        RISK_SCORE: 75 (Alto - Valida√ß√£o de m√©tricas)
        """
        # Coleta m√©tricas de teste
        metrics = metrics_collector.collect_test_metrics(
            test_id="test_001",
            test_name="test_instagram_search",
            test_file="tests/integration/test_instagram_real_integration.py",
            test_class="TestInstagramIntegration",
            execution_time=2.5,
            risk_score=85,
            semantic_similarity=0.95,
            data_source="real",
            test_type="integration",
            status="passed",
            side_effects_covered=True,
            performance_impact=0.1
        )
        
        # Valida√ß√µes
        assert metrics is not None
        assert metrics.test_id == "test_001"
        assert metrics.risk_score == 85
        assert metrics.semantic_similarity == 0.95
        assert metrics.data_source == "real"
        assert metrics.test_type == "integration"
        assert metrics.status == "passed"
        
        # Verifica contadores
        assert metrics_collector.test_counter["integration"] == 1
        assert metrics_collector.test_counter["real"] == 1
        assert metrics_collector.test_counter["passed"] == 1
        
        # Log estruturado
        logger.info(f"Test metrics collection test completed: {metrics.test_id}")
        logger.info(f"Risk score: {metrics.risk_score}")
        logger.info(f"Semantic similarity: {metrics.semantic_similarity}")
    
    def test_coverage_metrics_collection(self, metrics_collector):
        """
        Testa coleta de m√©tricas de cobertura
        
        RISK_SCORE: 70 (Alto - Valida√ß√£o de cobertura)
        """
        # Coleta m√©tricas de cobertura
        metrics = metrics_collector.collect_coverage_metrics(
            layer="domain",
            total_functions=100,
            covered_functions=95,
            critical_functions=20,
            critical_covered=20
        )
        
        # Valida√ß√µes
        assert metrics is not None
        assert metrics.layer == "domain"
        assert metrics.coverage_percentage == 95.0
        assert metrics.critical_coverage_percentage == 100.0
        assert metrics.risk_score >= 70
        
        # Log estruturado
        logger.info(f"Coverage metrics collection test completed: {metrics.layer}")
        logger.info(f"Coverage percentage: {metrics.coverage_percentage}%")
        logger.info(f"Critical coverage: {metrics.critical_coverage_percentage}%")
    
    def test_risk_metrics_collection(self, metrics_collector):
        """
        Testa coleta de m√©tricas de risco
        
        RISK_SCORE: 80 (Alto - An√°lise de risco)
        """
        # Cria m√©tricas de teste para o fluxo
        test_metrics = [
            TestMetrics(
                test_id="test_001",
                test_name="test_1",
                test_file="test.py",
                test_class=None,
                execution_time=1.0,
                risk_score=85,
                semantic_similarity=0.95,
                data_source="real",
                test_type="integration",
                status="passed"
            ),
            TestMetrics(
                test_id="test_002",
                test_name="test_2",
                test_file="test.py",
                test_class=None,
                execution_time=1.5,
                risk_score=75,
                semantic_similarity=0.92,
                data_source="real",
                test_type="integration",
                status="passed"
            )
        ]
        
        # Coleta m√©tricas de risco
        metrics = metrics_collector.collect_risk_metrics("instagram_flow", test_metrics)
        
        # Valida√ß√µes
        assert metrics is not None
        assert metrics.flow_name == "instagram_flow"
        assert metrics.total_tests == 2
        assert metrics.avg_risk_score == 80.0
        assert metrics.high_risk_tests == 2
        assert metrics.medium_risk_tests == 0
        assert metrics.low_risk_tests == 0
        
        # Log estruturado
        logger.info(f"Risk metrics collection test completed: {metrics.flow_name}")
        logger.info(f"Average risk score: {metrics.avg_risk_score}")
        logger.info(f"High risk tests: {metrics.high_risk_tests}")
    
    def test_comprehensive_report_generation(self, metrics_collector):
        """
        Testa gera√ß√£o de relat√≥rio abrangente
        
        RISK_SCORE: 70 (Alto - Relat√≥rios)
        """
        # Adiciona algumas m√©tricas para teste
        metrics_collector.collect_test_metrics(
            test_id="test_001",
            test_name="test_1",
            test_file="test.py",
            test_class=None,
            execution_time=1.0,
            risk_score=80,
            semantic_similarity=0.95,
            data_source="real",
            test_type="integration",
            status="passed"
        )
        
        metrics_collector.collect_coverage_metrics(
            layer="domain",
            total_functions=100,
            covered_functions=95,
            critical_functions=20,
            critical_covered=20
        )
        
        # Gera relat√≥rio
        report = metrics_collector.get_comprehensive_report()
        
        # Valida√ß√µes
        assert "timestamp" in report
        assert "summary" in report
        assert "test_metrics" in report
        assert "coverage_metrics" in report
        assert "risk_metrics" in report
        assert "mutation_metrics" in report
        assert "semantic_metrics" in report
        assert "performance_metrics" in report
        
        # Valida summary
        summary = report["summary"]
        assert "total_tests" in summary
        assert "avg_risk_score" in summary
        assert "avg_semantic_similarity" in summary
        
        # Log estruturado
        logger.info(f"Comprehensive report generation test completed")
        logger.info(f"Total tests: {summary['total_tests']}")
        logger.info(f"Average risk score: {summary['avg_risk_score']}")

if __name__ == "__main__":
    # Execu√ß√£o direta para testes
    pytest.main([__file__, "-v", "--tb=short"]) 