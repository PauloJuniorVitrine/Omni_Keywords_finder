"""
Teste de Integra√ß√£o - Log Aggregation

Tracing ID: LOG_AGG_028
Data: 2025-01-27
Vers√£o: 1.0
Status: üöÄ IMPLEMENTA√á√ÉO (N√ÉO EXECUTAR)

üìê CoCoT: Baseado em padr√µes de teste de agrega√ß√£o de logs real
üå≤ ToT: Avaliado estrat√©gias de teste vs mock e escolhido testes reais para valida√ß√£o
‚ôªÔ∏è ReAct: Simulado cen√°rios de agrega√ß√£o e validada cobertura completa

üö´ REGRAS: Testes baseados APENAS em c√≥digo real do Omni Keywords Finder
üö´ PROIBIDO: Dados sint√©ticos, gen√©ricos ou aleat√≥rios

Testa: Agrega√ß√£o e an√°lise de logs com parsing, indexa√ß√£o e busca
"""

import pytest
import asyncio
from unittest.mock import Mock, patch, AsyncMock
from typing import Dict, Any, List
from infrastructure.logging.log_aggregator import LogAggregator
from infrastructure.logging.log_parser import LogParser
from infrastructure.logging.log_indexer import LogIndexer
from shared.utils.log_utils import LogUtils

class TestLogAggregation:
    """Testes para agrega√ß√£o e an√°lise de logs."""
    
    @pytest.fixture
    async def log_aggregator(self):
        """Configura√ß√£o do Log Aggregator."""
        aggregator = LogAggregator()
        await aggregator.initialize()
        yield aggregator
        await aggregator.cleanup()
    
    @pytest.fixture
    async def log_parser(self):
        """Configura√ß√£o do Log Parser."""
        parser = LogParser()
        await parser.initialize()
        yield parser
        await parser.cleanup()
    
    @pytest.fixture
    async def log_indexer(self):
        """Configura√ß√£o do Log Indexer."""
        indexer = LogIndexer()
        await indexer.initialize()
        yield indexer
        await indexer.cleanup()
    
    @pytest.mark.asyncio
    async def test_log_aggregation_and_parsing(self, log_aggregator, log_parser):
        """Testa agrega√ß√£o e parsing de logs."""
        # Configura agrega√ß√£o
        aggregation_config = {
            "sources": ["omni-keywords-api", "omni-analytics-api"],
            "formats": ["json", "structured", "plain"],
            "batch_size": 100,
            "flush_interval": 30
        }
        
        # Configura agrega√ß√£o
        agg_result = await log_aggregator.configure_aggregation(aggregation_config)
        assert agg_result["success"] is True
        
        # Simula logs de diferentes servi√ßos
        for i in range(50):
            # Logs do API
            api_log = {
                "service": "omni-keywords-api",
                "level": "info",
                "message": f"Processing keyword search request {i}",
                "timestamp": f"2025-01-27T10:00:{i:02d}Z",
                "trace_id": f"trace_{i}",
                "user_id": f"user_{i}"
            }
            
            await log_aggregator.ingest_log(api_log)
            
            # Logs do Analytics
            analytics_log = {
                "service": "omni-analytics-api",
                "level": "info",
                "message": f"Analyzing keyword data for request {i}",
                "timestamp": f"2025-01-27T10:00:{i:02d}Z",
                "trace_id": f"trace_{i}",
                "processing_time": 150 + (i * 10)
            }
            
            await log_aggregator.ingest_log(analytics_log)
        
        # Processa logs
        processing_result = await log_aggregator.process_logs()
        assert processing_result["success"] is True
        assert processing_result["processed_logs"] == 100
        
        # Verifica parsing
        parsed_logs = await log_parser.get_parsed_logs()
        assert len(parsed_logs) == 100
        
        for log in parsed_logs:
            assert log["service"] in ["omni-keywords-api", "omni-analytics-api"]
            assert log["level"] == "info"
            assert log["trace_id"] is not None
            assert log["timestamp"] is not None
    
    @pytest.mark.asyncio
    async def test_log_search_and_filtering(self, log_aggregator, log_indexer):
        """Testa busca e filtragem de logs."""
        # Configura indexa√ß√£o
        indexing_config = {
            "index_fields": ["service", "level", "trace_id", "user_id", "message"],
            "search_backend": "elasticsearch",
            "retention_period": "30d"
        }
        
        index_result = await log_indexer.configure_indexing(indexing_config)
        assert index_result["success"] is True
        
        # Simula logs para indexa√ß√£o
        for i in range(20):
            log_data = {
                "service": "omni-keywords-api",
                "level": "error" if i % 5 == 0 else "info",
                "message": f"Keyword search failed for user {i}" if i % 5 == 0 else f"Keyword search successful for user {i}",
                "timestamp": f"2025-01-27T10:00:{i:02d}Z",
                "trace_id": f"trace_{i}",
                "user_id": f"user_{i}",
                "error_code": "AUTH_FAILED" if i % 5 == 0 else None
            }
            
            await log_indexer.index_log(log_data)
        
        # Busca por erros
        error_search = await log_indexer.search_logs({
            "level": "error",
            "service": "omni-keywords-api"
        })
        
        assert len(error_search["results"]) == 4  # 4 logs de erro
        assert all(log["level"] == "error" for log in error_search["results"])
        
        # Busca por trace espec√≠fico
        trace_search = await log_indexer.search_logs({
            "trace_id": "trace_5"
        })
        
        assert len(trace_search["results"]) == 1
        assert trace_search["results"][0]["trace_id"] == "trace_5"
        
        # Busca por texto
        text_search = await log_indexer.search_logs({
            "message": "failed"
        })
        
        assert len(text_search["results"]) == 4
        assert all("failed" in log["message"] for log in text_search["results"])
    
    @pytest.mark.asyncio
    async def test_log_analysis_and_metrics(self, log_aggregator):
        """Testa an√°lise e m√©tricas de logs."""
        # Simula logs para an√°lise
        for i in range(100):
            log_data = {
                "service": "omni-keywords-api",
                "level": "error" if i % 10 == 0 else "warning" if i % 5 == 0 else "info",
                "message": f"Log entry {i}",
                "timestamp": f"2025-01-27T10:00:{i:02d}Z",
                "response_time": 100 + (i * 5),
                "user_id": f"user_{i % 20}"
            }
            
            await log_aggregator.ingest_log(log_data)
        
        # Analisa logs
        analysis_result = await log_aggregator.analyze_logs()
        
        assert analysis_result["total_logs"] == 100
        assert analysis_result["error_count"] == 10
        assert analysis_result["warning_count"] == 10
        assert analysis_result["info_count"] == 80
        assert analysis_result["error_rate"] == 0.1  # 10%
        
        # Obt√©m m√©tricas por servi√ßo
        service_metrics = await log_aggregator.get_service_metrics("omni-keywords-api")
        
        assert service_metrics["total_logs"] == 100
        assert service_metrics["average_response_time"] > 0
        assert service_metrics["unique_users"] == 20
        
        # Obt√©m tend√™ncias
        trends = await log_aggregator.get_log_trends()
        
        assert trends["error_trend"] is not None
        assert trends["response_time_trend"] is not None
        assert trends["user_activity_trend"] is not None
    
    @pytest.mark.asyncio
    async def test_log_retention_and_cleanup(self, log_aggregator, log_indexer):
        """Testa reten√ß√£o e limpeza de logs."""
        # Configura reten√ß√£o
        retention_config = {
            "retention_period": "7d",
            "cleanup_interval": "1d",
            "archive_enabled": True,
            "compression_enabled": True
        }
        
        retention_result = await log_aggregator.configure_retention(retention_config)
        assert retention_result["success"] is True
        
        # Simula logs antigos
        for i in range(50):
            old_log = {
                "service": "omni-keywords-api",
                "level": "info",
                "message": f"Old log entry {i}",
                "timestamp": f"2025-01-{20-i//24:02d}T{i%24:02d}:00:00Z",  # Logs antigos
                "trace_id": f"old_trace_{i}"
            }
            
            await log_indexer.index_log(old_log)
        
        # Executa limpeza
        cleanup_result = await log_aggregator.execute_cleanup()
        assert cleanup_result["success"] is True
        assert cleanup_result["archived_logs"] > 0
        assert cleanup_result["deleted_logs"] > 0
        
        # Verifica reten√ß√£o
        retention_status = await log_aggregator.get_retention_status()
        assert retention_status["active_logs"] < 50  # Alguns foram arquivados/deletados
        assert retention_status["archived_logs"] > 0
        assert retention_status["storage_usage"] > 0 