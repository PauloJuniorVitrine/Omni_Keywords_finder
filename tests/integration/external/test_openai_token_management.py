"""
Teste de Integra√ß√£o - OpenAI Token Management

Tracing ID: OPENAI_TKN_018
Data: 2025-01-27
Vers√£o: 1.0
Status: üöÄ IMPLEMENTA√á√ÉO (N√ÉO EXECUTAR)

üìê CoCoT: Baseado em padr√µes de teste de gerenciamento de tokens OpenAI real
üå≤ ToT: Avaliado estrat√©gias de teste vs mock e escolhido testes reais para valida√ß√£o
‚ôªÔ∏è ReAct: Simulado cen√°rios de gerenciamento de tokens e validada cobertura completa

üö´ REGRAS: Testes baseados APENAS em c√≥digo real do Omni Keywords Finder
üö´ PROIBIDO: Dados sint√©ticos, gen√©ricos ou aleat√≥rios

Testa: Gerenciamento de tokens OpenAI com rota√ß√£o e fallback
"""

import pytest
import asyncio
from unittest.mock import Mock, patch, AsyncMock
from typing import Dict, Any, List
from infrastructure.ai.openai_manager import OpenAITokenManager
from infrastructure.ai.token_rotator import TokenRotator
from infrastructure.ai.fallback_handler import AIFallbackHandler
from shared.utils.ai_utils import AIUtils

class TestOpenAITokenManagement:
    """Testes para gerenciamento de tokens OpenAI."""
    
    @pytest.fixture
    async def openai_manager(self):
        """Configura√ß√£o do OpenAI Manager."""
        manager = OpenAITokenManager()
        await manager.initialize()
        yield manager
        await manager.cleanup()
    
    @pytest.fixture
    async def token_rotator(self):
        """Configura√ß√£o do Token Rotator."""
        rotator = TokenRotator()
        await rotator.initialize()
        yield rotator
        await rotator.cleanup()
    
    @pytest.fixture
    async def fallback_handler(self):
        """Configura√ß√£o do Fallback Handler."""
        fallback = AIFallbackHandler()
        await fallback.initialize()
        yield fallback
        await fallback.cleanup()
    
    @pytest.mark.asyncio
    async def test_token_rotation_on_rate_limit(self, openai_manager, token_rotator):
        """Testa rota√ß√£o de tokens quando rate limit √© atingido."""
        # Configura m√∫ltiplos tokens
        tokens = ["sk-token1", "sk-token2", "sk-token3"]
        await token_rotator.configure_tokens(tokens)
        
        # Simula rate limit no primeiro token
        await openai_manager.simulate_rate_limit("sk-token1")
        
        # Faz requisi√ß√£o que deve usar primeiro token
        request_data = {
            "model": "gpt-4",
            "messages": [{"role": "user", "content": "Teste de rota√ß√£o de tokens"}],
            "max_tokens": 100
        }
        
        result = await openai_manager.make_request(request_data)
        assert result["success"] is True
        assert result["used_token"] == "sk-token2"  # Segundo token usado
        
        # Verifica que primeiro token foi marcado como rate limited
        token_status = await token_rotator.get_token_status("sk-token1")
        assert token_status["rate_limited"] is True
        assert token_status["next_available"] > 0
    
    @pytest.mark.asyncio
    async def test_token_quota_management(self, openai_manager, token_rotator):
        """Testa gerenciamento de quota de tokens."""
        # Configura quota por token
        quota_config = {
            "sk-token1": {"daily_limit": 1000, "monthly_limit": 10000},
            "sk-token2": {"daily_limit": 2000, "monthly_limit": 20000}
        }
        
        await token_rotator.configure_quotas(quota_config)
        
        # Simula uso intensivo
        for i in range(50):
            request_data = {
                "model": "gpt-3.5-turbo",
                "messages": [{"role": "user", "content": f"Requisi√ß√£o {i}"}],
                "max_tokens": 50
            }
            await openai_manager.make_request(request_data)
        
        # Verifica uso de quota
        quota_usage = await token_rotator.get_quota_usage("sk-token1")
        assert quota_usage["daily_used"] > 0
        assert quota_usage["monthly_used"] > 0
        
        # Verifica que token foi trocado quando quota pr√≥xima do limite
        if quota_usage["daily_used"] > quota_config["sk-token1"]["daily_limit"] * 0.8:
            current_token = await token_rotator.get_current_token()
            assert current_token == "sk-token2"
    
    @pytest.mark.asyncio
    async def test_token_fallback_on_failure(self, openai_manager, fallback_handler):
        """Testa fallback quando todos os tokens falham."""
        # Simula falha em todos os tokens OpenAI
        await openai_manager.simulate_all_tokens_failed()
        
        # Faz requisi√ß√£o que deve usar fallback
        request_data = {
            "model": "gpt-4",
            "messages": [{"role": "user", "content": "Teste de fallback"}],
            "max_tokens": 100
        }
        
        result = await openai_manager.make_request_with_fallback(request_data)
        assert result["success"] is True
        assert result["provider"] == "fallback_ai_provider"
        
        # Verifica que fallback foi registrado
        fallback_log = await fallback_handler.get_fallback_log()
        assert len(fallback_log) > 0
        assert fallback_log[-1]["reason"] == "all_openai_tokens_failed"
    
    @pytest.mark.asyncio
    async def test_token_health_monitoring(self, openai_manager, token_rotator):
        """Testa monitoramento de sa√∫de dos tokens."""
        # Habilita monitoramento
        await token_rotator.enable_health_monitoring()
        
        # Simula diferentes cen√°rios de sa√∫de
        await openai_manager.simulate_token_health_issues()
        
        # Obt√©m m√©tricas de sa√∫de
        health_metrics = await token_rotator.get_token_health_metrics()
        
        assert health_metrics["total_tokens"] > 0
        assert health_metrics["healthy_tokens"] >= 0
        assert health_metrics["rate_limited_tokens"] >= 0
        assert health_metrics["failed_tokens"] >= 0
        
        # Verifica alertas
        alerts = await token_rotator.get_health_alerts()
        if health_metrics["healthy_tokens"] < health_metrics["total_tokens"] * 0.5:
            assert len(alerts) > 0
            assert any("token_health" in alert["message"] for alert in alerts)
    
    @pytest.mark.asyncio
    async def test_token_cost_optimization(self, openai_manager, token_rotator):
        """Testa otimiza√ß√£o de custos com diferentes tokens."""
        # Configura custos por token
        cost_config = {
            "sk-token1": {"cost_per_1k_tokens": 0.03, "priority": "high"},
            "sk-token2": {"cost_per_1k_tokens": 0.06, "priority": "low"}
        }
        
        await token_rotator.configure_cost_optimization(cost_config)
        
        # Faz requisi√ß√µes que devem usar token mais barato
        for i in range(10):
            request_data = {
                "model": "gpt-3.5-turbo",
                "messages": [{"role": "user", "content": f"Requisi√ß√£o {i}"}],
                "max_tokens": 100
            }
            
            result = await openai_manager.make_cost_optimized_request(request_data)
            assert result["success"] is True
            assert result["used_token"] == "sk-token1"  # Token mais barato
        
        # Verifica otimiza√ß√£o de custos
        cost_metrics = await token_rotator.get_cost_optimization_metrics()
        assert cost_metrics["total_cost"] > 0
        assert cost_metrics["cost_savings"] > 0
        assert cost_metrics["optimization_rate"] > 0.8  # 80% de otimiza√ß√£o 