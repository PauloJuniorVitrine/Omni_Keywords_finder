"""
üß™ Teste de Ambiente - Similar √† Produ√ß√£o

Tracing ID: environment-production-like-test-2025-01-27-001
Timestamp: 2025-01-27T22:30:00Z
Vers√£o: 1.0
Status: üöÄ IMPLEMENTA√á√ÉO

üìê CoCoT: Testes baseados em ambiente similar √† produ√ß√£o real do sistema Omni Keywords Finder
üå≤ ToT: Avaliadas m√∫ltiplas estrat√©gias de simula√ß√£o de produ√ß√£o (escala, configura√ß√£o, dados)
‚ôªÔ∏è ReAct: Simulado cen√°rios de produ√ß√£o e validada compatibilidade

Testa ambiente similar √† produ√ß√£o incluindo:
- Configura√ß√£o de produ√ß√£o
- Escala de produ√ß√£o
- Dados de produ√ß√£o
- Configura√ß√µes de alta disponibilidade
- Configura√ß√µes de backup
- Configura√ß√µes de monitoramento
- Configura√ß√µes de seguran√ßa
- Configura√ß√µes de performance
- Configura√ß√µes de disaster recovery
- Valida√ß√£o de integridade
"""

import pytest
import asyncio
import time
import json
import statistics
import requests
import psutil
import os
import sys
from datetime import datetime, timedelta
from typing import Dict, List, Any, Optional, Tuple
from unittest.mock import Mock, patch, AsyncMock
import logging
from dataclasses import dataclass
import subprocess
import platform
import socket
import ssl

# Importa√ß√µes do sistema real
from backend.app.config.settings import Settings
from backend.app.database.connection import DatabaseConnection
from infrastructure.cache.redis_cache import RedisCache
from infrastructure.queue.rabbitmq_queue import RabbitMQQueue
from infrastructure.monitoring.metrics_collector import MetricsCollector
from infrastructure.logging.structured_logger import StructuredLogger
from infrastructure.security.security_validator import SecurityValidator
from infrastructure.performance.performance_monitor import PerformanceMonitor
from infrastructure.backup.backup_service import BackupService
from infrastructure.high_availability.ha_manager import HighAvailabilityManager

# Configura√ß√£o de logging
logger = logging.getLogger(__name__)

@dataclass
class ProductionLikeEnvironmentConfig:
    """Configura√ß√£o para testes de ambiente similar √† produ√ß√£o"""
    production_like_url: str = "https://prod-like.omni-keywords-finder.com"
    production_like_api_url: str = "https://prod-like.omni-keywords-finder.com/api"
    database_url: str = "postgresql://prod_user:prod_pass@prod-like-db:5432/prod_like_db"
    redis_url: str = "redis://prod-like-redis:6379"
    rabbitmq_url: str = "amqp://prod_user:prod_pass@prod-like-rabbitmq:5672"
    max_response_time: float = 3.0  # 3 segundos (mais rigoroso que staging)
    max_memory_usage_mb: int = 4096  # 4GB
    max_cpu_usage_percent: float = 70.0
    enable_ha_checks: bool = True
    enable_backup_checks: bool = True
    enable_security_checks: bool = True
    enable_performance_checks: bool = True
    enable_connectivity_checks: bool = True
    enable_data_validation: bool = True
    enable_config_validation: bool = True
    test_timeout: int = 600  # 10 minutos
    production_data_size: int = 1000000  # 1M registros

class ProductionLikeEnvironmentTest:
    """Teste de ambiente similar √† produ√ß√£o"""
    
    def __init__(self, config: Optional[ProductionLikeEnvironmentConfig] = None):
        self.config = config or ProductionLikeEnvironmentConfig()
        self.logger = StructuredLogger(
            module="production_like_environment_test",
            tracing_id="prod_like_env_test_001"
        )
        self.metrics = MetricsCollector()
        self.performance_monitor = PerformanceMonitor()
        self.security_validator = SecurityValidator()
        self.ha_manager = HighAvailabilityManager()
        self.backup_service = BackupService()
        
        # Conex√µes
        self.db_connection = None
        self.redis_cache = None
        self.rabbitmq_queue = None
        
        # M√©tricas de teste
        self.environment_events: List[Dict[str, Any]] = []
        self.performance_metrics: List[Dict[str, float]] = []
        self.security_events: List[Dict[str, Any]] = []
        self.ha_events: List[Dict[str, Any]] = []
        
        logger.info(f"Production Like Environment Test inicializado com configura√ß√£o: {self.config}")
    
    async def setup_test_environment(self):
        """Configura ambiente de teste"""
        try:
            # Configurar monitor de performance
            self.performance_monitor.configure({
                "enable_metrics": True,
                "enable_monitoring": True,
                "memory_threshold_mb": self.config.max_memory_usage_mb,
                "cpu_threshold_percent": self.config.max_cpu_usage_percent
            })
            
            # Configurar validador de seguran√ßa
            self.security_validator.configure({
                "enable_ssl_validation": True,
                "enable_security_headers": True,
                "enable_rate_limiting": True,
                "enable_waf_validation": True
            })
            
            # Configurar gerenciador de alta disponibilidade
            self.ha_manager.configure({
                "enable_ha_checks": self.config.enable_ha_checks,
                "enable_failover_tests": True,
                "enable_load_balancing_tests": True
            })
            
            # Configurar servi√ßo de backup
            self.backup_service.configure({
                "enable_backup_checks": self.config.enable_backup_checks,
                "enable_restore_tests": True,
                "backup_retention_days": 30
            })
            
            logger.info("Ambiente de teste configurado com sucesso")
            
        except Exception as e:
            logger.error(f"Erro ao configurar ambiente de teste: {e}")
            raise
    
    async def test_production_like_connectivity(self):
        """Testa conectividade com ambiente similar √† produ√ß√£o"""
        try:
            start_time = time.time()
            
            # Testar conectividade HTTP
            response = requests.get(
                f"{self.config.production_like_url}/health",
                timeout=self.config.max_response_time
            )
            
            connectivity_time = time.time() - start_time
            
            # Validar resposta (mais rigoroso que staging)
            assert response.status_code == 200, f"Status code inv√°lido: {response.status_code}"
            assert connectivity_time < self.config.max_response_time, f"Tempo de resposta muito alto: {connectivity_time}s"
            
            # Verificar headers de seguran√ßa
            security_headers = self._validate_production_security_headers(response.headers)
            
            # Verificar conte√∫do da resposta
            health_data = response.json()
            assert "status" in health_data, "Resposta de health check inv√°lida"
            assert health_data["status"] == "healthy", f"Status de health inv√°lido: {health_data['status']}"
            
            # Verificar m√©tricas de produ√ß√£o
            if "metrics" in health_data:
                metrics = health_data["metrics"]
                assert metrics.get("uptime", 0) > 0, "Uptime deve ser maior que 0"
                assert metrics.get("version") is not None, "Vers√£o deve estar presente"
            
            self.environment_events.append({
                "test_type": "production_like_connectivity",
                "response_time": connectivity_time,
                "status_code": response.status_code,
                "security_headers": security_headers,
                "health_status": health_data["status"],
                "uptime": health_data.get("metrics", {}).get("uptime", 0)
            })
            
            logger.info(f"Conectividade com produ√ß√£o-like testada: {connectivity_time:.3f}s, status {response.status_code}")
            
            return {
                "success": True,
                "response_time": connectivity_time,
                "status_code": response.status_code,
                "security_headers": security_headers,
                "health_status": health_data["status"],
                "uptime": health_data.get("metrics", {}).get("uptime", 0)
            }
            
        except Exception as e:
            logger.error(f"Erro no teste de conectividade com produ√ß√£o-like: {e}")
            raise
    
    async def test_production_like_api_performance(self):
        """Testa performance da API similar √† produ√ß√£o"""
        try:
            endpoints = [
                "/api/v1/health",
                "/api/v1/status",
                "/api/v1/version",
                "/api/v1/config",
                "/api/v1/metrics",
                "/api/v1/keywords/search",
                "/api/v1/analytics/overview",
                "/api/v1/reports/generate"
            ]
            
            results = []
            performance_metrics = []
            
            for endpoint in endpoints:
                # Testar m√∫ltiplas requisi√ß√µes para cada endpoint
                endpoint_times = []
                
                for i in range(5):  # 5 requisi√ß√µes por endpoint
                    start_time = time.time()
                    
                    try:
                        response = requests.get(
                            f"{self.config.production_like_api_url}{endpoint}",
                            timeout=self.config.max_response_time
                        )
                        
                        response_time = time.time() - start_time
                        endpoint_times.append(response_time)
                        
                        # Validar resposta
                        assert response.status_code in [200, 401, 403], f"Status code inesperado para {endpoint}: {response.status_code}"
                        assert response_time < self.config.max_response_time, f"Tempo de resposta muito alto para {endpoint}: {response_time}s"
                        
                    except Exception as e:
                        endpoint_times.append(self.config.max_response_time)
                
                # Calcular m√©tricas do endpoint
                avg_time = statistics.mean(endpoint_times)
                min_time = min(endpoint_times)
                max_time = max(endpoint_times)
                p95_time = statistics.quantiles(endpoint_times, n=20)[18] if len(endpoint_times) >= 20 else max_time
                
                results.append({
                    "endpoint": endpoint,
                    "avg_response_time": avg_time,
                    "min_response_time": min_time,
                    "max_response_time": max_time,
                    "p95_response_time": p95_time,
                    "success_rate": len([t for t in endpoint_times if t < self.config.max_response_time]) / len(endpoint_times)
                })
                
                performance_metrics.append({
                    "endpoint": endpoint,
                    "avg_time": avg_time,
                    "p95_time": p95_time
                })
            
            # Calcular m√©tricas gerais
            all_times = [r["avg_response_time"] for r in results]
            overall_avg = statistics.mean(all_times)
            overall_p95 = statistics.quantiles(all_times, n=20)[18] if len(all_times) >= 20 else max(all_times)
            
            # Verificar performance de produ√ß√£o
            assert overall_avg < self.config.max_response_time, f"Tempo m√©dio de resposta muito alto: {overall_avg}s"
            assert overall_p95 < self.config.max_response_time * 1.5, f"P95 de resposta muito alto: {overall_p95}s"
            
            success_rate = statistics.mean([r["success_rate"] for r in results])
            assert success_rate > 0.95, f"Taxa de sucesso muito baixa: {success_rate:.2%}"
            
            self.environment_events.append({
                "test_type": "production_like_api_performance",
                "total_endpoints": len(endpoints),
                "overall_avg_response_time": overall_avg,
                "overall_p95_response_time": overall_p95,
                "success_rate": success_rate,
                "results": results
            })
            
            logger.info(f"Performance da API produ√ß√£o-like testada: {overall_avg:.3f}s m√©dio, {success_rate:.1%} sucesso")
            
            return {
                "success": True,
                "total_endpoints": len(endpoints),
                "overall_avg_response_time": overall_avg,
                "overall_p95_response_time": overall_p95,
                "success_rate": success_rate,
                "results": results
            }
            
        except Exception as e:
            logger.error(f"Erro no teste de performance da API produ√ß√£o-like: {e}")
            raise
    
    async def test_production_like_database_performance(self):
        """Testa performance do banco de dados similar √† produ√ß√£o"""
        try:
            start_time = time.time()
            
            # Conectar ao banco
            self.db_connection = DatabaseConnection(self.config.database_url)
            await self.db_connection.connect()
            
            connection_time = time.time() - start_time
            
            # Testar queries de produ√ß√£o
            queries = [
                ("SELECT COUNT(*) FROM keywords", "count_keywords"),
                ("SELECT COUNT(*) FROM execucoes", "count_execucoes"),
                ("SELECT COUNT(*) FROM categorias", "count_categorias"),
                ("SELECT * FROM keywords LIMIT 100", "select_keywords"),
                ("SELECT * FROM execucoes WHERE status = 'completed' LIMIT 50", "select_completed_execucoes"),
                ("SELECT c.nome, COUNT(k.id) FROM categorias c LEFT JOIN keywords k ON c.id = k.categoria_id GROUP BY c.id", "join_query")
            ]
            
            query_results = []
            
            for query, query_name in queries:
                query_start = time.time()
                
                try:
                    result = await self.db_connection.execute_query(query)
                    query_time = time.time() - query_start
                    
                    query_results.append({
                        "query_name": query_name,
                        "query_time": query_time,
                        "result_count": len(result) if result else 0,
                        "success": True
                    })
                    
                    # Verificar performance
                    assert query_time < 5.0, f"Query {query_name} muito lenta: {query_time}s"
                    
                except Exception as e:
                    query_results.append({
                        "query_name": query_name,
                        "query_time": 0,
                        "result_count": 0,
                        "success": False,
                        "error": str(e)
                    })
            
            # Calcular m√©tricas
            successful_queries = [r for r in query_results if r["success"]]
            avg_query_time = statistics.mean([r["query_time"] for r in successful_queries]) if successful_queries else 0
            max_query_time = max([r["query_time"] for r in successful_queries]) if successful_queries else 0
            
            # Verificar performance
            assert len(successful_queries) > 0, "Nenhuma query foi executada com sucesso"
            assert avg_query_time < 2.0, f"Tempo m√©dio de query muito alto: {avg_query_time}s"
            assert max_query_time < 5.0, f"Tempo m√°ximo de query muito alto: {max_query_time}s"
            
            self.environment_events.append({
                "test_type": "production_like_database_performance",
                "connection_time": connection_time,
                "total_queries": len(queries),
                "successful_queries": len(successful_queries),
                "avg_query_time": avg_query_time,
                "max_query_time": max_query_time,
                "query_results": query_results
            })
            
            logger.info(f"Performance do banco produ√ß√£o-like testada: {avg_query_time:.3f}s m√©dio, {len(successful_queries)}/{len(queries)} sucessos")
            
            return {
                "success": True,
                "connection_time": connection_time,
                "total_queries": len(queries),
                "successful_queries": len(successful_queries),
                "avg_query_time": avg_query_time,
                "max_query_time": max_query_time
            }
            
        except Exception as e:
            logger.error(f"Erro no teste de performance do banco produ√ß√£o-like: {e}")
            raise
    
    async def test_production_like_high_availability(self):
        """Testa alta disponibilidade do ambiente similar √† produ√ß√£o"""
        try:
            # Testar failover
            ha_start = time.time()
            failover_result = await self.ha_manager.test_failover()
            ha_time = time.time() - ha_start
            
            # Testar load balancing
            lb_start = time.time()
            lb_result = await self.ha_manager.test_load_balancing()
            lb_time = time.time() - lb_start
            
            # Testar health checks
            hc_start = time.time()
            hc_result = await self.ha_manager.test_health_checks()
            hc_time = time.time() - hc_start
            
            # Validar resultados
            assert failover_result["success"], f"Teste de failover falhou: {failover_result.get('error')}"
            assert lb_result["success"], f"Teste de load balancing falhou: {lb_result.get('error')}"
            assert hc_result["success"], f"Teste de health checks falhou: {hc_result.get('error')}"
            
            # Verificar performance
            assert ha_time < 30.0, f"Tempo de teste de failover muito alto: {ha_time}s"
            assert lb_time < 10.0, f"Tempo de teste de load balancing muito alto: {lb_time}s"
            assert hc_time < 5.0, f"Tempo de teste de health checks muito alto: {hc_time}s"
            
            self.ha_events.append({
                "test_type": "production_like_high_availability",
                "failover_test": failover_result,
                "load_balancing_test": lb_result,
                "health_checks_test": hc_result,
                "total_time": ha_time + lb_time + hc_time
            })
            
            logger.info(f"Alta disponibilidade produ√ß√£o-like testada: failover {ha_time:.3f}s, LB {lb_time:.3f}s")
            
            return {
                "success": True,
                "failover_test": failover_result,
                "load_balancing_test": lb_result,
                "health_checks_test": hc_result,
                "total_time": ha_time + lb_time + hc_time
            }
            
        except Exception as e:
            logger.error(f"Erro no teste de alta disponibilidade produ√ß√£o-like: {e}")
            raise
    
    async def test_production_like_backup_restore(self):
        """Testa backup e restore do ambiente similar √† produ√ß√£o"""
        try:
            # Testar backup
            backup_start = time.time()
            backup_result = await self.backup_service.test_backup()
            backup_time = time.time() - backup_start
            
            # Testar restore
            restore_start = time.time()
            restore_result = await self.backup_service.test_restore()
            restore_time = time.time() - restore_start
            
            # Testar valida√ß√£o de backup
            validation_start = time.time()
            validation_result = await self.backup_service.validate_backup()
            validation_time = time.time() - validation_start
            
            # Validar resultados
            assert backup_result["success"], f"Teste de backup falhou: {backup_result.get('error')}"
            assert restore_result["success"], f"Teste de restore falhou: {restore_result.get('error')}"
            assert validation_result["success"], f"Valida√ß√£o de backup falhou: {validation_result.get('error')}"
            
            # Verificar performance
            assert backup_time < 300.0, f"Tempo de backup muito alto: {backup_time}s"
            assert restore_time < 600.0, f"Tempo de restore muito alto: {restore_time}s"
            assert validation_time < 60.0, f"Tempo de valida√ß√£o muito alto: {validation_time}s"
            
            self.environment_events.append({
                "test_type": "production_like_backup_restore",
                "backup_test": backup_result,
                "restore_test": restore_result,
                "validation_test": validation_result,
                "backup_time": backup_time,
                "restore_time": restore_time,
                "validation_time": validation_time
            })
            
            logger.info(f"Backup/Restore produ√ß√£o-like testado: backup {backup_time:.3f}s, restore {restore_time:.3f}s")
            
            return {
                "success": True,
                "backup_test": backup_result,
                "restore_test": restore_result,
                "validation_test": validation_result,
                "backup_time": backup_time,
                "restore_time": restore_time,
                "validation_time": validation_time
            }
            
        except Exception as e:
            logger.error(f"Erro no teste de backup/restore produ√ß√£o-like: {e}")
            raise
    
    async def test_production_like_security(self):
        """Testa seguran√ßa do ambiente similar √† produ√ß√£o"""
        try:
            # Testar SSL/TLS
            ssl_start = time.time()
            ssl_result = await self.security_validator.test_ssl_configuration(self.config.production_like_url)
            ssl_time = time.time() - ssl_start
            
            # Testar headers de seguran√ßa
            headers_start = time.time()
            headers_result = await self.security_validator.test_security_headers(self.config.production_like_url)
            headers_time = time.time() - headers_start
            
            # Testar rate limiting
            rate_start = time.time()
            rate_result = await self.security_validator.test_rate_limiting(self.config.production_like_api_url)
            rate_time = time.time() - rate_start
            
            # Testar WAF
            waf_start = time.time()
            waf_result = await self.security_validator.test_waf_protection(self.config.production_like_url)
            waf_time = time.time() - waf_start
            
            # Validar resultados
            assert ssl_result["success"], f"Teste de SSL falhou: {ssl_result.get('error')}"
            assert headers_result["success"], f"Teste de headers de seguran√ßa falhou: {headers_result.get('error')}"
            assert rate_result["success"], f"Teste de rate limiting falhou: {rate_result.get('error')}"
            assert waf_result["success"], f"Teste de WAF falhou: {waf_result.get('error')}"
            
            self.security_events.append({
                "test_type": "production_like_security",
                "ssl_test": ssl_result,
                "headers_test": headers_result,
                "rate_limiting_test": rate_result,
                "waf_test": waf_result,
                "total_time": ssl_time + headers_time + rate_time + waf_time
            })
            
            logger.info(f"Seguran√ßa produ√ß√£o-like testada: SSL {ssl_time:.3f}s, WAF {waf_time:.3f}s")
            
            return {
                "success": True,
                "ssl_test": ssl_result,
                "headers_test": headers_result,
                "rate_limiting_test": rate_result,
                "waf_test": waf_result,
                "total_time": ssl_time + headers_time + rate_time + waf_time
            }
            
        except Exception as e:
            logger.error(f"Erro no teste de seguran√ßa produ√ß√£o-like: {e}")
            raise
    
    def _validate_production_security_headers(self, headers: Dict[str, str]) -> Dict[str, Any]:
        """Valida headers de seguran√ßa de produ√ß√£o"""
        security_headers = {
            "x_frame_options": headers.get("X-Frame-Options"),
            "x_content_type_options": headers.get("X-Content-Type-Options"),
            "x_xss_protection": headers.get("X-XSS-Protection"),
            "strict_transport_security": headers.get("Strict-Transport-Security"),
            "content_security_policy": headers.get("Content-Security-Policy"),
            "referrer_policy": headers.get("Referrer-Policy"),
            "permissions_policy": headers.get("Permissions-Policy"),
            "x_permitted_cross_domain_policies": headers.get("X-Permitted-Cross-Domain-Policies")
        }
        
        # Verificar headers obrigat√≥rios para produ√ß√£o
        required_headers = [
            "x_frame_options", 
            "x_content_type_options", 
            "strict_transport_security",
            "content_security_policy"
        ]
        missing_headers = [h for h in required_headers if not security_headers.get(h)]
        
        # Verificar valores espec√≠ficos para produ√ß√£o
        hsts_valid = security_headers.get("strict_transport_security", "").startswith("max-age=")
        csp_valid = "default-src 'self'" in security_headers.get("content_security_policy", "")
        
        return {
            "valid": len(missing_headers) == 0 and hsts_valid and csp_valid,
            "missing_headers": missing_headers,
            "hsts_valid": hsts_valid,
            "csp_valid": csp_valid,
            "headers": security_headers
        }
    
    def get_performance_metrics(self) -> Dict[str, Any]:
        """Obt√©m m√©tricas de performance dos testes"""
        if not self.environment_events:
            return {"error": "Nenhum teste de ambiente executado"}
        
        return {
            "total_environment_tests": len(self.environment_events),
            "environment_events": self.environment_events,
            "performance_metrics": self.performance_metrics,
            "security_events": self.security_events,
            "ha_events": self.ha_events
        }
    
    async def cleanup(self):
        """Limpa recursos de teste"""
        try:
            if self.db_connection:
                await self.db_connection.disconnect()
            if self.redis_cache:
                await self.redis_cache.disconnect()
            if self.rabbitmq_queue:
                await self.rabbitmq_queue.disconnect()
            
            logger.info("Recursos de teste limpos com sucesso")
        except Exception as e:
            logger.error(f"Erro ao limpar recursos: {e}")

# Testes pytest
@pytest.mark.asyncio
class TestProductionLikeEnvironment:
    """Testes de ambiente similar √† produ√ß√£o"""
    
    @pytest.fixture(autouse=True)
    async def setup_test(self):
        """Configura√ß√£o do teste"""
        self.test_instance = ProductionLikeEnvironmentTest()
        await self.test_instance.setup_test_environment()
        yield
        await self.test_instance.cleanup()
    
    async def test_production_like_connectivity(self):
        """Testa conectividade com ambiente similar √† produ√ß√£o"""
        result = await self.test_instance.test_production_like_connectivity()
        assert result["success"] is True
        assert result["response_time"] < 3.0
    
    async def test_production_like_api_performance(self):
        """Testa performance da API similar √† produ√ß√£o"""
        result = await self.test_instance.test_production_like_api_performance()
        assert result["success"] is True
        assert result["success_rate"] > 0.95
    
    async def test_production_like_database_performance(self):
        """Testa performance do banco de dados similar √† produ√ß√£o"""
        result = await self.test_instance.test_production_like_database_performance()
        assert result["success"] is True
        assert result["avg_query_time"] < 2.0
    
    async def test_production_like_high_availability(self):
        """Testa alta disponibilidade do ambiente similar √† produ√ß√£o"""
        result = await self.test_instance.test_production_like_high_availability()
        assert result["success"] is True
    
    async def test_production_like_backup_restore(self):
        """Testa backup e restore do ambiente similar √† produ√ß√£o"""
        result = await self.test_instance.test_production_like_backup_restore()
        assert result["success"] is True
    
    async def test_production_like_security(self):
        """Testa seguran√ßa do ambiente similar √† produ√ß√£o"""
        result = await self.test_instance.test_production_like_security()
        assert result["success"] is True
    
    async def test_overall_production_like_environment_metrics(self):
        """Testa m√©tricas gerais do ambiente similar √† produ√ß√£o"""
        # Executar todos os testes
        await self.test_instance.test_production_like_connectivity()
        await self.test_instance.test_production_like_api_performance()
        await self.test_instance.test_production_like_database_performance()
        await self.test_instance.test_production_like_high_availability()
        await self.test_instance.test_production_like_backup_restore()
        await self.test_instance.test_production_like_security()
        
        # Obter m√©tricas
        metrics = self.test_instance.get_performance_metrics()
        
        # Verificar m√©tricas
        assert metrics["total_environment_tests"] > 0

if __name__ == "__main__":
    # Execu√ß√£o direta do teste
    async def main():
        test_instance = ProductionLikeEnvironmentTest()
        try:
            await test_instance.setup_test_environment()
            
            # Executar todos os testes
            await test_instance.test_production_like_connectivity()
            await test_instance.test_production_like_api_performance()
            await test_instance.test_production_like_database_performance()
            await test_instance.test_production_like_high_availability()
            await test_instance.test_production_like_backup_restore()
            await test_instance.test_production_like_security()
            
            # Obter m√©tricas finais
            metrics = test_instance.get_performance_metrics()
            print(f"M√©tricas do Ambiente Similar √† Produ√ß√£o: {json.dumps(metrics, indent=2, default=str)}")
            
        finally:
            await test_instance.cleanup()
    
    asyncio.run(main()) 