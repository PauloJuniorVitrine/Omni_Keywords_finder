#!/usr/bin/env python3
"""
üöÄ SCRIPT DE EXECU√á√ÉO - TESTE DE CARGA AUTH REFRESH
üéØ Objetivo: Executar teste de carga no endpoint /api/auth/refresh
üìÖ Data: 2025-01-27
üîó Tracing ID: RUN_AUTH_REFRESH_001
üìã Ruleset: enterprise_control_layer.yaml

üìä ABORDAGENS APLICADAS:
‚úÖ CoCoT (Completo, Coerente, Transparente)
‚úÖ ToT (Tree of Thoughts) - M√∫ltiplas rotas de pensamento
‚úÖ ReAct (Reasoning + Acting) - Racioc√≠nio + A√ß√£o iterativa
‚úÖ Representa√ß√µes Visuais - Diagramas e fluxos
"""

import os
import sys
import time
import json
import subprocess
import argparse
from datetime import datetime
from pathlib import Path
from typing import Dict, Any, Optional

# Configura√ß√µes do teste
TEST_CONFIG = {
    "locust_file": "locustfile_auth_refresh_v1.py",
    "host": "http://localhost:8000",
    "users": 50,
    "spawn_rate": 10,
    "run_time": "5m",
    "headless": True,
    "html_report": "auth_refresh_test_report.html",
    "json_report": "auth_refresh_test_report.json",
    "csv_report": "auth_refresh_test_report.csv",
    "log_file": "auth_refresh_test.log"
}

class AuthRefreshTestRunner:
    """Executor do teste de carga para auth refresh"""
    
    def __init__(self, config: Dict[str, Any]):
        self.config = config
        self.test_start_time = None
        self.test_end_time = None
        self.results = {}
        
    def validate_environment(self) -> bool:
        """Valida ambiente antes da execu√ß√£o"""
        print("üîç Validando ambiente...")
        
        # Verificar se Locust est√° instalado
        try:
            subprocess.run(["locust", "--version"], 
                         capture_output=True, check=True)
            print("‚úÖ Locust instalado")
        except (subprocess.CalledProcessError, FileNotFoundError):
            print("‚ùå Locust n√£o encontrado. Instale com: pip install locust")
            return False
        
        # Verificar se o arquivo de teste existe
        if not Path(self.config["locust_file"]).exists():
            print(f"‚ùå Arquivo de teste n√£o encontrado: {self.config['locust_file']}")
            return False
        print(f"‚úÖ Arquivo de teste encontrado: {self.config['locust_file']}")
        
        # Verificar conectividade com o servidor
        try:
            import requests
            response = requests.get(f"{self.config['host']}/health", timeout=5)
            if response.status_code == 200:
                print(f"‚úÖ Servidor acess√≠vel: {self.config['host']}")
            else:
                print(f"‚ö†Ô∏è Servidor respondeu com status {response.status_code}")
        except Exception as e:
            print(f"‚ö†Ô∏è N√£o foi poss√≠vel conectar ao servidor: {e}")
            print("   Certifique-se de que o servidor est√° rodando")
        
        return True
    
    def prepare_test_environment(self) -> bool:
        """Prepara ambiente para o teste"""
        print("üîß Preparando ambiente...")
        
        # Criar diret√≥rio de relat√≥rios se n√£o existir
        reports_dir = Path("reports")
        reports_dir.mkdir(exist_ok=True)
        
        # Limpar relat√≥rios anteriores
        for report_file in ["html", "json", "csv"]:
            report_path = reports_dir / self.config[f"{report_file}_report"]
            if report_path.exists():
                report_path.unlink()
                print(f"üóëÔ∏è Removido relat√≥rio anterior: {report_path}")
        
        print("‚úÖ Ambiente preparado")
        return True
    
    def build_locust_command(self) -> list:
        """Constr√≥i comando Locust"""
        cmd = [
            "locust",
            "-f", self.config["locust_file"],
            "--host", self.config["host"],
            "--users", str(self.config["users"]),
            "--spawn-rate", str(self.config["spawn_rate"]),
            "--run-time", self.config["run_time"],
            "--html", f"reports/{self.config['html_report']}",
            "--json", f"reports/{self.config['json_report']}",
            "--csv", f"reports/{self.config['csv_report']}",
            "--logfile", f"reports/{self.config['log_file']}",
            "--loglevel", "INFO"
        ]
        
        if self.config["headless"]:
            cmd.append("--headless")
        
        return cmd
    
    def execute_test(self) -> bool:
        """Executa o teste de carga"""
        print("üöÄ Executando teste de carga...")
        print(f"üìä Configura√ß√£o:")
        print(f"   - Host: {self.config['host']}")
        print(f"   - Usu√°rios: {self.config['users']}")
        print(f"   - Spawn Rate: {self.config['spawn_rate']}")
        print(f"   - Dura√ß√£o: {self.config['run_time']}")
        print(f"   - Modo: {'Headless' if self.config['headless'] else 'Web'}")
        
        self.test_start_time = datetime.now()
        
        try:
            cmd = self.build_locust_command()
            print(f"üîß Comando: {' '.join(cmd)}")
            
            # Executar teste
            result = subprocess.run(
                cmd,
                capture_output=True,
                text=True,
                timeout=600  # 10 minutos timeout
            )
            
            self.test_end_time = datetime.now()
            
            if result.returncode == 0:
                print("‚úÖ Teste executado com sucesso")
                return True
            else:
                print(f"‚ùå Teste falhou com c√≥digo {result.returncode}")
                print(f"üìÑ Sa√≠da de erro: {result.stderr}")
                return False
                
        except subprocess.TimeoutExpired:
            print("‚è∞ Teste excedeu tempo limite (10 minutos)")
            return False
        except Exception as e:
            print(f"‚ùå Erro durante execu√ß√£o: {e}")
            return False
    
    def analyze_results(self) -> Dict[str, Any]:
        """Analisa resultados do teste"""
        print("üìä Analisando resultados...")
        
        results = {
            "test_info": {
                "name": "Auth Refresh Load Test",
                "start_time": self.test_start_time.isoformat() if self.test_start_time else None,
                "end_time": self.test_end_time.isoformat() if self.test_end_time else None,
                "duration": None,
                "config": self.config
            },
            "metrics": {},
            "recommendations": []
        }
        
        # Calcular dura√ß√£o
        if self.test_start_time and self.test_end_time:
            duration = self.test_end_time - self.test_start_time
            results["test_info"]["duration"] = str(duration)
        
        # Ler relat√≥rio JSON se existir
        json_report_path = Path("reports") / self.config["json_report"]
        if json_report_path.exists():
            try:
                with open(json_report_path, 'r') as f:
                    json_data = json.load(f)
                
                # Extrair m√©tricas principais
                if "stats" in json_data:
                    stats = json_data["stats"]
                    results["metrics"] = {
                        "total_requests": stats.get("total_requests", 0),
                        "total_failures": stats.get("total_failures", 0),
                        "avg_response_time": stats.get("avg_response_time", 0),
                        "max_response_time": stats.get("max_response_time", 0),
                        "min_response_time": stats.get("min_response_time", 0),
                        "requests_per_sec": stats.get("requests_per_sec", 0),
                        "failure_rate": stats.get("failure_rate", 0)
                    }
                
                # Gerar recomenda√ß√µes
                results["recommendations"] = self.generate_recommendations(results["metrics"])
                
            except Exception as e:
                print(f"‚ö†Ô∏è Erro ao ler relat√≥rio JSON: {e}")
        
        return results
    
    def generate_recommendations(self, metrics: Dict[str, Any]) -> list:
        """Gera recomenda√ß√µes baseadas nas m√©tricas"""
        recommendations = []
        
        # An√°lise de taxa de falha
        failure_rate = metrics.get("failure_rate", 0)
        if failure_rate > 5:
            recommendations.append({
                "type": "warning",
                "message": f"Taxa de falha alta ({failure_rate:.2f}%). Verificar logs de erro.",
                "action": "Investigar causas das falhas nos logs"
            })
        elif failure_rate > 1:
            recommendations.append({
                "type": "info",
                "message": f"Taxa de falha moderada ({failure_rate:.2f}%). Monitorar.",
                "action": "Acompanhar tend√™ncia de falhas"
            })
        else:
            recommendations.append({
                "type": "success",
                "message": f"Taxa de falha baixa ({failure_rate:.2f}%). Excelente!",
                "action": "Manter configura√ß√µes atuais"
            })
        
        # An√°lise de tempo de resposta
        avg_response_time = metrics.get("avg_response_time", 0)
        if avg_response_time > 2000:
            recommendations.append({
                "type": "warning",
                "message": f"Tempo de resposta alto ({avg_response_time:.0f}ms).",
                "action": "Otimizar endpoint ou aumentar recursos"
            })
        elif avg_response_time > 1000:
            recommendations.append({
                "type": "info",
                "message": f"Tempo de resposta moderado ({avg_response_time:.0f}ms).",
                "action": "Considerar otimiza√ß√µes se necess√°rio"
            })
        else:
            recommendations.append({
                "type": "success",
                "message": f"Tempo de resposta bom ({avg_response_time:.0f}ms).",
                "action": "Manter performance atual"
            })
        
        # An√°lise de throughput
        requests_per_sec = metrics.get("requests_per_sec", 0)
        if requests_per_sec < 10:
            recommendations.append({
                "type": "warning",
                "message": f"Throughput baixo ({requests_per_sec:.1f} req/s).",
                "action": "Investigar gargalos de performance"
            })
        elif requests_per_sec < 50:
            recommendations.append({
                "type": "info",
                "message": f"Throughput moderado ({requests_per_sec:.1f} req/s).",
                "action": "Avaliar necessidade de otimiza√ß√£o"
            })
        else:
            recommendations.append({
                "type": "success",
                "message": f"Throughput bom ({requests_per_sec:.1f} req/s).",
                "action": "Manter configura√ß√µes atuais"
            })
        
        return recommendations
    
    def generate_report(self, results: Dict[str, Any]) -> None:
        """Gera relat√≥rio final"""
        print("üìã Gerando relat√≥rio final...")
        
        report_path = Path("reports") / "auth_refresh_test_summary.md"
        
        with open(report_path, 'w', encoding='utf-8') as f:
            f.write("# üß™ RELAT√ìRIO - TESTE DE CARGA AUTH REFRESH\n\n")
            
            f.write("## üìä Informa√ß√µes do Teste\n\n")
            f.write(f"- **Nome**: {results['test_info']['name']}\n")
            f.write(f"- **In√≠cio**: {results['test_info']['start_time']}\n")
            f.write(f"- **Fim**: {results['test_info']['end_time']}\n")
            f.write(f"- **Dura√ß√£o**: {results['test_info']['duration']}\n\n")
            
            f.write("## ‚öôÔ∏è Configura√ß√£o\n\n")
            config = results['test_info']['config']
            f.write(f"- **Host**: {config['host']}\n")
            f.write(f"- **Usu√°rios**: {config['users']}\n")
            f.write(f"- **Spawn Rate**: {config['spawn_rate']}\n")
            f.write(f"- **Dura√ß√£o**: {config['run_time']}\n\n")
            
            f.write("## üìà M√©tricas Principais\n\n")
            metrics = results['metrics']
            f.write(f"- **Total de Requisi√ß√µes**: {metrics.get('total_requests', 0):,}\n")
            f.write(f"- **Falhas**: {metrics.get('total_failures', 0):,}\n")
            f.write(f"- **Taxa de Falha**: {metrics.get('failure_rate', 0):.2f}%\n")
            f.write(f"- **Tempo M√©dio de Resposta**: {metrics.get('avg_response_time', 0):.0f}ms\n")
            f.write(f"- **Tempo M√°ximo de Resposta**: {metrics.get('max_response_time', 0):.0f}ms\n")
            f.write(f"- **Throughput**: {metrics.get('requests_per_sec', 0):.1f} req/s\n\n")
            
            f.write("## üí° Recomenda√ß√µes\n\n")
            for rec in results['recommendations']:
                icon = "‚úÖ" if rec['type'] == 'success' else "‚ö†Ô∏è" if rec['type'] == 'warning' else "‚ÑπÔ∏è"
                f.write(f"{icon} **{rec['message']}**\n")
                f.write(f"   *A√ß√£o: {rec['action']}*\n\n")
            
            f.write("## üìÅ Arquivos Gerados\n\n")
            f.write("- `auth_refresh_test_report.html` - Relat√≥rio HTML detalhado\n")
            f.write("- `auth_refresh_test_report.json` - Dados brutos em JSON\n")
            f.write("- `auth_refresh_test_report.csv` - Dados em CSV\n")
            f.write("- `auth_refresh_test.log` - Logs da execu√ß√£o\n")
            f.write("- `auth_refresh_test_summary.md` - Este relat√≥rio\n\n")
            
            f.write("## üéØ Pr√≥ximos Passos\n\n")
            f.write("1. Analisar relat√≥rio HTML para detalhes completos\n")
            f.write("2. Verificar logs para identificar problemas espec√≠ficos\n")
            f.write("3. Implementar recomenda√ß√µes conforme necess√°rio\n")
            f.write("4. Executar testes de regress√£o ap√≥s otimiza√ß√µes\n")
        
        print(f"‚úÖ Relat√≥rio gerado: {report_path}")
    
    def run(self) -> bool:
        """Executa o teste completo"""
        print("üöÄ INICIANDO TESTE DE CARGA - AUTH REFRESH")
        print("=" * 60)
        
        # Valida√ß√£o do ambiente
        if not self.validate_environment():
            return False
        
        # Prepara√ß√£o do ambiente
        if not self.prepare_test_environment():
            return False
        
        # Execu√ß√£o do teste
        if not self.execute_test():
            return False
        
        # An√°lise dos resultados
        results = self.analyze_results()
        
        # Gera√ß√£o do relat√≥rio
        self.generate_report(results)
        
        print("=" * 60)
        print("‚úÖ TESTE CONCLU√çDO COM SUCESSO!")
        print(f"üìä Taxa de falha: {results['metrics'].get('failure_rate', 0):.2f}%")
        print(f"‚è±Ô∏è Tempo m√©dio: {results['metrics'].get('avg_response_time', 0):.0f}ms")
        print(f"üöÄ Throughput: {results['metrics'].get('requests_per_sec', 0):.1f} req/s")
        
        return True

def main():
    """Fun√ß√£o principal"""
    parser = argparse.ArgumentParser(description="Executar teste de carga para auth refresh")
    parser.add_argument("--host", default="http://localhost:8000", help="Host do servidor")
    parser.add_argument("--users", type=int, default=50, help="N√∫mero de usu√°rios")
    parser.add_argument("--spawn-rate", type=int, default=10, help="Taxa de spawn")
    parser.add_argument("--run-time", default="5m", help="Dura√ß√£o do teste")
    parser.add_argument("--web", action="store_true", help="Executar em modo web")
    
    args = parser.parse_args()
    
    # Atualizar configura√ß√£o com argumentos
    TEST_CONFIG.update({
        "host": args.host,
        "users": args.users,
        "spawn_rate": args.spawn_rate,
        "run_time": args.run_time,
        "headless": not args.web
    })
    
    # Executar teste
    runner = AuthRefreshTestRunner(TEST_CONFIG)
    success = runner.run()
    
    sys.exit(0 if success else 1)

if __name__ == "__main__":
    main() 