#!/usr/bin/env python3
"""
üöÄ Script de Execu√ß√£o - Teste de Carga Logout
üéØ Objetivo: Executar testes de carga para endpoint /api/auth/logout
üìÖ Data: 2025-01-27
üîó Tracing ID: RUN_LOGOUT_TEST_20250127_001
üìã Ruleset: enterprise_control_layer.yaml

üìä Abordagens de Racioc√≠nio:
- CoCoT: Comprova√ß√£o, Causalidade, Contexto, Tend√™ncia
- ToT: Tree of Thoughts - M√∫ltiplas abordagens de execu√ß√£o
- ReAct: Simula√ß√£o e reflex√£o sobre resultados
- Representa√ß√µes Visuais: Relat√≥rios e m√©tricas
"""

import os
import sys
import json
import time
import subprocess
import argparse
from datetime import datetime
from pathlib import Path
from typing import Dict, Any, Optional
import logging

# Configura√ß√£o de logging
logging.basicConfig(
    level=logging.INFO,
    format='%(asctime)s - %(levelname)s - %(message)s',
    handlers=[
        logging.FileHandler('logout_test_execution.log'),
        logging.StreamHandler(sys.stdout)
    ]
)
logger = logging.getLogger(__name__)

class LogoutTestRunner:
    """
    Executor de testes de carga para endpoint de logout
    Implementa valida√ß√µes e m√©tricas baseadas no c√≥digo real
    """
    
    def __init__(self, config: Dict[str, Any]):
        self.config = config
        self.results_dir = Path("tests/load/critical/auth/results")
        self.results_dir.mkdir(parents=True, exist_ok=True)
        self.timestamp = datetime.now().strftime("%Y%m%d_%H%M%S")
        
    def validate_environment(self) -> bool:
        """
        üîç Valida√ß√£o do ambiente de teste
        üìä CoCoT: Comprova√ß√£o de requisitos
        """
        logger.info("üîç Validando ambiente de teste...")
        
        # Verificar depend√™ncias
        try:
            import locust
            logger.info(f"‚úÖ Locust instalado: {locust.__version__}")
        except ImportError:
            logger.error("‚ùå Locust n√£o instalado")
            return False
        
        # Verificar arquivo de teste
        test_file = Path("tests/load/critical/auth/locustfile_auth_logout_v1.py")
        if not test_file.exists():
            logger.error(f"‚ùå Arquivo de teste n√£o encontrado: {test_file}")
            return False
        
        # Verificar configura√ß√£o do ambiente
        required_env_vars = [
            "JWT_SECRET_KEY",
            "DATABASE_URL",
            "REDIS_URL"
        ]
        
        missing_vars = []
        for var in required_env_vars:
            if not os.getenv(var):
                missing_vars.append(var)
        
        if missing_vars:
            logger.warning(f"‚ö†Ô∏è Vari√°veis de ambiente ausentes: {missing_vars}")
            logger.info("üí° Usando valores padr√£o para desenvolvimento")
        
        # Verificar conectividade com API
        try:
            import requests
            response = requests.get(f"{self.config['api_url']}/health", timeout=5)
            if response.status_code == 200:
                logger.info("‚úÖ API acess√≠vel")
            else:
                logger.warning(f"‚ö†Ô∏è API retornou status {response.status_code}")
        except Exception as e:
            logger.warning(f"‚ö†Ô∏è N√£o foi poss√≠vel conectar com API: {e}")
        
        logger.info("‚úÖ Valida√ß√£o de ambiente conclu√≠da")
        return True
    
    def prepare_test_data(self) -> bool:
        """
        üìä Prepara√ß√£o de dados de teste
        üìä CoCoT: Contexto de dados necess√°rios
        """
        logger.info("üìä Preparando dados de teste...")
        
        try:
            # Criar usu√°rios de teste se necess√°rio
            self._create_test_users()
            
            # Preparar tokens de teste
            self._prepare_test_tokens()
            
            logger.info("‚úÖ Dados de teste preparados")
            return True
            
        except Exception as e:
            logger.error(f"‚ùå Erro na prepara√ß√£o de dados: {e}")
            return False
    
    def _create_test_users(self):
        """Criar usu√°rios de teste para o cen√°rio"""
        import requests
        
        test_users = [
            {"username": "test_user_load", "senha": "TestPass123!", "email": "test_load@example.com"},
            {"username": "test_user_1", "senha": "TestPass123!", "email": "test1@example.com"},
            {"username": "test_user_2", "senha": "TestPass123!", "email": "test2@example.com"},
            {"username": "test_user_3", "senha": "TestPass123!", "email": "test3@example.com"}
        ]
        
        for user_data in test_users:
            try:
                # Tentar criar usu√°rio (pode falhar se j√° existir)
                response = requests.post(
                    f"{self.config['api_url']}/api/auth/register",
                    json=user_data,
                    headers={"Content-Type": "application/json"},
                    timeout=10
                )
                
                if response.status_code in [200, 201, 409]:  # 409 = usu√°rio j√° existe
                    logger.info(f"‚úÖ Usu√°rio {user_data['username']} dispon√≠vel")
                else:
                    logger.warning(f"‚ö†Ô∏è Erro ao criar usu√°rio {user_data['username']}: {response.status_code}")
                    
            except Exception as e:
                logger.warning(f"‚ö†Ô∏è Erro ao preparar usu√°rio {user_data['username']}: {e}")
    
    def _prepare_test_tokens(self):
        """Preparar tokens de teste para cen√°rios espec√≠ficos"""
        # Tokens expirados para teste
        expired_tokens = [
            "eyJhbGciOiJIUzI1NiIsInR5cCI6IkpXVCJ9.eyJzdWIiOiIxMjM0NTY3ODkwIiwibmFtZSI6IkpvaG4gRG9lIiwiaWF0IjoxNTE2MjM5MDIyLCJleHAiOjE1MTYyMzkwMjJ9.SflKxwRJSMeKKF2QT4fwpMeJf36POk6yJV_adQssw5c"
        ]
        
        # Salvar tokens em arquivo para uso no teste
        tokens_file = self.results_dir / "test_tokens.json"
        with open(tokens_file, 'w') as f:
            json.dump({
                "expired_tokens": expired_tokens,
                "created_at": datetime.now().isoformat()
            }, f, indent=2)
        
        logger.info(f"‚úÖ Tokens de teste salvos em {tokens_file}")
    
    def run_load_test(self) -> bool:
        """
        üöÄ Execu√ß√£o do teste de carga
        üìä CoCoT: Comprova√ß√£o de performance
        üéØ ToT: M√∫ltiplas abordagens de execu√ß√£o
        """
        logger.info("üöÄ Iniciando teste de carga de logout...")
        
        # Configurar par√¢metros do Locust
        locust_params = [
            "locust",
            "-f", "tests/load/critical/auth/locustfile_auth_logout_v1.py",
            "--host", self.config["api_url"],
            "--users", str(self.config["users"]),
            "--spawn-rate", str(self.config["spawn_rate"]),
            "--run-time", self.config["run_time"],
            "--headless",
            "--html", str(self.results_dir / f"logout_test_report_{self.timestamp}.html"),
            "--json", str(self.results_dir / f"logout_test_results_{self.timestamp}.json"),
            "--csv", str(self.results_dir / f"logout_test_metrics_{self.timestamp}"),
            "--logfile", str(self.results_dir / f"logout_test_{self.timestamp}.log")
        ]
        
        try:
            # Executar teste
            logger.info(f"üìä Executando: {' '.join(locust_params)}")
            
            start_time = time.time()
            result = subprocess.run(
                locust_params,
                capture_output=True,
                text=True,
                timeout=int(self.config["run_time"].replace("m", "")) * 60 + 300  # +5 min buffer
            )
            execution_time = time.time() - start_time
            
            # Analisar resultado
            if result.returncode == 0:
                logger.info("‚úÖ Teste de carga executado com sucesso")
                self._analyze_results()
                return True
            else:
                logger.error(f"‚ùå Erro na execu√ß√£o do teste: {result.stderr}")
                return False
                
        except subprocess.TimeoutExpired:
            logger.error("‚ùå Teste de carga excedeu tempo limite")
            return False
        except Exception as e:
            logger.error(f"‚ùå Erro inesperado: {e}")
            return False
    
    def _analyze_results(self):
        """
        üìà An√°lise dos resultados do teste
        üìä CoCoT: Tend√™ncia de performance
        üéØ ReAct: Reflex√£o sobre m√©tricas
        """
        logger.info("üìà Analisando resultados...")
        
        # Carregar resultados JSON
        results_file = self.results_dir / f"logout_test_results_{self.timestamp}.json"
        if results_file.exists():
            with open(results_file, 'r') as f:
                results = json.load(f)
            
            # Extrair m√©tricas principais
            stats = results.get("stats", [])
            
            # An√°lise por cen√°rio
            scenario_analysis = {}
            for stat in stats:
                name = stat.get("name", "")
                if name.startswith("logout_"):
                    scenario_analysis[name] = {
                        "num_requests": stat.get("num_requests", 0),
                        "num_failures": stat.get("num_failures", 0),
                        "avg_response_time": stat.get("avg_response_time", 0),
                        "max_response_time": stat.get("max_response_time", 0),
                        "min_response_time": stat.get("min_response_time", 0),
                        "current_rps": stat.get("current_rps", 0)
                    }
            
            # Calcular m√©tricas agregadas
            total_requests = sum(s["num_requests"] for s in scenario_analysis.values())
            total_failures = sum(s["num_failures"] for s in scenario_analysis.values())
            avg_response_time = sum(s["avg_response_time"] * s["num_requests"] for s in scenario_analysis.values()) / total_requests if total_requests > 0 else 0
            error_rate = total_failures / total_requests if total_requests > 0 else 0
            
            # Crit√©rios de sucesso
            success_criteria = {
                "response_time_p95": 1500,  # 95% das requisi√ß√µes < 1.5s
                "error_rate": 0.05,         # Taxa de erro < 5%
                "throughput": 50,           # M√≠nimo 50 RPS
                "availability": 0.99        # 99% de disponibilidade
            }
            
            # Avaliar crit√©rios
            evaluation = {
                "response_time_ok": avg_response_time <= success_criteria["response_time_p95"],
                "error_rate_ok": error_rate <= success_criteria["error_rate"],
                "availability_ok": (1 - error_rate) >= success_criteria["availability"]
            }
            
            # Gerar relat√≥rio
            report = {
                "timestamp": self.timestamp,
                "test_config": self.config,
                "scenario_analysis": scenario_analysis,
                "aggregate_metrics": {
                    "total_requests": total_requests,
                    "total_failures": total_failures,
                    "avg_response_time": avg_response_time,
                    "error_rate": error_rate,
                    "availability": 1 - error_rate
                },
                "success_criteria": success_criteria,
                "evaluation": evaluation,
                "overall_success": all(evaluation.values())
            }
            
            # Salvar relat√≥rio
            report_file = self.results_dir / f"logout_test_analysis_{self.timestamp}.json"
            with open(report_file, 'w') as f:
                json.dump(report, f, indent=2)
            
            # Log dos resultados
            logger.info(f"üìä Total de requisi√ß√µes: {total_requests}")
            logger.info(f"üìä Taxa de erro: {error_rate:.2%}")
            logger.info(f"üìä Tempo m√©dio de resposta: {avg_response_time:.2f}ms")
            logger.info(f"üìä Disponibilidade: {(1-error_rate):.2%}")
            
            if report["overall_success"]:
                logger.info("‚úÖ Todos os crit√©rios de sucesso atendidos")
            else:
                logger.warning("‚ö†Ô∏è Alguns crit√©rios de sucesso n√£o foram atendidos")
                for criterion, passed in evaluation.items():
                    if not passed:
                        logger.warning(f"   - {criterion}: ‚ùå")
            
            logger.info(f"üìÑ Relat√≥rio salvo em: {report_file}")
    
    def generate_visual_report(self):
        """
        üìä Gera√ß√£o de relat√≥rio visual
        üìä Representa√ß√µes Visuais: Gr√°ficos e diagramas
        """
        logger.info("üìä Gerando relat√≥rio visual...")
        
        try:
            import matplotlib.pyplot as plt
            import pandas as pd
            
            # Carregar dados
            results_file = self.results_dir / f"logout_test_results_{self.timestamp}.json"
            if not results_file.exists():
                logger.warning("‚ö†Ô∏è Arquivo de resultados n√£o encontrado para visualiza√ß√£o")
                return
            
            with open(results_file, 'r') as f:
                results = json.load(f)
            
            # Criar gr√°ficos
            fig, axes = plt.subplots(2, 2, figsize=(15, 10))
            fig.suptitle(f'Teste de Carga - Endpoint Logout ({self.timestamp})', fontsize=16)
            
            # Gr√°fico 1: Requisi√ß√µes por cen√°rio
            stats = results.get("stats", [])
            logout_stats = [s for s in stats if s.get("name", "").startswith("logout_")]
            
            if logout_stats:
                names = [s["name"] for s in logout_stats]
                requests = [s["num_requests"] for s in logout_stats]
                
                axes[0, 0].bar(names, requests, color='skyblue')
                axes[0, 0].set_title('Requisi√ß√µes por Cen√°rio')
                axes[0, 0].set_ylabel('N√∫mero de Requisi√ß√µes')
                axes[0, 0].tick_params(axis='x', rotation=45)
                
                # Gr√°fico 2: Taxa de erro por cen√°rio
                error_rates = [s["num_failures"]/s["num_requests"] if s["num_requests"] > 0 else 0 for s in logout_stats]
                
                axes[0, 1].bar(names, error_rates, color='lightcoral')
                axes[0, 1].set_title('Taxa de Erro por Cen√°rio')
                axes[0, 1].set_ylabel('Taxa de Erro')
                axes[0, 1].tick_params(axis='x', rotation=45)
                
                # Gr√°fico 3: Tempo de resposta por cen√°rio
                response_times = [s["avg_response_time"] for s in logout_stats]
                
                axes[1, 0].bar(names, response_times, color='lightgreen')
                axes[1, 0].set_title('Tempo M√©dio de Resposta por Cen√°rio')
                axes[1, 0].set_ylabel('Tempo (ms)')
                axes[1, 0].tick_params(axis='x', rotation=45)
                
                # Gr√°fico 4: RPS por cen√°rio
                rps_values = [s["current_rps"] for s in logout_stats]
                
                axes[1, 1].bar(names, rps_values, color='gold')
                axes[1, 1].set_title('Requests por Segundo por Cen√°rio')
                axes[1, 1].set_ylabel('RPS')
                axes[1, 1].tick_params(axis='x', rotation=45)
            
            plt.tight_layout()
            
            # Salvar gr√°fico
            chart_file = self.results_dir / f"logout_test_charts_{self.timestamp}.png"
            plt.savefig(chart_file, dpi=300, bbox_inches='tight')
            logger.info(f"üìä Gr√°ficos salvos em: {chart_file}")
            
        except ImportError:
            logger.warning("‚ö†Ô∏è matplotlib n√£o instalado - pulando gera√ß√£o de gr√°ficos")
        except Exception as e:
            logger.error(f"‚ùå Erro na gera√ß√£o de gr√°ficos: {e}")
    
    def run(self) -> bool:
        """
        üéØ Execu√ß√£o completa do teste
        üìä CoCoT: Comprova√ß√£o completa do processo
        """
        logger.info("üéØ Iniciando execu√ß√£o completa do teste de logout")
        
        # Valida√ß√£o de ambiente
        if not self.validate_environment():
            return False
        
        # Prepara√ß√£o de dados
        if not self.prepare_test_data():
            return False
        
        # Execu√ß√£o do teste
        if not self.run_load_test():
            return False
        
        # Gera√ß√£o de relat√≥rio visual
        self.generate_visual_report()
        
        logger.info("‚úÖ Execu√ß√£o completa finalizada")
        return True

def main():
    """Fun√ß√£o principal"""
    parser = argparse.ArgumentParser(description="Executor de teste de carga para logout")
    parser.add_argument("--api-url", default="http://localhost:8000", help="URL da API")
    parser.add_argument("--users", type=int, default=50, help="N√∫mero de usu√°rios")
    parser.add_argument("--spawn-rate", type=int, default=10, help="Taxa de spawn (usu√°rios/seg)")
    parser.add_argument("--run-time", default="5m", help="Tempo de execu√ß√£o (ex: 5m, 1h)")
    parser.add_argument("--config-file", help="Arquivo de configura√ß√£o JSON")
    
    args = parser.parse_args()
    
    # Configura√ß√£o padr√£o
    config = {
        "api_url": args.api_url,
        "users": args.users,
        "spawn_rate": args.spawn_rate,
        "run_time": args.run_time
    }
    
    # Carregar configura√ß√£o de arquivo se fornecido
    if args.config_file:
        try:
            with open(args.config_file, 'r') as f:
                file_config = json.load(f)
                config.update(file_config)
        except Exception as e:
            logger.error(f"‚ùå Erro ao carregar arquivo de configura√ß√£o: {e}")
            return 1
    
    # Executar teste
    runner = LogoutTestRunner(config)
    success = runner.run()
    
    return 0 if success else 1

if __name__ == "__main__":
    sys.exit(main()) 