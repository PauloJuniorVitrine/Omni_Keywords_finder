#!/usr/bin/env python3
"""
Script de Execu√ß√£o - Testes de Carga Gera√ß√£o de Relat√≥rios
Baseado em: locustfile_reports_generate_v1.py

Tracing ID: RUN_REPORTS_GENERATE_20250127_001
Data/Hora: 2025-01-27 19:00:00 UTC
Vers√£o: 1.0
Ruleset: enterprise_control_layer.yaml

Funcionalidades:
- Execu√ß√£o de testes de gera√ß√£o de relat√≥rios
- Configura√ß√£o de par√¢metros
- Gera√ß√£o de relat√≥rios
- Valida√ß√£o de thresholds
- Monitoramento de m√©tricas
"""

import os
import sys
import json
import time
import subprocess
import argparse
from datetime import datetime, timedelta
from typing import Dict, Any, List, Optional
from pathlib import Path

# Configura√ß√£o de paths
SCRIPT_DIR = Path(__file__).parent
PROJECT_ROOT = SCRIPT_DIR.parent.parent.parent
TESTS_DIR = PROJECT_ROOT / "tests" / "load"
REPORTS_DIR = PROJECT_ROOT / "reports" / "load_tests"
LOGS_DIR = PROJECT_ROOT / "logs" / "load_tests"

# Configura√ß√µes padr√£o
DEFAULT_CONFIG = {
    "base_url": "http://localhost:8000",
    "users": 50,
    "spawn_rate": 10,
    "run_time": "10m",
    "host": "http://localhost:8000",
    "locustfile": "tests/load/high/analytics/locustfile_reports_generate_v1.py",
    "html_report": "reports/load_tests/reports_generate_report.html",
    "json_report": "reports/load_tests/reports_generate_report.json",
    "csv_report": "reports/load_tests/reports_generate_report.csv",
    "log_file": "logs/load_tests/reports_generate_test.log"
}

# Thresholds de qualidade
QUALITY_THRESHOLDS = {
    "response_time_p95": 5000,  # 5 segundos
    "response_time_p99": 10000,  # 10 segundos
    "error_rate": 0.05,  # 5%
    "requests_per_second": 10,
    "success_rate": 0.95  # 95%
}

class ReportsGenerateLoadTestRunner:
    """
    Executor de testes de carga para gera√ß√£o de relat√≥rios
    """
    
    def __init__(self, config: Dict[str, Any]):
        self.config = {**DEFAULT_CONFIG, **config}
        self.tracing_id = f"RUN_REPORTS_GENERATE_{datetime.now().strftime('%Y%m%d_%H%M%S')}"
        self.start_time = None
        self.end_time = None
        self.results = {}
        
        # Criar diret√≥rios necess√°rios
        self._create_directories()
        
        print(f"üöÄ [REPORTS_GENERATE] Iniciando executor de testes")
        print(f"üìã Tracing ID: {self.tracing_id}")
        print(f"‚è∞ Data/Hora: {datetime.now().strftime('%Y-%m-%d %H:%M:%S')}")
        print(f"üéØ Configura√ß√£o: {json.dumps(self.config, indent=2)}")
    
    def _create_directories(self):
        """Criar diret√≥rios necess√°rios"""
        directories = [
            REPORTS_DIR,
            LOGS_DIR,
            REPORTS_DIR / "reports_generate",
            LOGS_DIR / "reports_generate"
        ]
        
        for directory in directories:
            directory.mkdir(parents=True, exist_ok=True)
    
    def run_test(self, test_type: str = "normal") -> Dict[str, Any]:
        """
        Executar teste de carga espec√≠fico
        
        Args:
            test_type: Tipo de teste (normal, complex, batch, scheduled, stress)
            
        Returns:
            Resultados do teste
        """
        print(f"\nüìä [REPORTS_GENERATE] Executando teste: {test_type}")
        
        # Configurar par√¢metros espec√≠ficos do teste
        test_config = self._get_test_config(test_type)
        
        # Construir comando Locust
        cmd = self._build_locust_command(test_config)
        
        # Executar teste
        self.start_time = datetime.now()
        result = self._execute_locust_test(cmd, test_type)
        self.end_time = datetime.now()
        
        # Processar resultados
        processed_result = self._process_results(result, test_type)
        
        # Validar thresholds
        validation_result = self._validate_thresholds(processed_result, test_type)
        
        # Salvar resultados
        self._save_results(processed_result, test_type)
        
        # Gerar relat√≥rio
        self._generate_report(processed_result, test_type)
        
        return {
            "test_type": test_type,
            "success": validation_result["passed"],
            "results": processed_result,
            "validation": validation_result,
            "duration": (self.end_time - self.start_time).total_seconds()
        }
    
    def _get_test_config(self, test_type: str) -> Dict[str, Any]:
        """Obter configura√ß√£o espec√≠fica do teste"""
        base_config = self.config.copy()
        
        if test_type == "normal":
            base_config.update({
                "users": 30,
                "spawn_rate": 5,
                "run_time": "5m"
            })
        elif test_type == "complex":
            base_config.update({
                "users": 20,
                "spawn_rate": 3,
                "run_time": "8m"
            })
        elif test_type == "batch":
            base_config.update({
                "users": 15,
                "spawn_rate": 2,
                "run_time": "6m"
            })
        elif test_type == "scheduled":
            base_config.update({
                "users": 10,
                "spawn_rate": 2,
                "run_time": "4m"
            })
        elif test_type == "stress":
            base_config.update({
                "users": 50,
                "spawn_rate": 10,
                "run_time": "15m"
            })
        
        return base_config
    
    def _build_locust_command(self, config: Dict[str, Any]) -> List[str]:
        """Construir comando Locust"""
        cmd = [
            "locust",
            "--locustfile", config["locustfile"],
            "--host", config["host"],
            "--users", str(config["users"]),
            "--spawn-rate", str(config["spawn_rate"]),
            "--run-time", config["run_time"],
            "--headless",
            "--html", config["html_report"],
            "--json", config["json_report"],
            "--csv", config["csv_report"],
            "--logfile", config["log_file"],
            "--loglevel", "INFO"
        ]
        
        return cmd
    
    def _execute_locust_test(self, cmd: List[str], test_type: str) -> Dict[str, Any]:
        """Executar teste Locust"""
        print(f"üîß [REPORTS_GENERATE] Comando: {' '.join(cmd)}")
        
        try:
            # Executar comando
            process = subprocess.Popen(
                cmd,
                stdout=subprocess.PIPE,
                stderr=subprocess.PIPE,
                text=True,
                cwd=PROJECT_ROOT
            )
            
            stdout, stderr = process.communicate()
            
            # Verificar se executou com sucesso
            if process.returncode != 0:
                print(f"‚ùå [REPORTS_GENERATE] Erro na execu√ß√£o: {stderr}")
                return {
                    "success": False,
                    "error": stderr,
                    "stdout": stdout,
                    "return_code": process.returncode
                }
            
            print(f"‚úÖ [REPORTS_GENERATE] Teste executado com sucesso")
            return {
                "success": True,
                "stdout": stdout,
                "stderr": stderr,
                "return_code": process.returncode
            }
            
        except Exception as e:
            print(f"‚ùå [REPORTS_GENERATE] Exce√ß√£o na execu√ß√£o: {str(e)}")
            return {
                "success": False,
                "error": str(e),
                "exception": True
            }
    
    def _process_results(self, result: Dict[str, Any], test_type: str) -> Dict[str, Any]:
        """Processar resultados do teste"""
        if not result["success"]:
            return {
                "test_type": test_type,
                "success": False,
                "error": result.get("error", "Unknown error"),
                "timestamp": datetime.now().isoformat()
            }
        
        # Ler relat√≥rio JSON
        json_report_path = self.config["json_report"]
        if os.path.exists(json_report_path):
            try:
                with open(json_report_path, 'r') as f:
                    report_data = json.load(f)
                
                # Extrair m√©tricas principais
                metrics = {
                    "test_type": test_type,
                    "timestamp": datetime.now().isoformat(),
                    "success": True,
                    "total_requests": report_data.get("total_requests", 0),
                    "total_failures": report_data.get("total_failures", 0),
                    "total_median_response_time": report_data.get("total_median_response_time", 0),
                    "total_avg_response_time": report_data.get("total_avg_response_time", 0),
                    "total_min_response_time": report_data.get("total_min_response_time", 0),
                    "total_max_response_time": report_data.get("total_max_response_time", 0),
                    "total_rps": report_data.get("total_rps", 0),
                    "total_fail_per_sec": report_data.get("total_fail_per_sec", 0),
                    "response_time_percentiles": report_data.get("response_time_percentiles", {}),
                    "num_requests": report_data.get("num_requests", {}),
                    "num_failures": report_data.get("num_failures", {}),
                    "median_response_time": report_data.get("median_response_time", {}),
                    "avg_response_time": report_data.get("avg_response_time", {}),
                    "min_response_time": report_data.get("min_response_time", {}),
                    "max_response_time": report_data.get("max_response_time", {}),
                    "current_rps": report_data.get("current_rps", {}),
                    "current_fail_per_sec": report_data.get("current_fail_per_sec", {})
                }
                
                # Calcular m√©tricas derivadas
                metrics["error_rate"] = (
                    metrics["total_failures"] / metrics["total_requests"]
                    if metrics["total_requests"] > 0 else 0
                )
                metrics["success_rate"] = 1 - metrics["error_rate"]
                
                return metrics
                
            except Exception as e:
                print(f"‚ùå [REPORTS_GENERATE] Erro ao processar relat√≥rio: {str(e)}")
                return {
                    "test_type": test_type,
                    "success": False,
                    "error": f"Erro ao processar relat√≥rio: {str(e)}",
                    "timestamp": datetime.now().isoformat()
                }
        else:
            return {
                "test_type": test_type,
                "success": False,
                "error": "Relat√≥rio JSON n√£o encontrado",
                "timestamp": datetime.now().isoformat()
            }
    
    def _validate_thresholds(self, results: Dict[str, Any], test_type: str) -> Dict[str, Any]:
        """Validar thresholds de qualidade"""
        if not results.get("success", False):
            return {
                "passed": False,
                "errors": [results.get("error", "Teste falhou")]
            }
        
        errors = []
        warnings = []
        
        # Validar response time P95
        p95_response_time = results.get("response_time_percentiles", {}).get("95", 0)
        if p95_response_time > QUALITY_THRESHOLDS["response_time_p95"]:
            errors.append(f"P95 response time ({p95_response_time}ms) excede threshold ({QUALITY_THRESHOLDS['response_time_p95']}ms)")
        
        # Validar response time P99
        p99_response_time = results.get("response_time_percentiles", {}).get("99", 0)
        if p99_response_time > QUALITY_THRESHOLDS["response_time_p99"]:
            errors.append(f"P99 response time ({p99_response_time}ms) excede threshold ({QUALITY_THRESHOLDS['response_time_p99']}ms)")
        
        # Validar error rate
        error_rate = results.get("error_rate", 0)
        if error_rate > QUALITY_THRESHOLDS["error_rate"]:
            errors.append(f"Error rate ({error_rate:.2%}) excede threshold ({QUALITY_THRESHOLDS['error_rate']:.2%})")
        
        # Validar requests per second
        rps = results.get("total_rps", 0)
        if rps < QUALITY_THRESHOLDS["requests_per_second"]:
            warnings.append(f"RPS ({rps:.2f}) abaixo do threshold ({QUALITY_THRESHOLDS['requests_per_second']})")
        
        # Validar success rate
        success_rate = results.get("success_rate", 0)
        if success_rate < QUALITY_THRESHOLDS["success_rate"]:
            errors.append(f"Success rate ({success_rate:.2%}) abaixo do threshold ({QUALITY_THRESHOLDS['success_rate']:.2%})")
        
        return {
            "passed": len(errors) == 0,
            "errors": errors,
            "warnings": warnings,
            "thresholds": QUALITY_THRESHOLDS
        }
    
    def _save_results(self, results: Dict[str, Any], test_type: str):
        """Salvar resultados do teste"""
        timestamp = datetime.now().strftime("%Y%m%d_%H%M%S")
        filename = f"reports_generate_{test_type}_{timestamp}.json"
        filepath = REPORTS_DIR / "reports_generate" / filename
        
        try:
            with open(filepath, 'w') as f:
                json.dump(results, f, indent=2, default=str)
            
            print(f"üíæ [REPORTS_GENERATE] Resultados salvos: {filepath}")
            
        except Exception as e:
            print(f"‚ùå [REPORTS_GENERATE] Erro ao salvar resultados: {str(e)}")
    
    def _generate_report(self, results: Dict[str, Any], test_type: str):
        """Gerar relat√≥rio executivo"""
        timestamp = datetime.now().strftime("%Y%m%d_%H%M%S")
        filename = f"reports_generate_{test_type}_report_{timestamp}.md"
        filepath = REPORTS_DIR / "reports_generate" / filename
        
        try:
            report_content = self._create_report_content(results, test_type)
            
            with open(filepath, 'w', encoding='utf-8') as f:
                f.write(report_content)
            
            print(f"üìÑ [REPORTS_GENERATE] Relat√≥rio gerado: {filepath}")
            
        except Exception as e:
            print(f"‚ùå [REPORTS_GENERATE] Erro ao gerar relat√≥rio: {str(e)}")
    
    def _create_report_content(self, results: Dict[str, Any], test_type: str) -> str:
        """Criar conte√∫do do relat√≥rio"""
        content = f"""# üìä Relat√≥rio de Teste de Carga - Gera√ß√£o de Relat√≥rios

**Tracing ID**: {self.tracing_id}  
**Data/Hora**: {datetime.now().strftime('%Y-%m-%d %H:%M:%S')}  
**Tipo de Teste**: {test_type.upper()}  
**Vers√£o**: 1.0  

## üéØ Resumo Executivo

### Status do Teste
- **Sucesso**: {'‚úÖ PASSOU' if results.get('success') else '‚ùå FALHOU'}
- **Dura√ß√£o**: {(self.end_time - self.start_time).total_seconds():.2f} segundos
- **Tipo**: {test_type}

### M√©tricas Principais
- **Total de Requests**: {results.get('total_requests', 0):,}
- **Total de Falhas**: {results.get('total_failures', 0):,}
- **Taxa de Erro**: {results.get('error_rate', 0):.2%}
- **Taxa de Sucesso**: {results.get('success_rate', 0):.2%}
- **RPS M√©dio**: {results.get('total_rps', 0):.2f}

### Response Times
- **M√©dio**: {results.get('total_avg_response_time', 0):.2f}ms
- **Mediana**: {results.get('total_median_response_time', 0):.2f}ms
- **M√≠nimo**: {results.get('total_min_response_time', 0):.2f}ms
- **M√°ximo**: {results.get('total_max_response_time', 0):.2f}ms

### Percentis
"""
        
        # Adicionar percentis
        percentiles = results.get('response_time_percentiles', {})
        for p in [50, 75, 90, 95, 99]:
            value = percentiles.get(str(p), 0)
            content += f"- **P{p}**: {value:.2f}ms\n"
        
        content += f"""
## üìà An√°lise Detalhada

### Configura√ß√£o do Teste
- **Base URL**: {self.config['base_url']}
- **Usu√°rios**: {self.config['users']}
- **Spawn Rate**: {self.config['spawn_rate']}
- **Dura√ß√£o**: {self.config['run_time']}

### Thresholds de Qualidade
"""
        
        for threshold, value in QUALITY_THRESHOLDS.items():
            content += f"- **{threshold}**: {value}\n"
        
        content += f"""
### Valida√ß√£o de Thresholds
"""
        
        validation = self._validate_thresholds(results, test_type)
        if validation["passed"]:
            content += "‚úÖ **Todos os thresholds foram atendidos**\n"
        else:
            content += "‚ùå **Alguns thresholds n√£o foram atendidos:**\n"
            for error in validation["errors"]:
                content += f"- {error}\n"
        
        if validation["warnings"]:
            content += "\n‚ö†Ô∏è **Avisos:**\n"
            for warning in validation["warnings"]:
                content += f"- {warning}\n"
        
        content += f"""
## üîß Recomenda√ß√µes

### Performance
- Analisar otimiza√ß√µes para response times altos
- Considerar cache para relat√≥rios frequentes
- Implementar processamento ass√≠ncrono para relat√≥rios complexos

### Escalabilidade
- Monitorar uso de recursos durante picos
- Considerar auto-scaling baseado em demanda
- Implementar rate limiting inteligente

### Qualidade
- Revisar tratamento de erros
- Melhorar valida√ß√£o de entrada
- Implementar retry logic para falhas tempor√°rias

## üìã Pr√≥ximos Passos

1. **An√°lise de Performance**: Investigar gargalos identificados
2. **Otimiza√ß√£o**: Implementar melhorias baseadas nos resultados
3. **Monitoramento**: Configurar alertas para m√©tricas cr√≠ticas
4. **Testes Regulares**: Agendar execu√ß√£o peri√≥dica dos testes

---
**Gerado automaticamente pelo sistema de testes de carga**  
**Tracing ID**: {self.tracing_id}
"""
        
        return content
    
    def run_all_tests(self) -> Dict[str, Any]:
        """Executar todos os tipos de teste"""
        print(f"\nüöÄ [REPORTS_GENERATE] Executando suite completa de testes")
        
        test_types = ["normal", "complex", "batch", "scheduled", "stress"]
        all_results = {}
        
        for test_type in test_types:
            print(f"\n{'='*60}")
            print(f"üß™ Executando teste: {test_type.upper()}")
            print(f"{'='*60}")
            
            result = self.run_test(test_type)
            all_results[test_type] = result
            
            # Aguardar entre testes
            if test_type != test_types[-1]:
                print(f"‚è≥ Aguardando 30 segundos antes do pr√≥ximo teste...")
                time.sleep(30)
        
        # Gerar relat√≥rio consolidado
        self._generate_consolidated_report(all_results)
        
        return all_results
    
    def _generate_consolidated_report(self, all_results: Dict[str, Any]):
        """Gerar relat√≥rio consolidado de todos os testes"""
        timestamp = datetime.now().strftime("%Y%m%d_%H%M%S")
        filename = f"reports_generate_consolidated_report_{timestamp}.md"
        filepath = REPORTS_DIR / "reports_generate" / filename
        
        try:
            content = self._create_consolidated_report_content(all_results)
            
            with open(filepath, 'w', encoding='utf-8') as f:
                f.write(content)
            
            print(f"\nüìÑ [REPORTS_GENERATE] Relat√≥rio consolidado gerado: {filepath}")
            
        except Exception as e:
            print(f"‚ùå [REPORTS_GENERATE] Erro ao gerar relat√≥rio consolidado: {str(e)}")
    
    def _create_consolidated_report_content(self, all_results: Dict[str, Any]) -> str:
        """Criar conte√∫do do relat√≥rio consolidado"""
        content = f"""# üìä Relat√≥rio Consolidado - Testes de Carga Gera√ß√£o de Relat√≥rios

**Tracing ID**: {self.tracing_id}  
**Data/Hora**: {datetime.now().strftime('%Y-%m-%d %H:%M:%S')}  
**Vers√£o**: 1.0  

## üéØ Resumo Executivo

### Status Geral
"""
        
        total_tests = len(all_results)
        passed_tests = sum(1 for r in all_results.values() if r.get("success", False))
        
        content += f"""
- **Total de Testes**: {total_tests}
- **Testes Aprovados**: {passed_tests}
- **Testes Reprovados**: {total_tests - passed_tests}
- **Taxa de Sucesso**: {passed_tests/total_tests:.2%}
"""

        content += """
### Resumo por Tipo de Teste
| Tipo | Status | Dura√ß√£o | Requests | Taxa de Erro | RPS |
|------|--------|---------|----------|--------------|-----|
"""
        
        for test_type, result in all_results.items():
            results_data = result.get("results", {})
            status = "‚úÖ PASSOU" if result.get("success", False) else "‚ùå FALHOU"
            duration = f"{result.get('duration', 0):.2f}s"
            requests = f"{results_data.get('total_requests', 0):,}"
            error_rate = f"{results_data.get('error_rate', 0):.2%}"
            rps = f"{results_data.get('total_rps', 0):.2f}"
            
            content += f"| {test_type.upper()} | {status} | {duration} | {requests} | {error_rate} | {rps} |\n"
        
        content += f"""
## üìà An√°lise Detalhada por Teste

"""
        
        for test_type, result in all_results.items():
            content += f"### {test_type.upper()}\n\n"
            
            if result.get("success", False):
                results_data = result.get("results", {})
                validation = result.get("validation", {})
                
                content += f"**Status**: ‚úÖ PASSOU\n"
                content += f"**Dura√ß√£o**: {result.get('duration', 0):.2f} segundos\n\n"
                
                content += "**M√©tricas Principais**:\n"
                content += f"- Requests: {results_data.get('total_requests', 0):,}\n"
                content += f"- Falhas: {results_data.get('total_failures', 0):,}\n"
                content += f"- Taxa de Erro: {results_data.get('error_rate', 0):.2%}\n"
                content += f"- RPS: {results_data.get('total_rps', 0):.2f}\n"
                content += f"- Response Time M√©dio: {results_data.get('total_avg_response_time', 0):.2f}ms\n\n"
                
                if validation.get("warnings"):
                    content += "**Avisos**:\n"
                    for warning in validation["warnings"]:
                        content += f"- {warning}\n"
                    content += "\n"
            else:
                content += f"**Status**: ‚ùå FALHOU\n"
                content += f"**Erro**: {result.get('results', {}).get('error', 'Erro desconhecido')}\n\n"
            
            content += "---\n\n"
        
        content += f"""
## üîß Recomenda√ß√µes Gerais

### Performance
- Implementar cache para relat√≥rios frequentes
- Otimizar queries de banco de dados
- Considerar processamento ass√≠ncrono

### Escalabilidade
- Implementar auto-scaling
- Otimizar uso de recursos
- Melhorar rate limiting

### Qualidade
- Revisar tratamento de erros
- Implementar retry logic
- Melhorar valida√ß√£o de entrada

## üìã Pr√≥ximos Passos

1. **An√°lise**: Investigar gargalos identificados
2. **Otimiza√ß√£o**: Implementar melhorias
3. **Monitoramento**: Configurar alertas
4. **Testes Regulares**: Agendar execu√ß√£o peri√≥dica

---
**Gerado automaticamente pelo sistema de testes de carga**  
**Tracing ID**: {self.tracing_id}
"""
        
        return content

def main():
    """Fun√ß√£o principal"""
    parser = argparse.ArgumentParser(description="Executor de testes de carga para gera√ß√£o de relat√≥rios")
    parser.add_argument("--test-type", choices=["normal", "complex", "batch", "scheduled", "stress", "all"],
                       default="all", help="Tipo de teste a executar")
    parser.add_argument("--users", type=int, help="N√∫mero de usu√°rios")
    parser.add_argument("--spawn-rate", type=int, help="Taxa de spawn")
    parser.add_argument("--run-time", help="Tempo de execu√ß√£o")
    parser.add_argument("--base-url", help="URL base da aplica√ß√£o")
    
    args = parser.parse_args()
    
    # Configura√ß√£o
    config = {}
    if args.users:
        config["users"] = args.users
    if args.spawn_rate:
        config["spawn_rate"] = args.spawn_rate
    if args.run_time:
        config["run_time"] = args.run_time
    if args.base_url:
        config["base_url"] = args.base_url
        config["host"] = args.base_url
    
    # Executar testes
    runner = ReportsGenerateLoadTestRunner(config)
    
    if args.test_type == "all":
        results = runner.run_all_tests()
    else:
        results = runner.run_test(args.test_type)
    
    # Resumo final
    print(f"\n{'='*60}")
    print(f"üéâ EXECU√á√ÉO CONCLU√çDA")
    print(f"{'='*60}")
    
    if isinstance(results, dict) and "test_type" in results:
        # Teste √∫nico
        success = results.get("success", False)
        print(f"‚úÖ Status: {'PASSOU' if success else 'FALHOU'}")
        print(f"üìä Tipo: {results.get('test_type', 'unknown')}")
        print(f"‚è±Ô∏è Dura√ß√£o: {results.get('duration', 0):.2f}s")
    else:
        # M√∫ltiplos testes
        total_tests = len(results)
        passed_tests = sum(1 for r in results.values() if r.get("success", False))
        print(f"üìä Total de Testes: {total_tests}")
        print(f"‚úÖ Aprovados: {passed_tests}")
        print(f"‚ùå Reprovados: {total_tests - passed_tests}")
        print(f"üìà Taxa de Sucesso: {passed_tests/total_tests:.2%}")
    
    print(f"üìã Tracing ID: {runner.tracing_id}")
    print(f"üìÅ Relat√≥rios: {REPORTS_DIR / 'reports_generate'}")
    print(f"üìù Logs: {LOGS_DIR / 'reports_generate'}")

if __name__ == "__main__":
    main() 