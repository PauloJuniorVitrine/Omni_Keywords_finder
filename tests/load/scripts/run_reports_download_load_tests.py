#!/usr/bin/env python3
"""
Script de ExecuÃ§Ã£o - Testes de Carga para Download de RelatÃ³rios
Baseado em: /api/reports/download

Tracing ID: LOAD_TEST_REPORTS_DOWNLOAD_SCRIPT_20250127_001
Data/Hora: 2025-01-27 19:30:00 UTC
VersÃ£o: 1.0
Ruleset: enterprise_control_layer.yaml

Funcionalidades:
- ExecuÃ§Ã£o de testes de download de relatÃ³rios
- ValidaÃ§Ã£o de mÃ©tricas de performance
- GeraÃ§Ã£o de relatÃ³rios de teste
- Monitoramento de recursos
- AnÃ¡lise de resultados
"""

import os
import sys
import json
import time
import subprocess
import argparse
from datetime import datetime, timedelta
from pathlib import Path
from typing import Dict, Any, List, Optional
import logging

# ConfiguraÃ§Ã£o de logging
logging.basicConfig(
    level=logging.INFO,
    format='%(asctime)s - %(name)s - %(levelname)s - %(message)s',
    handlers=[
        logging.FileHandler('logs/reports_download_load_tests.log'),
        logging.StreamHandler()
    ]
)
logger = logging.getLogger(__name__)

class ReportsDownloadLoadTestRunner:
    """
    Executor de testes de carga para download de relatÃ³rios
    """
    
    def __init__(self, config: Dict[str, Any]):
        self.config = config
        self.test_results = {}
        self.start_time = None
        self.end_time = None
        
        # ConfiguraÃ§Ãµes baseadas no cÃ³digo real
        self.test_scenarios = {
            "normal": {
                "users": 10,
                "spawn_rate": 2,
                "run_time": "5m",
                "description": "Download normal de relatÃ³rios"
            },
            "batch": {
                "users": 5,
                "spawn_rate": 1,
                "run_time": "3m",
                "description": "Download em lote de relatÃ³rios"
            },
            "scheduled": {
                "users": 3,
                "spawn_rate": 1,
                "run_time": "2m",
                "description": "Download de relatÃ³rios agendados"
            },
            "large": {
                "users": 2,
                "spawn_rate": 1,
                "run_time": "3m",
                "description": "Download de relatÃ³rios grandes"
            },
            "stress": {
                "users": 20,
                "spawn_rate": 5,
                "run_time": "10m",
                "description": "Teste de stress para downloads"
            },
            "error_scenarios": {
                "users": 5,
                "spawn_rate": 1,
                "run_time": "2m",
                "description": "CenÃ¡rios de erro para downloads"
            }
        }
        
        # MÃ©tricas de validaÃ§Ã£o baseadas no cÃ³digo real
        self.validation_metrics = {
            "response_time_p95": 2000,  # 2 segundos
            "response_time_p99": 5000,  # 5 segundos
            "error_rate": 0.05,  # 5%
            "throughput_min": 10,  # 10 requests/segundo
            "download_speed_min": 10000,  # 10KB/s
            "file_size_validation": True,
            "mime_type_validation": True,
            "compression_validation": True
        }

    def setup_environment(self):
        """ConfiguraÃ§Ã£o do ambiente de teste"""
        logger.info("ğŸ”§ Configurando ambiente de teste...")
        
        try:
            # Criar diretÃ³rios necessÃ¡rios
            directories = [
                "logs",
                "reports",
                "downloads",
                "metrics",
                "artifacts"
            ]
            
            for directory in directories:
                Path(directory).mkdir(exist_ok=True)
            
            # Verificar dependÃªncias
            self._check_dependencies()
            
            # Configurar variÃ¡veis de ambiente
            os.environ["LOAD_TEST_ENVIRONMENT"] = "reports_download"
            os.environ["LOAD_TEST_TIMESTAMP"] = datetime.now().isoformat()
            
            logger.info("âœ… Ambiente configurado com sucesso")
            
        except Exception as e:
            logger.error(f"âŒ Erro na configuraÃ§Ã£o do ambiente: {e}")
            raise

    def _check_dependencies(self):
        """Verificar dependÃªncias necessÃ¡rias"""
        dependencies = ["locust", "requests", "psutil"]
        
        for dep in dependencies:
            try:
                __import__(dep)
                logger.info(f"âœ… DependÃªncia {dep} encontrada")
            except ImportError:
                logger.error(f"âŒ DependÃªncia {dep} nÃ£o encontrada")
                raise ImportError(f"DependÃªncia {dep} nÃ£o estÃ¡ instalada")

    def run_test_scenario(self, scenario_name: str) -> Dict[str, Any]:
        """
        Executar cenÃ¡rio de teste especÃ­fico
        """
        logger.info(f"ğŸš€ Executando cenÃ¡rio: {scenario_name}")
        
        if scenario_name not in self.test_scenarios:
            raise ValueError(f"CenÃ¡rio {scenario_name} nÃ£o encontrado")
        
        scenario_config = self.test_scenarios[scenario_name]
        
        # Configurar comando Locust
        locust_file = "tests/load/high/analytics/locustfile_reports_download_v1.py"
        
        cmd = [
            "locust",
            "-f", locust_file,
            "--users", str(scenario_config["users"]),
            "--spawn-rate", str(scenario_config["spawn_rate"]),
            "--run-time", scenario_config["run_time"],
            "--headless",
            "--html", f"reports/reports_download_{scenario_name}_{datetime.now().strftime('%Y%m%d_%H%M%S')}.html",
            "--json", f"metrics/reports_download_{scenario_name}_{datetime.now().strftime('%Y%m%d_%H%M%S')}.json",
            "--csv", f"metrics/reports_download_{scenario_name}_{datetime.now().strftime('%Y%m%d_%H%M%S')}",
            "--host", self.config.get("host", "http://localhost:8000")
        ]
        
        # Executar teste
        start_time = time.time()
        
        try:
            result = subprocess.run(
                cmd,
                capture_output=True,
                text=True,
                timeout=int(scenario_config["run_time"].replace("m", "")) * 60 + 300  # +5 min buffer
            )
            
            end_time = time.time()
            
            # Processar resultados
            test_result = {
                "scenario": scenario_name,
                "start_time": datetime.fromtimestamp(start_time).isoformat(),
                "end_time": datetime.fromtimestamp(end_time).isoformat(),
                "duration": end_time - start_time,
                "return_code": result.returncode,
                "stdout": result.stdout,
                "stderr": result.stderr,
                "config": scenario_config
            }
            
            # Validar resultados
            test_result["validation"] = self._validate_test_results(test_result)
            
            logger.info(f"âœ… CenÃ¡rio {scenario_name} executado com sucesso")
            return test_result
            
        except subprocess.TimeoutExpired:
            logger.error(f"âŒ Timeout no cenÃ¡rio {scenario_name}")
            return {
                "scenario": scenario_name,
                "error": "Timeout",
                "start_time": datetime.fromtimestamp(start_time).isoformat(),
                "end_time": datetime.now().isoformat(),
                "duration": time.time() - start_time
            }
        except Exception as e:
            logger.error(f"âŒ Erro no cenÃ¡rio {scenario_name}: {e}")
            return {
                "scenario": scenario_name,
                "error": str(e),
                "start_time": datetime.fromtimestamp(start_time).isoformat(),
                "end_time": datetime.now().isoformat(),
                "duration": time.time() - start_time
            }

    def _validate_test_results(self, test_result: Dict[str, Any]) -> Dict[str, Any]:
        """
        Validar resultados do teste baseado no cÃ³digo real
        """
        validation = {
            "passed": True,
            "errors": [],
            "warnings": [],
            "metrics": {}
        }
        
        try:
            # Verificar se o teste executou com sucesso
            if test_result.get("return_code", 1) != 0:
                validation["passed"] = False
                validation["errors"].append("Teste falhou com cÃ³digo de retorno nÃ£o-zero")
            
            # Analisar stdout para mÃ©tricas
            stdout = test_result.get("stdout", "")
            
            # Extrair mÃ©tricas bÃ¡sicas
            if "requests" in stdout:
                # Procurar por padrÃµes de mÃ©tricas no output
                lines = stdout.split("\n")
                for line in lines:
                    if "requests/sec" in line:
                        try:
                            throughput = float(line.split()[0])
                            validation["metrics"]["throughput"] = throughput
                            
                            if throughput < self.validation_metrics["throughput_min"]:
                                validation["warnings"].append(f"Throughput baixo: {throughput} req/s")
                        except:
                            pass
                    
                    elif "response time" in line.lower():
                        try:
                            # Extrair tempo de resposta
                            response_time = float(line.split()[0])
                            validation["metrics"]["response_time"] = response_time
                            
                            if response_time > self.validation_metrics["response_time_p95"]:
                                validation["warnings"].append(f"Tempo de resposta alto: {response_time}ms")
                        except:
                            pass
                    
                    elif "failures" in line.lower():
                        try:
                            # Extrair taxa de erro
                            error_rate = float(line.split()[0].replace("%", "")) / 100
                            validation["metrics"]["error_rate"] = error_rate
                            
                            if error_rate > self.validation_metrics["error_rate"]:
                                validation["errors"].append(f"Taxa de erro alta: {error_rate*100}%")
                        except:
                            pass
            
            # Validar arquivos de saÃ­da
            csv_files = list(Path("metrics").glob(f"reports_download_{test_result['scenario']}_*.csv"))
            if not csv_files:
                validation["warnings"].append("Arquivo CSV de mÃ©tricas nÃ£o encontrado")
            
            html_files = list(Path("reports").glob(f"reports_download_{test_result['scenario']}_*.html"))
            if not html_files:
                validation["warnings"].append("RelatÃ³rio HTML nÃ£o encontrado")
            
            # Validar mÃ©tricas especÃ­ficas de download
            validation["metrics"]["download_validation"] = {
                "file_size_validation": self.validation_metrics["file_size_validation"],
                "mime_type_validation": self.validation_metrics["mime_type_validation"],
                "compression_validation": self.validation_metrics["compression_validation"]
            }
            
        except Exception as e:
            validation["passed"] = False
            validation["errors"].append(f"Erro na validaÃ§Ã£o: {e}")
        
        return validation

    def run_all_scenarios(self) -> Dict[str, Any]:
        """
        Executar todos os cenÃ¡rios de teste
        """
        logger.info("ğŸš€ Iniciando execuÃ§Ã£o de todos os cenÃ¡rios...")
        
        self.start_time = datetime.now()
        all_results = {}
        
        # Executar cenÃ¡rios em ordem de prioridade
        scenario_order = ["normal", "batch", "scheduled", "large", "error_scenarios", "stress"]
        
        for scenario in scenario_order:
            logger.info(f"ğŸ“‹ Executando cenÃ¡rio: {scenario}")
            
            try:
                result = self.run_test_scenario(scenario)
                all_results[scenario] = result
                
                # Aguardar entre cenÃ¡rios
                time.sleep(30)
                
            except Exception as e:
                logger.error(f"âŒ Erro no cenÃ¡rio {scenario}: {e}")
                all_results[scenario] = {
                    "scenario": scenario,
                    "error": str(e),
                    "start_time": datetime.now().isoformat(),
                    "end_time": datetime.now().isoformat()
                }
        
        self.end_time = datetime.now()
        
        # Gerar relatÃ³rio consolidado
        consolidated_report = self._generate_consolidated_report(all_results)
        
        logger.info("âœ… ExecuÃ§Ã£o de todos os cenÃ¡rios concluÃ­da")
        return consolidated_report

    def _generate_consolidated_report(self, all_results: Dict[str, Any]) -> Dict[str, Any]:
        """
        Gerar relatÃ³rio consolidado dos testes
        """
        logger.info("ğŸ“Š Gerando relatÃ³rio consolidado...")
        
        # EstatÃ­sticas gerais
        total_scenarios = len(all_results)
        successful_scenarios = sum(1 for r in all_results.values() if not r.get("error"))
        failed_scenarios = total_scenarios - successful_scenarios
        
        # MÃ©tricas agregadas
        total_duration = sum(r.get("duration", 0) for r in all_results.values())
        avg_duration = total_duration / total_scenarios if total_scenarios > 0 else 0
        
        # ValidaÃ§Ãµes
        total_validations = sum(1 for r in all_results.values() if "validation" in r)
        passed_validations = sum(1 for r in all_results.values() 
                               if "validation" in r and r["validation"].get("passed", False))
        
        consolidated_report = {
            "test_info": {
                "test_type": "reports_download_load_test",
                "start_time": self.start_time.isoformat(),
                "end_time": self.end_time.isoformat(),
                "total_duration": (self.end_time - self.start_time).total_seconds(),
                "tracing_id": "LOAD_TEST_REPORTS_DOWNLOAD_SCRIPT_20250127_001",
                "version": "1.0"
            },
            "summary": {
                "total_scenarios": total_scenarios,
                "successful_scenarios": successful_scenarios,
                "failed_scenarios": failed_scenarios,
                "success_rate": (successful_scenarios / total_scenarios * 100) if total_scenarios > 0 else 0,
                "total_duration": total_duration,
                "avg_duration": avg_duration,
                "validation_passed": passed_validations,
                "validation_total": total_validations,
                "validation_rate": (passed_validations / total_validations * 100) if total_validations > 0 else 0
            },
            "scenarios": all_results,
            "recommendations": self._generate_recommendations(all_results),
            "next_steps": self._generate_next_steps(all_results)
        }
        
        # Salvar relatÃ³rio
        report_file = f"reports/reports_download_consolidated_{datetime.now().strftime('%Y%m%d_%H%M%S')}.json"
        with open(report_file, 'w', encoding='utf-8') as f:
            json.dump(consolidated_report, f, indent=2, ensure_ascii=False, default=str)
        
        logger.info(f"ğŸ“„ RelatÃ³rio consolidado salvo: {report_file}")
        return consolidated_report

    def _generate_recommendations(self, all_results: Dict[str, Any]) -> List[str]:
        """
        Gerar recomendaÃ§Ãµes baseadas nos resultados
        """
        recommendations = []
        
        # AnÃ¡lise de performance
        for scenario, result in all_results.items():
            if "validation" in result:
                validation = result["validation"]
                
                if not validation.get("passed", True):
                    recommendations.append(f"ğŸ”§ Otimizar cenÃ¡rio {scenario}: {', '.join(validation.get('errors', []))}")
                
                if validation.get("warnings"):
                    recommendations.append(f"âš ï¸ Revisar cenÃ¡rio {scenario}: {', '.join(validation.get('warnings', []))}")
        
        # RecomendaÃ§Ãµes gerais baseadas no cÃ³digo real
        recommendations.extend([
            "ğŸ“Š Monitorar uso de memÃ³ria durante downloads grandes",
            "ğŸ”’ Validar controle de acesso para downloads sensÃ­veis",
            "âš¡ Implementar cache para downloads frequentes",
            "ğŸ“ˆ Otimizar compressÃ£o para diferentes tipos de arquivo",
            "ğŸ›¡ï¸ ReforÃ§ar validaÃ§Ã£o de tipos MIME",
            "ğŸ“‹ Implementar logs detalhados de download"
        ])
        
        return recommendations

    def _generate_next_steps(self, all_results: Dict[str, Any]) -> List[str]:
        """
        Gerar prÃ³ximos passos baseados nos resultados
        """
        next_steps = [
            "ğŸš€ Implementar testes de agendamento de relatÃ³rios",
            "ğŸ“Š Implementar testes de mÃ©tricas detalhadas",
            "ğŸ”§ Implementar testes de infraestrutura",
            "âš¡ Implementar testes de performance especÃ­ficos",
            "ğŸŒ Implementar testes de ambiente",
            "ğŸ¯ Implementar testes de chaos engineering",
            "ğŸ“ˆ Implementar testes de monitoramento",
            "ğŸ›¡ï¸ Implementar testes de compliance",
            "ğŸ® Implementar testes de gamificaÃ§Ã£o"
        ]
        
        return next_steps

def main():
    """FunÃ§Ã£o principal"""
    parser = argparse.ArgumentParser(description="Executor de testes de carga para Download de RelatÃ³rios")
    parser.add_argument("--scenario", help="CenÃ¡rio especÃ­fico para executar")
    parser.add_argument("--host", default="http://localhost:8000", help="Host do sistema")
    parser.add_argument("--config", help="Arquivo de configuraÃ§Ã£o JSON")
    parser.add_argument("--all", action="store_true", help="Executar todos os cenÃ¡rios")
    
    args = parser.parse_args()
    
    # ConfiguraÃ§Ã£o padrÃ£o
    config = {
        "host": args.host,
        "timeout": 300,
        "retries": 3
    }
    
    # Carregar configuraÃ§Ã£o customizada se fornecida
    if args.config:
        try:
            with open(args.config, 'r') as f:
                custom_config = json.load(f)
                config.update(custom_config)
        except Exception as e:
            logger.error(f"Erro ao carregar configuraÃ§Ã£o: {e}")
            return 1
    
    # Criar executor
    runner = ReportsDownloadLoadTestRunner(config)
    
    try:
        # Configurar ambiente
        runner.setup_environment()
        
        if args.scenario:
            # Executar cenÃ¡rio especÃ­fico
            logger.info(f"ğŸ¯ Executando cenÃ¡rio especÃ­fico: {args.scenario}")
            result = runner.run_test_scenario(args.scenario)
            print(json.dumps(result, indent=2, ensure_ascii=False, default=str))
            
        elif args.all:
            # Executar todos os cenÃ¡rios
            logger.info("ğŸš€ Executando todos os cenÃ¡rios")
            report = runner.run_all_scenarios()
            print(json.dumps(report, indent=2, ensure_ascii=False, default=str))
            
        else:
            # Executar cenÃ¡rio padrÃ£o (normal)
            logger.info("ğŸ¯ Executando cenÃ¡rio padrÃ£o: normal")
            result = runner.run_test_scenario("normal")
            print(json.dumps(result, indent=2, ensure_ascii=False, default=str))
        
        logger.info("âœ… ExecuÃ§Ã£o concluÃ­da com sucesso")
        return 0
        
    except Exception as e:
        logger.error(f"âŒ Erro na execuÃ§Ã£o: {e}")
        return 1

if __name__ == "__main__":
    sys.exit(main()) 