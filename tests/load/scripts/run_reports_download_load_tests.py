#!/usr/bin/env python3
"""
Script de Execu√ß√£o - Testes de Carga para Download de Relat√≥rios
Baseado em: /api/reports/download

Tracing ID: LOAD_TEST_REPORTS_DOWNLOAD_SCRIPT_20250127_001
Data/Hora: 2025-01-27 19:30:00 UTC
Vers√£o: 1.0
Ruleset: enterprise_control_layer.yaml

Funcionalidades:
- Execu√ß√£o de testes de download de relat√≥rios
- Valida√ß√£o de m√©tricas de performance
- Gera√ß√£o de relat√≥rios de teste
- Monitoramento de recursos
- An√°lise de resultados
"""

import os
import sys
import json
import time
import subprocess
import argparse
from datetime import datetime, timedelta
from pathlib import Path
from typing import Dict, Any, List, Optional
import logging

# Configura√ß√£o de logging
logging.basicConfig(
    level=logging.INFO,
    format='%(asctime)s - %(name)s - %(levelname)s - %(message)s',
    handlers=[
        logging.FileHandler('logs/reports_download_load_tests.log'),
        logging.StreamHandler()
    ]
)
logger = logging.getLogger(__name__)

class ReportsDownloadLoadTestRunner:
    """
    Executor de testes de carga para download de relat√≥rios
    """
    
    def __init__(self, config: Dict[str, Any]):
        self.config = config
        self.test_results = {}
        self.start_time = None
        self.end_time = None
        
        # Configura√ß√µes baseadas no c√≥digo real
        self.test_scenarios = {
            "normal": {
                "users": 10,
                "spawn_rate": 2,
                "run_time": "5m",
                "description": "Download normal de relat√≥rios"
            },
            "batch": {
                "users": 5,
                "spawn_rate": 1,
                "run_time": "3m",
                "description": "Download em lote de relat√≥rios"
            },
            "scheduled": {
                "users": 3,
                "spawn_rate": 1,
                "run_time": "2m",
                "description": "Download de relat√≥rios agendados"
            },
            "large": {
                "users": 2,
                "spawn_rate": 1,
                "run_time": "3m",
                "description": "Download de relat√≥rios grandes"
            },
            "stress": {
                "users": 20,
                "spawn_rate": 5,
                "run_time": "10m",
                "description": "Teste de stress para downloads"
            },
            "error_scenarios": {
                "users": 5,
                "spawn_rate": 1,
                "run_time": "2m",
                "description": "Cen√°rios de erro para downloads"
            }
        }
        
        # M√©tricas de valida√ß√£o baseadas no c√≥digo real
        self.validation_metrics = {
            "response_time_p95": 2000,  # 2 segundos
            "response_time_p99": 5000,  # 5 segundos
            "error_rate": 0.05,  # 5%
            "throughput_min": 10,  # 10 requests/segundo
            "download_speed_min": 10000,  # 10KB/s
            "file_size_validation": True,
            "mime_type_validation": True,
            "compression_validation": True
        }

    def setup_environment(self):
        """Configura√ß√£o do ambiente de teste"""
        logger.info("üîß Configurando ambiente de teste...")
        
        try:
            # Criar diret√≥rios necess√°rios
            directories = [
                "logs",
                "reports",
                "downloads",
                "metrics",
                "artifacts"
            ]
            
            for directory in directories:
                Path(directory).mkdir(exist_ok=True)
            
            # Verificar depend√™ncias
            self._check_dependencies()
            
            # Configurar vari√°veis de ambiente
            os.environ["LOAD_TEST_ENVIRONMENT"] = "reports_download"
            os.environ["LOAD_TEST_TIMESTAMP"] = datetime.now().isoformat()
            
            logger.info("‚úÖ Ambiente configurado com sucesso")
            
        except Exception as e:
            logger.error(f"‚ùå Erro na configura√ß√£o do ambiente: {e}")
            raise

    def _check_dependencies(self):
        """Verificar depend√™ncias necess√°rias"""
        dependencies = ["locust", "requests", "psutil"]
        
        for dep in dependencies:
            try:
                __import__(dep)
                logger.info(f"‚úÖ Depend√™ncia {dep} encontrada")
            except ImportError:
                logger.error(f"‚ùå Depend√™ncia {dep} n√£o encontrada")
                raise ImportError(f"Depend√™ncia {dep} n√£o est√° instalada")

    def run_test_scenario(self, scenario_name: str) -> Dict[str, Any]:
        """
        Executar cen√°rio de teste espec√≠fico
        """
        logger.info(f"üöÄ Executando cen√°rio: {scenario_name}")
        
        if scenario_name not in self.test_scenarios:
            raise ValueError(f"Cen√°rio {scenario_name} n√£o encontrado")
        
        scenario_config = self.test_scenarios[scenario_name]
        
        # Configurar comando Locust
        locust_file = "tests/load/high/analytics/locustfile_reports_download_v1.py"
        
        cmd = [
            "locust",
            "-f", locust_file,
            "--users", str(scenario_config["users"]),
            "--spawn-rate", str(scenario_config["spawn_rate"]),
            "--run-time", scenario_config["run_time"],
            "--headless",
            "--html", f"reports/reports_download_{scenario_name}_{datetime.now().strftime('%Y%m%d_%H%M%S')}.html",
            "--json", f"metrics/reports_download_{scenario_name}_{datetime.now().strftime('%Y%m%d_%H%M%S')}.json",
            "--csv", f"metrics/reports_download_{scenario_name}_{datetime.now().strftime('%Y%m%d_%H%M%S')}",
            "--host", self.config.get("host", "http://localhost:8000")
        ]
        
        # Executar teste
        start_time = time.time()
        
        try:
            result = subprocess.run(
                cmd,
                capture_output=True,
                text=True,
                timeout=int(scenario_config["run_time"].replace("m", "")) * 60 + 300  # +5 min buffer
            )
            
            end_time = time.time()
            
            # Processar resultados
            test_result = {
                "scenario": scenario_name,
                "start_time": datetime.fromtimestamp(start_time).isoformat(),
                "end_time": datetime.fromtimestamp(end_time).isoformat(),
                "duration": end_time - start_time,
                "return_code": result.returncode,
                "stdout": result.stdout,
                "stderr": result.stderr,
                "config": scenario_config
            }
            
            # Validar resultados
            test_result["validation"] = self._validate_test_results(test_result)
            
            logger.info(f"‚úÖ Cen√°rio {scenario_name} executado com sucesso")
            return test_result
            
        except subprocess.TimeoutExpired:
            logger.error(f"‚ùå Timeout no cen√°rio {scenario_name}")
            return {
                "scenario": scenario_name,
                "error": "Timeout",
                "start_time": datetime.fromtimestamp(start_time).isoformat(),
                "end_time": datetime.now().isoformat(),
                "duration": time.time() - start_time
            }
        except Exception as e:
            logger.error(f"‚ùå Erro no cen√°rio {scenario_name}: {e}")
            return {
                "scenario": scenario_name,
                "error": str(e),
                "start_time": datetime.fromtimestamp(start_time).isoformat(),
                "end_time": datetime.now().isoformat(),
                "duration": time.time() - start_time
            }

    def _validate_test_results(self, test_result: Dict[str, Any]) -> Dict[str, Any]:
        """
        Validar resultados do teste baseado no c√≥digo real
        """
        validation = {
            "passed": True,
            "errors": [],
            "warnings": [],
            "metrics": {}
        }
        
        try:
            # Verificar se o teste executou com sucesso
            if test_result.get("return_code", 1) != 0:
                validation["passed"] = False
                validation["errors"].append("Teste falhou com c√≥digo de retorno n√£o-zero")
            
            # Analisar stdout para m√©tricas
            stdout = test_result.get("stdout", "")
            
            # Extrair m√©tricas b√°sicas
            if "requests" in stdout:
                # Procurar por padr√µes de m√©tricas no output
                lines = stdout.split("\n")
                for line in lines:
                    if "requests/sec" in line:
                        try:
                            throughput = float(line.split()[0])
                            validation["metrics"]["throughput"] = throughput
                            
                            if throughput < self.validation_metrics["throughput_min"]:
                                validation["warnings"].append(f"Throughput baixo: {throughput} req/s")
                        except:
                            pass
                    
                    elif "response time" in line.lower():
                        try:
                            # Extrair tempo de resposta
                            response_time = float(line.split()[0])
                            validation["metrics"]["response_time"] = response_time
                            
                            if response_time > self.validation_metrics["response_time_p95"]:
                                validation["warnings"].append(f"Tempo de resposta alto: {response_time}ms")
                        except:
                            pass
                    
                    elif "failures" in line.lower():
                        try:
                            # Extrair taxa de erro
                            error_rate = float(line.split()[0].replace("%", "")) / 100
                            validation["metrics"]["error_rate"] = error_rate
                            
                            if error_rate > self.validation_metrics["error_rate"]:
                                validation["errors"].append(f"Taxa de erro alta: {error_rate*100}%")
                        except:
                            pass
            
            # Validar arquivos de sa√≠da
            csv_files = list(Path("metrics").glob(f"reports_download_{test_result['scenario']}_*.csv"))
            if not csv_files:
                validation["warnings"].append("Arquivo CSV de m√©tricas n√£o encontrado")
            
            html_files = list(Path("reports").glob(f"reports_download_{test_result['scenario']}_*.html"))
            if not html_files:
                validation["warnings"].append("Relat√≥rio HTML n√£o encontrado")
            
            # Validar m√©tricas espec√≠ficas de download
            validation["metrics"]["download_validation"] = {
                "file_size_validation": self.validation_metrics["file_size_validation"],
                "mime_type_validation": self.validation_metrics["mime_type_validation"],
                "compression_validation": self.validation_metrics["compression_validation"]
            }
            
        except Exception as e:
            validation["passed"] = False
            validation["errors"].append(f"Erro na valida√ß√£o: {e}")
        
        return validation

    def run_all_scenarios(self) -> Dict[str, Any]:
        """
        Executar todos os cen√°rios de teste
        """
        logger.info("üöÄ Iniciando execu√ß√£o de todos os cen√°rios...")
        
        self.start_time = datetime.now()
        all_results = {}
        
        # Executar cen√°rios em ordem de prioridade
        scenario_order = ["normal", "batch", "scheduled", "large", "error_scenarios", "stress"]
        
        for scenario in scenario_order:
            logger.info(f"üìã Executando cen√°rio: {scenario}")
            
            try:
                result = self.run_test_scenario(scenario)
                all_results[scenario] = result
                
                # Aguardar entre cen√°rios
                time.sleep(30)
                
            except Exception as e:
                logger.error(f"‚ùå Erro no cen√°rio {scenario}: {e}")
                all_results[scenario] = {
                    "scenario": scenario,
                    "error": str(e),
                    "start_time": datetime.now().isoformat(),
                    "end_time": datetime.now().isoformat()
                }
        
        self.end_time = datetime.now()
        
        # Gerar relat√≥rio consolidado
        consolidated_report = self._generate_consolidated_report(all_results)
        
        logger.info("‚úÖ Execu√ß√£o de todos os cen√°rios conclu√≠da")
        return consolidated_report

    def _generate_consolidated_report(self, all_results: Dict[str, Any]) -> Dict[str, Any]:
        """
        Gerar relat√≥rio consolidado dos testes
        """
        logger.info("üìä Gerando relat√≥rio consolidado...")
        
        # Estat√≠sticas gerais
        total_scenarios = len(all_results)
        successful_scenarios = sum(1 for r in all_results.values() if not r.get("error"))
        failed_scenarios = total_scenarios - successful_scenarios
        
        # M√©tricas agregadas
        total_duration = sum(r.get("duration", 0) for r in all_results.values())
        avg_duration = total_duration / total_scenarios if total_scenarios > 0 else 0
        
        # Valida√ß√µes
        total_validations = sum(1 for r in all_results.values() if "validation" in r)
        passed_validations = sum(1 for r in all_results.values() 
                               if "validation" in r and r["validation"].get("passed", False))
        
        consolidated_report = {
            "test_info": {
                "test_type": "reports_download_load_test",
                "start_time": self.start_time.isoformat(),
                "end_time": self.end_time.isoformat(),
                "total_duration": (self.end_time - self.start_time).total_seconds(),
                "tracing_id": "LOAD_TEST_REPORTS_DOWNLOAD_SCRIPT_20250127_001",
                "version": "1.0"
            },
            "summary": {
                "total_scenarios": total_scenarios,
                "successful_scenarios": successful_scenarios,
                "failed_scenarios": failed_scenarios,
                "success_rate": (successful_scenarios / total_scenarios * 100) if total_scenarios > 0 else 0,
                "total_duration": total_duration,
                "avg_duration": avg_duration,
                "validation_passed": passed_validations,
                "validation_total": total_validations,
                "validation_rate": (passed_validations / total_validations * 100) if total_validations > 0 else 0
            },
            "scenarios": all_results,
            "recommendations": self._generate_recommendations(all_results),
            "next_steps": self._generate_next_steps(all_results)
        }
        
        # Salvar relat√≥rio
        report_file = f"reports/reports_download_consolidated_{datetime.now().strftime('%Y%m%d_%H%M%S')}.json"
        with open(report_file, 'w', encoding='utf-8') as f:
            json.dump(consolidated_report, f, indent=2, ensure_ascii=False, default=str)
        
        logger.info(f"üìÑ Relat√≥rio consolidado salvo: {report_file}")
        return consolidated_report

    def _generate_recommendations(self, all_results: Dict[str, Any]) -> List[str]:
        """
        Gerar recomenda√ß√µes baseadas nos resultados
        """
        recommendations = []
        
        # An√°lise de performance
        for scenario, result in all_results.items():
            if "validation" in result:
                validation = result["validation"]
                
                if not validation.get("passed", True):
                    recommendations.append(f"üîß Otimizar cen√°rio {scenario}: {', '.join(validation.get('errors', []))}")
                
                if validation.get("warnings"):
                    recommendations.append(f"‚ö†Ô∏è Revisar cen√°rio {scenario}: {', '.join(validation.get('warnings', []))}")
        
        # Recomenda√ß√µes gerais baseadas no c√≥digo real
        recommendations.extend([
            "üìä Monitorar uso de mem√≥ria durante downloads grandes",
            "üîí Validar controle de acesso para downloads sens√≠veis",
            "‚ö° Implementar cache para downloads frequentes",
            "üìà Otimizar compress√£o para diferentes tipos de arquivo",
            "üõ°Ô∏è Refor√ßar valida√ß√£o de tipos MIME",
            "üìã Implementar logs detalhados de download"
        ])
        
        return recommendations

    def _generate_next_steps(self, all_results: Dict[str, Any]) -> List[str]:
        """
        Gerar pr√≥ximos passos baseados nos resultados
        """
        next_steps = [
            "üöÄ Implementar testes de agendamento de relat√≥rios",
            "üìä Implementar testes de m√©tricas detalhadas",
            "üîß Implementar testes de infraestrutura",
            "‚ö° Implementar testes de performance espec√≠ficos",
            "üåç Implementar testes de ambiente",
            "üéØ Implementar testes de chaos engineering",
            "üìà Implementar testes de monitoramento",
            "üõ°Ô∏è Implementar testes de compliance",
            "üéÆ Implementar testes de gamifica√ß√£o"
        ]
        
        return next_steps

def main():
    """Fun√ß√£o principal"""
    parser = argparse.ArgumentParser(description="Executor de testes de carga para Download de Relat√≥rios")
    parser.add_argument("--scenario", help="Cen√°rio espec√≠fico para executar")
    parser.add_argument("--host", default="http://localhost:8000", help="Host do sistema")
    parser.add_argument("--config", help="Arquivo de configura√ß√£o JSON")
    parser.add_argument("--all", action="store_true", help="Executar todos os cen√°rios")
    
    args = parser.parse_args()
    
    # Configura√ß√£o padr√£o
    config = {
        "host": args.host,
        "timeout": 300,
        "retries": 3
    }
    
    # Carregar configura√ß√£o customizada se fornecida
    if args.config:
        try:
            with open(args.config, 'r') as f:
                custom_config = json.load(f)
                config.update(custom_config)
        except Exception as e:
            logger.error(f"Erro ao carregar configura√ß√£o: {e}")
            return 1
    
    # Criar executor
    runner = ReportsDownloadLoadTestRunner(config)
    
    try:
        # Configurar ambiente
        runner.setup_environment()
        
        if args.scenario:
            # Executar cen√°rio espec√≠fico
            logger.info(f"üéØ Executando cen√°rio espec√≠fico: {args.scenario}")
            result = runner.run_test_scenario(args.scenario)
            print(json.dumps(result, indent=2, ensure_ascii=False, default=str))
            
        elif args.all:
            # Executar todos os cen√°rios
            logger.info("üöÄ Executando todos os cen√°rios")
            report = runner.run_all_scenarios()
            print(json.dumps(report, indent=2, ensure_ascii=False, default=str))
            
        else:
            # Executar cen√°rio padr√£o (normal)
            logger.info("üéØ Executando cen√°rio padr√£o: normal")
            result = runner.run_test_scenario("normal")
            print(json.dumps(result, indent=2, ensure_ascii=False, default=str))
        
        logger.info("‚úÖ Execu√ß√£o conclu√≠da com sucesso")
        return 0
        
    except Exception as e:
        logger.error(f"‚ùå Erro na execu√ß√£o: {e}")
        return 1

if __name__ == "__main__":
    sys.exit(main()) 