#!/usr/bin/env python3
"""
Script Principal de Gera√ß√£o de Documenta√ß√£o Enterprise
Tracing ID: GENERATE_ENTERPRISE_DOCS_20250127_001
Data: 2025-01-27
Vers√£o: 1.0

Objetivo: Orquestrar gera√ß√£o completa de documenta√ß√£o enterprise,
integrando todos os sistemas de qualidade, seguran√ßa, compliance
e m√©tricas em um pipeline automatizado.
"""

import os
import sys
import json
import logging
import argparse
import traceback
from datetime import datetime
from pathlib import Path
from typing import Dict, List, Any, Optional, Tuple
from dataclasses import dataclass, asdict
from concurrent.futures import ThreadPoolExecutor, as_completed

# Adicionar diret√≥rio raiz ao path
sys.path.append(str(Path(__file__).parent.parent))

try:
    from infrastructure.ml.semantic_embeddings import SemanticEmbeddingService
    from infrastructure.validation.doc_quality_score import DocQualityAnalyzer
    from infrastructure.security.advanced_security_system import SensitiveDataDetector
    from infrastructure.backup.rollback_system import RollbackSystem
    from shared.doc_generation_metrics import DocGenerationMetrics, MetricsCollector, MetricsAnalyzer
    from shared.semantic_contracts_generator import SemanticContractsGenerator
    from shared.log_analyzer import LogAnalyzer
    from shared.api_docs_generator import APIDocsGenerator
    from shared.trigger_config_validator import TriggerConfigValidator
    from shared.compliance_validator import ComplianceValidator
    from scripts.validate_doc_compliance import DocComplianceValidator
except ImportError as e:
    print(f"‚ùå Erro de importa√ß√£o: {e}")
    print("Certifique-se de que todos os m√≥dulos est√£o implementados")
    sys.exit(1)


@dataclass
class GenerationConfig:
    """Configura√ß√£o de gera√ß√£o de documenta√ß√£o"""
    docs_path: str = "docs/"
    output_path: str = "docs/enterprise/"
    backup_enabled: bool = True
    quality_threshold: float = 0.85
    security_threshold: float = 1.0
    compliance_threshold: float = 0.90
    max_workers: int = 4
    verbose: bool = False


@dataclass
class GenerationResult:
    """Resultado da gera√ß√£o de documenta√ß√£o"""
    timestamp: str
    tracing_id: str
    success: bool
    duration: float
    files_generated: int
    files_processed: int
    quality_score: float
    security_score: float
    compliance_score: float
    errors: List[Dict[str, Any]]
    warnings: List[Dict[str, Any]]
    metrics: Dict[str, Any]


class EnterpriseDocGenerator:
    """
    Gerador principal de documenta√ß√£o enterprise
    """
    
    def __init__(self, config: GenerationConfig):
        self.config = config
        self.tracing_id = f"ENTERPRISE_DOCS_{datetime.now().strftime('%Y%m%d_%H%M%S')}"
        self.setup_logging()
        
        # Inicializar servi√ßos
        self.initialize_services()
        
        # M√©tricas
        self.metrics_collector = MetricsCollector()
        self.metrics_analyzer = MetricsAnalyzer()
        
        self.logger.info(f"[{self.tracing_id}] Inicializado gerador enterprise")
    
    def setup_logging(self):
        """Configurar sistema de logging"""
        self.logger = logging.getLogger(f"EnterpriseDocGenerator_{self.tracing_id}")
        self.logger.setLevel(logging.DEBUG if self.config.verbose else logging.INFO)
        
        # Handler para console
        console_handler = logging.StreamHandler()
        console_handler.setLevel(logging.DEBUG if self.config.verbose else logging.INFO)
        
        # Handler para arquivo
        log_dir = Path("logs/enterprise")
        log_dir.mkdir(parents=True, exist_ok=True)
        file_handler = logging.FileHandler(
            log_dir / f"generation_{self.tracing_id}.log",
            encoding='utf-8'
        )
        file_handler.setLevel(logging.DEBUG)
        
        # Formato
        formatter = logging.Formatter(
            '[%(levelname)string_data] [%(name)string_data] %(message)string_data - %(asctime)string_data'
        )
        console_handler.setFormatter(formatter)
        file_handler.setFormatter(formatter)
        
        self.logger.addHandler(console_handler)
        self.logger.addHandler(file_handler)
    
    def initialize_services(self):
        """Inicializar todos os servi√ßos necess√°rios"""
        try:
            self.logger.info(f"[{self.tracing_id}] Inicializando servi√ßos...")
            
            # Servi√ßos de ML e an√°lise
            self.semantic_service = SemanticEmbeddingService()
            self.quality_analyzer = DocQualityAnalyzer()
            self.security_detector = SensitiveDataDetector()
            
            # Sistema de backup e rollback
            self.rollback_system = RollbackSystem()
            
            # Geradores de documenta√ß√£o
            self.contracts_generator = SemanticContractsGenerator()
            self.log_analyzer = LogAnalyzer()
            self.api_docs_generator = APIDocsGenerator()
            
            # Validadores
            self.trigger_validator = TriggerConfigValidator()
            self.compliance_validator = ComplianceValidator()
            self.compliance_checker = DocComplianceValidator()
            
            self.logger.info(f"[{self.tracing_id}] Todos os servi√ßos inicializados")
            
        except Exception as e:
            self.logger.error(f"[{self.tracing_id}] Erro na inicializa√ß√£o: {str(e)}")
            raise
    
    def create_backup(self) -> bool:
        """Criar backup antes da gera√ß√£o"""
        if not self.config.backup_enabled:
            return True
        
        try:
            self.logger.info(f"[{self.tracing_id}] Criando backup...")
            
            # Criar snapshot do estado atual
            snapshot_id = self.rollback_system.create_snapshot(
                description=f"Backup antes da gera√ß√£o {self.tracing_id}"
            )
            
            self.logger.info(f"[{self.tracing_id}] Backup criado: {snapshot_id}")
            return True
            
        except Exception as e:
            self.logger.error(f"[{self.tracing_id}] Erro no backup: {str(e)}")
            return False
    
    def validate_environment(self) -> Tuple[bool, List[str]]:
        """Validar ambiente antes da gera√ß√£o"""
        errors = []
        
        try:
            self.logger.info(f"[{self.tracing_id}] Validando ambiente...")
            
            # Verificar diret√≥rios
            docs_path = Path(self.config.docs_path)
            if not docs_path.exists():
                docs_path.mkdir(parents=True, exist_ok=True)
                self.logger.info(f"[{self.tracing_id}] Diret√≥rio criado: {docs_path}")
            
            output_path = Path(self.config.output_path)
            if not output_path.exists():
                output_path.mkdir(parents=True, exist_ok=True)
                self.logger.info(f"[{self.tracing_id}] Diret√≥rio criado: {output_path}")
            
            # Verificar configura√ß√£o de triggers
            trigger_config_path = "docs/trigger_config.json"
            if not Path(trigger_config_path).exists():
                errors.append(f"Arquivo de configura√ß√£o n√£o encontrado: {trigger_config_path}")
            
            # Verificar permiss√µes de escrita
            if not os.access(output_path, os.W_OK):
                errors.append(f"Sem permiss√£o de escrita em: {output_path}")
            
            # Verificar servi√ßos
            if not self.semantic_service:
                errors.append("Servi√ßo sem√¢ntico n√£o inicializado")
            
            if not self.quality_analyzer:
                errors.append("Analisador de qualidade n√£o inicializado")
            
            if len(errors) == 0:
                self.logger.info(f"[{self.tracing_id}] Ambiente validado com sucesso")
            
            return len(errors) == 0, errors
            
        except Exception as e:
            errors.append(f"Erro na valida√ß√£o: {str(e)}")
            return False, errors
    
    def generate_semantic_contracts(self) -> Tuple[bool, List[Dict]]:
        """Gerar contratos sem√¢nticos"""
        try:
            self.logger.info(f"[{self.tracing_id}] Gerando contratos sem√¢nticos...")
            
            # Gerar documenta√ß√£o de m√≥dulos
            modules_docs = self.contracts_generator.generate_module_docs()
            
            # Gerar documenta√ß√£o de fun√ß√µes
            functions_docs = self.contracts_generator.generate_function_docs()
            
            # Salvar contratos
            contracts_file = Path(self.config.output_path) / "semantic_contracts.md"
            with open(contracts_file, 'w', encoding='utf-8') as f:
                f.write("# Contratos Sem√¢nticos\n\n")
                f.write(f"Gerado em: {datetime.now().isoformat()}\n")
                f.write(f"Tracing ID: {self.tracing_id}\n\n")
                
                f.write("## M√≥dulos\n\n")
                for module, doc in modules_docs.items():
                    f.write(f"### {module}\n")
                    f.write(f"{doc}\n\n")
                
                f.write("## Fun√ß√µes\n\n")
                for func, doc in functions_docs.items():
                    f.write(f"### {func}\n")
                    f.write(f"{doc}\n\n")
            
            self.logger.info(f"[{self.tracing_id}] Contratos sem√¢nticos gerados: {contracts_file}")
            return True, []
            
        except Exception as e:
            error_msg = f"Erro na gera√ß√£o de contratos: {str(e)}"
            self.logger.error(f"[{self.tracing_id}] {error_msg}")
            return False, [{"type": "ERROR", "message": error_msg}]
    
    def generate_log_based_suggestions(self) -> Tuple[bool, List[Dict]]:
        """Gerar sugest√µes baseadas em logs"""
        try:
            self.logger.info(f"[{self.tracing_id}] Gerando sugest√µes baseadas em logs...")
            
            # Analisar logs
            suggestions = self.log_analyzer.extract_suggestions()
            improvements = self.log_analyzer.prioritize_improvements()
            
            # Salvar sugest√µes
            suggestions_file = Path(self.config.output_path) / "log_based_suggestions.md"
            with open(suggestions_file, 'w', encoding='utf-8') as f:
                f.write("# Sugest√µes Baseadas em Logs\n\n")
                f.write(f"Gerado em: {datetime.now().isoformat()}\n")
                f.write(f"Tracing ID: {self.tracing_id}\n\n")
                
                f.write("## Sugest√µes de Melhoria\n\n")
                for index, suggestion in enumerate(suggestions, 1):
                    f.write(f"{index}. {suggestion}\n")
                
                f.write("\n## Melhorias Priorit√°rias\n\n")
                for index, improvement in enumerate(improvements, 1):
                    f.write(f"{index}. {improvement}\n")
            
            self.logger.info(f"[{self.tracing_id}] Sugest√µes baseadas em logs geradas: {suggestions_file}")
            return True, []
            
        except Exception as e:
            error_msg = f"Erro na gera√ß√£o de sugest√µes: {str(e)}"
            self.logger.error(f"[{self.tracing_id}] {error_msg}")
            return False, [{"type": "ERROR", "message": error_msg}]
    
    def generate_api_documentation(self) -> Tuple[bool, List[Dict]]:
        """Gerar documenta√ß√£o de APIs"""
        try:
            self.logger.info(f"[{self.tracing_id}] Gerando documenta√ß√£o de APIs...")
            
            # Detectar e gerar documenta√ß√£o GraphQL
            graphql_docs = self.api_docs_generator.generate_graphql_docs()
            
            # Detectar e gerar documenta√ß√£o Protobuf
            protobuf_docs = self.api_docs_generator.generate_protobuf_docs()
            
            # Detectar e gerar documenta√ß√£o OpenAPI
            openapi_docs = self.api_docs_generator.generate_openapi_docs()
            
            # Salvar documenta√ß√£o de APIs
            api_docs_file = Path(self.config.output_path) / "api_documentation.md"
            with open(api_docs_file, 'w', encoding='utf-8') as f:
                f.write("# Documenta√ß√£o de APIs\n\n")
                f.write(f"Gerado em: {datetime.now().isoformat()}\n")
                f.write(f"Tracing ID: {self.tracing_id}\n\n")
                
                if graphql_docs:
                    f.write("## GraphQL\n\n")
                    f.write(graphql_docs)
                    f.write("\n\n")
                
                if protobuf_docs:
                    f.write("## Protobuf\n\n")
                    f.write(protobuf_docs)
                    f.write("\n\n")
                
                if openapi_docs:
                    f.write("## OpenAPI\n\n")
                    f.write(openapi_docs)
                    f.write("\n\n")
            
            self.logger.info(f"[{self.tracing_id}] Documenta√ß√£o de APIs gerada: {api_docs_file}")
            return True, []
            
        except Exception as e:
            error_msg = f"Erro na gera√ß√£o de documenta√ß√£o de APIs: {str(e)}"
            self.logger.error(f"[{self.tracing_id}] {error_msg}")
            return False, [{"type": "ERROR", "message": error_msg}]
    
    def generate_quality_report(self) -> Tuple[bool, List[Dict]]:
        """Gerar relat√≥rio de qualidade"""
        try:
            self.logger.info(f"[{self.tracing_id}] Gerando relat√≥rio de qualidade...")
            
            # Analisar qualidade de todos os arquivos
            quality_scores = {}
            total_score = 0.0
            file_count = 0
            
            docs_path = Path(self.config.docs_path)
            for doc_file in docs_path.rglob("*.md"):
                try:
                    with open(doc_file, 'r', encoding='utf-8') as f:
                        content = f.read()
                    
                    quality_score = self.quality_analyzer.calculate_doc_quality_score(content)
                    quality_scores[str(doc_file)] = quality_score
                    total_score += quality_score
                    file_count += 1
                    
                except Exception as e:
                    self.logger.warning(f"[{self.tracing_id}] Erro ao analisar {doc_file}: {str(e)}")
            
            avg_quality = total_score / file_count if file_count > 0 else 0.0
            
            # Salvar relat√≥rio
            quality_file = Path(self.config.output_path) / "quality_report.md"
            with open(quality_file, 'w', encoding='utf-8') as f:
                f.write("# Relat√≥rio de Qualidade\n\n")
                f.write(f"Gerado em: {datetime.now().isoformat()}\n")
                f.write(f"Tracing ID: {self.tracing_id}\n\n")
                
                f.write(f"## Resumo\n\n")
                f.write(f"- **Arquivos analisados**: {file_count}\n")
                f.write(f"- **Qualidade m√©dia**: {avg_quality:.2f}\n")
                f.write(f"- **Threshold**: {self.config.quality_threshold}\n")
                f.write(f"- **Status**: {'‚úÖ APROVADO' if avg_quality >= self.config.quality_threshold else '‚ùå REPROVADO'}\n\n")
                
                f.write("## Detalhes por Arquivo\n\n")
                for file_path, score in quality_scores.items():
                    status = "‚úÖ" if score >= self.config.quality_threshold else "‚ùå"
                    f.write(f"- {status} {file_path}: {score:.2f}\n")
            
            self.logger.info(f"[{self.tracing_id}] Relat√≥rio de qualidade gerado: {quality_file}")
            return True, []
            
        except Exception as e:
            error_msg = f"Erro na gera√ß√£o de relat√≥rio de qualidade: {str(e)}"
            self.logger.error(f"[{self.tracing_id}] {error_msg}")
            return False, [{"type": "ERROR", "message": error_msg}]
    
    def generate_security_report(self) -> Tuple[bool, List[Dict]]:
        """Gerar relat√≥rio de seguran√ßa"""
        try:
            self.logger.info(f"[{self.tracing_id}] Gerando relat√≥rio de seguran√ßa...")
            
            # Escanear todos os arquivos
            security_incidents = []
            docs_path = Path(self.config.docs_path)
            
            for doc_file in docs_path.rglob("*.md"):
                try:
                    with open(doc_file, 'r', encoding='utf-8') as f:
                        content = f.read()
                    
                    sensitive_data = self.security_detector.scan_documentation(content)
                    if sensitive_data:
                        for data in sensitive_data:
                            security_incidents.append({
                                'file': str(doc_file),
                                'type': data['type'],
                                'line': data.get('line', 'N/A'),
                                'pattern': data.get('pattern', 'N/A'),
                                'severity': 'CRITICAL'
                            })
                    
                except Exception as e:
                    self.logger.warning(f"[{self.tracing_id}] Erro ao escanear {doc_file}: {str(e)}")
            
            # Salvar relat√≥rio
            security_file = Path(self.config.output_path) / "security_report.md"
            with open(security_file, 'w', encoding='utf-8') as f:
                f.write("# Relat√≥rio de Seguran√ßa\n\n")
                f.write(f"Gerado em: {datetime.now().isoformat()}\n")
                f.write(f"Tracing ID: {self.tracing_id}\n\n")
                
                f.write(f"## Resumo\n\n")
                f.write(f"- **Incidentes encontrados**: {len(security_incidents)}\n")
                f.write(f"- **Status**: {'‚ùå FALHA' if security_incidents else '‚úÖ APROVADO'}\n\n")
                
                if security_incidents:
                    f.write("## Incidentes de Seguran√ßa\n\n")
                    for index, incident in enumerate(security_incidents, 1):
                        f.write(f"### Incidente {index}\n")
                        f.write(f"- **Arquivo**: {incident['file']}\n")
                        f.write(f"- **Tipo**: {incident['type']}\n")
                        f.write(f"- **Linha**: {incident['line']}\n")
                        f.write(f"- **Padr√£o**: {incident['pattern']}\n")
                        f.write(f"- **Severidade**: {incident['severity']}\n\n")
                else:
                    f.write("‚úÖ Nenhum incidente de seguran√ßa encontrado.\n\n")
            
            self.logger.info(f"[{self.tracing_id}] Relat√≥rio de seguran√ßa gerado: {security_file}")
            return True, []
            
        except Exception as e:
            error_msg = f"Erro na gera√ß√£o de relat√≥rio de seguran√ßa: {str(e)}"
            self.logger.error(f"[{self.tracing_id}] {error_msg}")
            return False, [{"type": "ERROR", "message": error_msg}]
    
    def generate_compliance_report(self) -> Tuple[bool, List[Dict]]:
        """Gerar relat√≥rio de compliance"""
        try:
            self.logger.info(f"[{self.tracing_id}] Gerando relat√≥rio de compliance...")
            
            # Validar compliance
            compliance_result = self.compliance_validator.validate_all_standards()
            
            # Salvar relat√≥rio
            compliance_file = Path(self.config.output_path) / "compliance_report.md"
            with open(compliance_file, 'w', encoding='utf-8') as f:
                f.write("# Relat√≥rio de Compliance\n\n")
                f.write(f"Gerado em: {datetime.now().isoformat()}\n")
                f.write(f"Tracing ID: {self.tracing_id}\n\n")
                
                f.write(f"## Resumo\n\n")
                f.write(f"- **Score geral**: {compliance_result.get('overall_score', 0):.2f}\n")
                f.write(f"- **Viola√ß√µes**: {len(compliance_result.get('violations', []))}\n")
                f.write(f"- **Status**: {'‚úÖ APROVADO' if compliance_result.get('overall_score', 0) >= self.config.compliance_threshold else '‚ùå REPROVADO'}\n\n")
                
                violations = compliance_result.get('violations', [])
                if violations:
                    f.write("## Viola√ß√µes\n\n")
                    for index, violation in enumerate(violations, 1):
                        f.write(f"### Viola√ß√£o {index}\n")
                        f.write(f"- **Descri√ß√£o**: {violation.get('description', 'N/A')}\n")
                        f.write(f"- **Padr√£o**: {violation.get('standard', 'N/A')}\n")
                        f.write(f"- **Severidade**: {violation.get('severity', 'N/A')}\n\n")
                else:
                    f.write("‚úÖ Nenhuma viola√ß√£o de compliance encontrada.\n\n")
            
            self.logger.info(f"[{self.tracing_id}] Relat√≥rio de compliance gerado: {compliance_file}")
            return True, []
            
        except Exception as e:
            error_msg = f"Erro na gera√ß√£o de relat√≥rio de compliance: {str(e)}"
            self.logger.error(f"[{self.tracing_id}] {error_msg}")
            return False, [{"type": "ERROR", "message": error_msg}]
    
    def generate_metrics_report(self) -> Tuple[bool, List[Dict]]:
        """Gerar relat√≥rio de m√©tricas"""
        try:
            self.logger.info(f"[{self.tracing_id}] Gerando relat√≥rio de m√©tricas...")
            
            # Coletar m√©tricas
            metrics = self.metrics_collector.collect_all_metrics()
            analysis = self.metrics_analyzer.analyze_metrics(metrics)
            
            # Salvar relat√≥rio
            metrics_file = Path(self.config.output_path) / "metrics_report.md"
            with open(metrics_file, 'w', encoding='utf-8') as f:
                f.write("# Relat√≥rio de M√©tricas\n\n")
                f.write(f"Gerado em: {datetime.now().isoformat()}\n")
                f.write(f"Tracing ID: {self.tracing_id}\n\n")
                
                f.write("## M√©tricas de Performance\n\n")
                f.write(f"- **Tempo m√©dio de gera√ß√£o**: {metrics.get('avg_generation_time', 0):.2f}string_data\n")
                f.write(f"- **Tokens consumidos**: {metrics.get('avg_tokens_used', 0)}\n")
                f.write(f"- **Cobertura de documenta√ß√£o**: {metrics.get('documentation_coverage', 0):.2%}\n\n")
                
                f.write("## An√°lise de Tend√™ncias\n\n")
                for trend in analysis.get('trends', []):
                    f.write(f"- {trend}\n")
                
                f.write("\n## Otimiza√ß√µes Recomendadas\n\n")
                for optimization in analysis.get('optimizations', []):
                    f.write(f"- {optimization}\n")
            
            self.logger.info(f"[{self.tracing_id}] Relat√≥rio de m√©tricas gerado: {metrics_file}")
            return True, []
            
        except Exception as e:
            error_msg = f"Erro na gera√ß√£o de relat√≥rio de m√©tricas: {str(e)}"
            self.logger.error(f"[{self.tracing_id}] {error_msg}")
            return False, [{"type": "ERROR", "message": error_msg}]
    
    def generate_summary_report(self, results: Dict[str, Any]) -> bool:
        """Gerar relat√≥rio resumo"""
        try:
            self.logger.info(f"[{self.tracing_id}] Gerando relat√≥rio resumo...")
            
            summary_file = Path(self.config.output_path) / "summary_report.md"
            with open(summary_file, 'w', encoding='utf-8') as f:
                f.write("# Relat√≥rio Resumo - Gera√ß√£o de Documenta√ß√£o Enterprise\n\n")
                f.write(f"**Tracing ID**: {self.tracing_id}\n")
                f.write(f"**Data**: {datetime.now().strftime('%Y-%m-%data %H:%M:%S')}\n")
                f.write(f"**Vers√£o**: 1.0\n\n")
                
                f.write("## üìä Resumo Executivo\n\n")
                f.write(f"- **Status**: {'‚úÖ SUCESSO' if results['success'] else '‚ùå FALHA'}\n")
                f.write(f"- **Dura√ß√£o**: {results['duration']:.2f}string_data\n")
                f.write(f"- **Arquivos gerados**: {results['files_generated']}\n")
                f.write(f"- **Arquivos processados**: {results['files_processed']}\n\n")
                
                f.write("## üìà Scores de Qualidade\n\n")
                f.write(f"- **Qualidade**: {results['quality_score']:.2f}\n")
                f.write(f"- **Seguran√ßa**: {results['security_score']:.2f}\n")
                f.write(f"- **Compliance**: {results['compliance_score']:.2f}\n\n")
                
                if results['errors']:
                    f.write("## ‚ùå Erros\n\n")
                    for index, error in enumerate(results['errors'], 1):
                        f.write(f"{index}. {error['message']}\n")
                    f.write("\n")
                
                if results['warnings']:
                    f.write("## ‚ö†Ô∏è Avisos\n\n")
                    for index, warning in enumerate(results['warnings'], 1):
                        f.write(f"{index}. {warning['message']}\n")
                    f.write("\n")
                
                f.write("## üìÅ Arquivos Gerados\n\n")
                output_path = Path(self.config.output_path)
                for file_path in output_path.glob("*.md"):
                    f.write(f"- `{file_path.name}`\n")
                
                f.write("\n---\n")
                f.write("*Gerado automaticamente pelo EnterpriseDocGenerator*\n")
            
            self.logger.info(f"[{self.tracing_id}] Relat√≥rio resumo gerado: {summary_file}")
            return True
            
        except Exception as e:
            self.logger.error(f"[{self.tracing_id}] Erro ao gerar relat√≥rio resumo: {str(e)}")
            return False
    
    def run_generation_pipeline(self) -> GenerationResult:
        """Executar pipeline completo de gera√ß√£o"""
        start_time = datetime.now()
        
        try:
            self.logger.info(f"[{self.tracing_id}] Iniciando pipeline de gera√ß√£o...")
            
            # 1. Criar backup
            if not self.create_backup():
                self.logger.warning(f"[{self.tracing_id}] Backup falhou, continuando...")
            
            # 2. Validar ambiente
            env_valid, env_errors = self.validate_environment()
            if not env_valid:
                raise Exception(f"Ambiente inv√°lido: {', '.join(env_errors)}")
            
            # 3. Executar gera√ß√µes em paralelo
            generation_tasks = [
                ("semantic_contracts", self.generate_semantic_contracts),
                ("log_suggestions", self.generate_log_based_suggestions),
                ("api_docs", self.generate_api_documentation),
                ("quality_report", self.generate_quality_report),
                ("security_report", self.generate_security_report),
                ("compliance_report", self.generate_compliance_report),
                ("metrics_report", self.generate_metrics_report)
            ]
            
            results = {}
            errors = []
            warnings = []
            
            with ThreadPoolExecutor(max_workers=self.config.max_workers) as executor:
                future_to_task = {
                    executor.submit(task_func): task_name 
                    for task_name, task_func in generation_tasks
                }
                
                for future in as_completed(future_to_task):
                    task_name = future_to_task[future]
                    try:
                        success, task_errors = future.result()
                        results[task_name] = success
                        
                        if not success:
                            errors.extend(task_errors)
                        elif task_errors:
                            warnings.extend(task_errors)
                            
                    except Exception as e:
                        error_msg = f"Erro na tarefa {task_name}: {str(e)}"
                        self.logger.error(f"[{self.tracing_id}] {error_msg}")
                        errors.append({"type": "ERROR", "message": error_msg})
                        results[task_name] = False
            
            # 4. Validar compliance final
            self.logger.info(f"[{self.tracing_id}] Executando valida√ß√£o final de compliance...")
            compliance_report = self.compliance_checker.run_validation(self.config.docs_path)
            
            # 5. Gerar relat√≥rio resumo
            summary_data = {
                'success': len(errors) == 0,
                'duration': (datetime.now() - start_time).total_seconds(),
                'files_generated': len([r for r in results.values() if r]),
                'files_processed': len(results),
                'quality_score': compliance_report.quality_score,
                'security_score': compliance_report.security_score,
                'compliance_score': compliance_report.compliance_score,
                'errors': errors,
                'warnings': warnings
            }
            
            self.generate_summary_report(summary_data)
            
            # 6. Coletar m√©tricas finais
            final_metrics = self.metrics_collector.collect_all_metrics()
            
            end_time = datetime.now()
            duration = (end_time - start_time).total_seconds()
            
            # Criar resultado
            result = GenerationResult(
                timestamp=datetime.now().isoformat(),
                tracing_id=self.tracing_id,
                success=len(errors) == 0,
                duration=duration,
                files_generated=summary_data['files_generated'],
                files_processed=summary_data['files_processed'],
                quality_score=compliance_report.quality_score,
                security_score=compliance_report.security_score,
                compliance_score=compliance_report.compliance_score,
                errors=errors,
                warnings=warnings,
                metrics=final_metrics
            )
            
            self.logger.info(f"[{self.tracing_id}] Pipeline conclu√≠do em {duration:.2f}string_data")
            self.logger.info(f"[{self.tracing_id}] Sucesso: {result.success}")
            self.logger.info(f"[{self.tracing_id}] Arquivos gerados: {result.files_generated}")
            
            return result
            
        except Exception as e:
            self.logger.error(f"[{self.tracing_id}] Erro no pipeline: {str(e)}")
            self.logger.error(traceback.format_exc())
            
            # Rollback em caso de erro
            if self.config.backup_enabled:
                self.logger.info(f"[{self.tracing_id}] Executando rollback...")
                try:
                    self.rollback_system.restore_latest_snapshot()
                    self.logger.info(f"[{self.tracing_id}] Rollback executado com sucesso")
                except Exception as rollback_error:
                    self.logger.error(f"[{self.tracing_id}] Erro no rollback: {str(rollback_error)}")
            
            # Retornar resultado de erro
            return GenerationResult(
                timestamp=datetime.now().isoformat(),
                tracing_id=self.tracing_id,
                success=False,
                duration=(datetime.now() - start_time).total_seconds(),
                files_generated=0,
                files_processed=0,
                quality_score=0.0,
                security_score=0.0,
                compliance_score=0.0,
                errors=[{"type": "CRITICAL", "message": str(e)}],
                warnings=[],
                metrics={}
            )
    
    def save_result(self, result: GenerationResult, output_path: str = "logs/generation_result.json"):
        """Salvar resultado da gera√ß√£o"""
        try:
            # Criar diret√≥rio se n√£o existir
            output_dir = Path(output_path).parent
            output_dir.mkdir(parents=True, exist_ok=True)
            
            # Converter para dict
            result_dict = asdict(result)
            
            # Salvar arquivo
            with open(output_path, 'w', encoding='utf-8') as f:
                json.dump(result_dict, f, indent=2, ensure_ascii=False)
            
            self.logger.info(f"[{self.tracing_id}] Resultado salvo em: {output_path}")
            
        except Exception as e:
            self.logger.error(f"[{self.tracing_id}] Erro ao salvar resultado: {str(e)}")
    
    def print_summary(self, result: GenerationResult):
        """Imprimir resumo da gera√ß√£o"""
        print("\n" + "="*80)
        print("üöÄ RELAT√ìRIO DE GERA√á√ÉO DE DOCUMENTA√á√ÉO ENTERPRISE")
        print("="*80)
        print(f"Tracing ID: {result.tracing_id}")
        print(f"Timestamp: {result.timestamp}")
        print(f"Status: {'‚úÖ SUCESSO' if result.success else '‚ùå FALHA'}")
        print(f"Dura√ß√£o: {result.duration:.2f}string_data")
        print("-"*80)
        print("üìä ESTAT√çSTICAS:")
        print(f"  Arquivos gerados: {result.files_generated}")
        print(f"  Arquivos processados: {result.files_processed}")
        print(f"  Score de qualidade: {result.quality_score:.2f}")
        print(f"  Score de seguran√ßa: {result.security_score:.2f}")
        print(f"  Score de compliance: {result.compliance_score:.2f}")
        print("-"*80)
        
        if result.errors:
            print("‚ùå ERROS:")
            for index, error in enumerate(result.errors, 1):
                print(f"  {index}. {error['message']}")
        else:
            print("‚úÖ Nenhum erro encontrado!")
        
        if result.warnings:
            print("-"*80)
            print("‚ö†Ô∏è AVISOS:")
            for index, warning in enumerate(result.warnings, 1):
                print(f"  {index}. {warning['message']}")
        
        print("="*80)


def main():
    """
    Fun√ß√£o principal do script
    """
    parser = argparse.ArgumentParser(
        description="Gerador de Documenta√ß√£o Enterprise"
    )
    parser.add_argument(
        "--docs-path",
        default="docs/",
        help="Caminho para documenta√ß√£o (padr√£o: docs/)"
    )
    parser.add_argument(
        "--output-path",
        default="docs/enterprise/",
        help="Caminho de sa√≠da (padr√£o: docs/enterprise/)"
    )
    parser.add_argument(
        "--no-backup",
        action="store_true",
        help="Desabilitar backup autom√°tico"
    )
    parser.add_argument(
        "--quality-threshold",
        type=float,
        default=0.85,
        help="Threshold de qualidade (padr√£o: 0.85)"
    )
    parser.add_argument(
        "--max-workers",
        type=int,
        default=4,
        help="N√∫mero m√°ximo de workers paralelos (padr√£o: 4)"
    )
    parser.add_argument(
        "--verbose",
        action="store_true",
        help="Modo verbose com logs detalhados"
    )
    
    args = parser.parse_args()
    
    # Configurar logging
    if args.verbose:
        logging.basicConfig(level=logging.DEBUG)
    
    try:
        # Criar configura√ß√£o
        config = GenerationConfig(
            docs_path=args.docs_path,
            output_path=args.output_path,
            backup_enabled=not args.no_backup,
            quality_threshold=args.quality_threshold,
            max_workers=args.max_workers,
            verbose=args.verbose
        )
        
        # Criar gerador
        generator = EnterpriseDocGenerator(config)
        
        # Executar pipeline
        result = generator.run_generation_pipeline()
        
        # Salvar resultado
        generator.save_result(result)
        
        # Imprimir resumo
        generator.print_summary(result)
        
        # Retornar c√≥digo de sa√≠da
        if result.success:
            sys.exit(0)
        else:
            sys.exit(1)
            
    except Exception as e:
        print(f"‚ùå Erro cr√≠tico: {str(e)}")
        if args.verbose:
            traceback.print_exc()
        sys.exit(1)


if __name__ == "__main__":
    main() 