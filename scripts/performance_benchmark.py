#!/usr/bin/env python3
"""
üéØ BENCHMARK COMPLETO DE PERFORMANCE - OMNƒ∞ KEYWORDS FINDER
üìê CoCoT: Comprova√ß√£o, Causalidade, Contexto, Tend√™ncia
üå≤ ToT: M√∫ltiplas abordagens de an√°lise
‚ôªÔ∏è ReAct: Simula√ß√£o e reflex√£o sobre impactos
üñºÔ∏è Visual: Representa√ß√µes de relacionamentos

Tracing ID: PERFORMANCE_BENCHMARK_20250127_001
Data: 2025-01-27
Vers√£o: 1.0.0
Status: ‚úÖ CRIA√á√ÉO DE SCRIPT
"""

import os
import sys
import json
import time
import psutil
import threading
import subprocess
from pathlib import Path
from typing import Dict, List, Set, Tuple, Optional, NamedTuple
from dataclasses import dataclass, field
from datetime import datetime
import logging
from collections import defaultdict, Counter
import statistics
import cProfile
import pstats
import io
import sqlite3
import requests
from concurrent.futures import ThreadPoolExecutor, as_completed

# Configura√ß√£o de logging estruturado
logging.basicConfig(
    level=logging.INFO,
    format='%(asctime)s - %(name)s - %(levelname)s - %(message)s'
)
logger = logging.getLogger(__name__)

@dataclass
class PerformanceMetric:
    """M√©trica de performance"""
    name: str
    value: float
    unit: str
    timestamp: datetime
    category: str  # cpu, memory, disk, network, database, api
    severity: str  # LOW, MEDIUM, HIGH, CRITICAL
    description: str = ""

@dataclass
class PerformanceResult:
    """Resultado de benchmark de performance"""
    total_metrics: int
    critical_issues: int
    high_priority_issues: int
    cpu_bottlenecks: int
    memory_bottlenecks: int
    disk_bottlenecks: int
    network_bottlenecks: int
    database_bottlenecks: int
    api_bottlenecks: int
    recommendations: List[str]
    performance_score: float  # 0.0 a 1.0
    bottleneck_analysis: Dict[str, List[PerformanceMetric]]
    resource_usage: Dict[str, Dict[str, float]]
    optimization_potential: float

class PerformanceBenchmark:
    """
    üìê CoCoT - Benchmark de Performance Avan√ßado
    
    Comprova√ß√£o: Baseado em m√©tricas reais e benchmarks
    Causalidade: Identifica gargalos de performance
    Contexto: Considera arquitetura e padr√µes do projeto
    Tend√™ncia: Aplica t√©cnicas modernas de profiling
    """
    
    def __init__(self, project_root: str):
        self.project_root = Path(project_root)
        self.metrics: List[PerformanceMetric] = []
        self.resource_usage: Dict[str, Dict[str, float]] = defaultdict(dict)
        self.bottlenecks: Dict[str, List[PerformanceMetric]] = defaultdict(list)
        self.performance_score = 0.0
        
        # Configura√ß√µes de benchmark
        self.benchmark_duration = 60  # segundos
        self.sample_interval = 1  # segundo
        self.load_test_duration = 30  # segundos
        self.concurrent_users = 10
        
        # Thresholds de performance
        self.thresholds = {
            "cpu_usage": 80.0,  # %
            "memory_usage": 85.0,  # %
            "disk_io": 1000.0,  # MB/s
            "network_latency": 100.0,  # ms
            "database_query_time": 1000.0,  # ms
            "api_response_time": 500.0  # ms
        }
    
    def run_complete_benchmark(self) -> PerformanceResult:
        """
        üå≤ ToT - Benchmark Completo de Performance
        
        Abordagem 1: An√°lise de recursos (ESCOLHIDA)
        Abordagem 2: An√°lise de c√≥digo
        Abordagem 3: An√°lise de banco de dados
        """
        logger.info("üöÄ Iniciando benchmark completo de performance...")
        
        # Etapa 1: An√°lise de recursos do sistema
        logger.info("üîç Etapa 1: Analisando recursos do sistema...")
        self._analyze_system_resources()
        
        # Etapa 2: An√°lise de performance de c√≥digo
        logger.info("üîç Etapa 2: Analisando performance de c√≥digo...")
        self._analyze_code_performance()
        
        # Etapa 3: An√°lise de banco de dados
        logger.info("üîç Etapa 3: Analisando performance de banco de dados...")
        self._analyze_database_performance()
        
        # Etapa 4: An√°lise de APIs
        logger.info("üîç Etapa 4: Analisando performance de APIs...")
        self._analyze_api_performance()
        
        # Etapa 5: Load testing
        logger.info("üîç Etapa 5: Executando load testing...")
        self._run_load_tests()
        
        # Etapa 6: An√°lise de gargalos
        logger.info("üîç Etapa 6: Analisando gargalos...")
        self._analyze_bottlenecks()
        
        # Etapa 7: C√°lculo de score de performance
        logger.info("üîç Etapa 7: Calculando score de performance...")
        self._calculate_performance_score()
        
        # Etapa 8: Gera√ß√£o de resultado
        result = self._generate_benchmark_result()
        
        logger.info(f"‚úÖ Benchmark conclu√≠do. {len(self.metrics)} m√©tricas coletadas")
        return result
    
    def _analyze_system_resources(self):
        """
        ‚ôªÔ∏è ReAct - An√°lise de Recursos do Sistema
        
        Simula√ß√£o: Monitoramento cont√≠nuo de recursos
        Efeitos: Identifica√ß√£o de gargalos de hardware
        Ganhos: Otimiza√ß√£o de recursos
        Riscos: Overhead de monitoramento (mitig√°vel com sampling)
        """
        logger.info("üìä Monitorando recursos do sistema...")
        
        start_time = time.time()
        samples = []
        
        while time.time() - start_time < self.benchmark_duration:
            # CPU
            cpu_percent = psutil.cpu_percent(interval=1)
            cpu_count = psutil.cpu_count()
            cpu_freq = psutil.cpu_freq()
            
            # Mem√≥ria
            memory = psutil.virtual_memory()
            swap = psutil.swap_memory()
            
            # Disco
            disk_usage = psutil.disk_usage('/')
            disk_io = psutil.disk_io_counters()
            
            # Rede
            network_io = psutil.net_io_counters()
            
            # Armazenar amostra
            sample = {
                'timestamp': datetime.now(),
                'cpu_percent': cpu_percent,
                'cpu_count': cpu_count,
                'cpu_freq': cpu_freq.current if cpu_freq else 0,
                'memory_percent': memory.percent,
                'memory_used': memory.used,
                'memory_available': memory.available,
                'swap_percent': swap.percent,
                'disk_percent': disk_usage.percent,
                'disk_io_read': disk_io.read_bytes if disk_io else 0,
                'disk_io_write': disk_io.write_bytes if disk_io else 0,
                'network_bytes_sent': network_io.bytes_sent,
                'network_bytes_recv': network_io.bytes_recv
            }
            samples.append(sample)
            
            # Criar m√©tricas
            self._create_resource_metrics(sample)
            
            time.sleep(self.sample_interval)
        
        # An√°lise estat√≠stica
        self._analyze_resource_statistics(samples)
    
    def _create_resource_metrics(self, sample: Dict):
        """Cria m√©tricas de recursos"""
        # CPU
        if sample['cpu_percent'] > self.thresholds['cpu_usage']:
            metric = PerformanceMetric(
                name="CPU Usage",
                value=sample['cpu_percent'],
                unit="%",
                timestamp=sample['timestamp'],
                category="cpu",
                severity="HIGH" if sample['cpu_percent'] > 90 else "MEDIUM",
                description=f"CPU usage at {sample['cpu_percent']:.1f}%"
            )
            self.metrics.append(metric)
        
        # Mem√≥ria
        if sample['memory_percent'] > self.thresholds['memory_usage']:
            metric = PerformanceMetric(
                name="Memory Usage",
                value=sample['memory_percent'],
                unit="%",
                timestamp=sample['timestamp'],
                category="memory",
                severity="HIGH" if sample['memory_percent'] > 95 else "MEDIUM",
                description=f"Memory usage at {sample['memory_percent']:.1f}%"
            )
            self.metrics.append(metric)
        
        # Disco
        if sample['disk_percent'] > 90:
            metric = PerformanceMetric(
                name="Disk Usage",
                value=sample['disk_percent'],
                unit="%",
                timestamp=sample['timestamp'],
                category="disk",
                severity="HIGH" if sample['disk_percent'] > 95 else "MEDIUM",
                description=f"Disk usage at {sample['disk_percent']:.1f}%"
            )
            self.metrics.append(metric)
    
    def _analyze_resource_statistics(self, samples: List[Dict]):
        """Analisa estat√≠sticas de recursos"""
        if not samples:
            return
        
        # CPU
        cpu_values = [s['cpu_percent'] for s in samples]
        self.resource_usage['cpu'] = {
            'avg': statistics.mean(cpu_values),
            'max': max(cpu_values),
            'min': min(cpu_values),
            'std': statistics.stdev(cpu_values) if len(cpu_values) > 1 else 0
        }
        
        # Mem√≥ria
        memory_values = [s['memory_percent'] for s in samples]
        self.resource_usage['memory'] = {
            'avg': statistics.mean(memory_values),
            'max': max(memory_values),
            'min': min(memory_values),
            'std': statistics.stdev(memory_values) if len(memory_values) > 1 else 0
        }
        
        # Disco
        disk_values = [s['disk_percent'] for s in samples]
        self.resource_usage['disk'] = {
            'avg': statistics.mean(disk_values),
            'max': max(disk_values),
            'min': min(disk_values),
            'std': statistics.stdev(disk_values) if len(disk_values) > 1 else 0
        }
    
    def _analyze_code_performance(self):
        """
        üñºÔ∏è Visual - An√°lise de Performance de C√≥digo
        
        Representa hotspots de performance no c√≥digo
        """
        logger.info("üîç Analisando performance de c√≥digo Python...")
        
        # Encontrar arquivos Python principais
        python_files = list(self.project_root.rglob("*.py"))
        main_files = [f for f in python_files if 'main' in f.name.lower() or 'app' in f.name.lower()]
        
        for file_path in main_files[:5]:  # Limitar a 5 arquivos principais
            try:
                self._profile_python_file(file_path)
            except Exception as e:
                logger.warning(f"‚ö†Ô∏è Erro ao analisar {file_path}: {e}")
    
    def _profile_python_file(self, file_path: Path):
        """Profila um arquivo Python"""
        try:
            # Criar c√≥digo de teste simples
            test_code = f"""
import sys
sys.path.append('{self.project_root}')
import {file_path.stem}
"""
            
            # Profiling
            pr = cProfile.Profile()
            pr.enable()
            
            # Executar c√≥digo
            exec(test_code)
            
            pr.disable()
            
            # Analisar resultados
            s = io.StringIO()
            ps = pstats.Stats(pr, stream=s).sort_stats('cumulative')
            ps.print_stats(10)  # Top 10 fun√ß√µes
            
            # Extrair m√©tricas
            stats = pr.getstats()
            for stat in stats[:5]:  # Top 5
                if stat.tottime > 0.1:  # Mais de 100ms
                    metric = PerformanceMetric(
                        name=f"Function: {stat.code.co_name}",
                        value=stat.tottime * 1000,  # Converter para ms
                        unit="ms",
                        timestamp=datetime.now(),
                        category="code",
                        severity="HIGH" if stat.tottime > 1.0 else "MEDIUM",
                        description=f"Function {stat.code.co_name} took {stat.tottime*1000:.2f}ms"
                    )
                    self.metrics.append(metric)
        
        except Exception as e:
            logger.warning(f"‚ö†Ô∏è Erro no profiling de {file_path}: {e}")
    
    def _analyze_database_performance(self):
        """Analisa performance de banco de dados"""
        logger.info("üóÑÔ∏è Analisando performance de banco de dados...")
        
        # Verificar se existe banco SQLite
        db_files = list(self.project_root.rglob("*.sqlite3")) + list(self.project_root.rglob("*.db"))
        
        for db_file in db_files[:3]:  # Limitar a 3 bancos
            try:
                self._profile_sqlite_database(db_file)
            except Exception as e:
                logger.warning(f"‚ö†Ô∏è Erro ao analisar banco {db_file}: {e}")
    
    def _profile_sqlite_database(self, db_file: Path):
        """Profila banco SQLite"""
        try:
            conn = sqlite3.connect(db_file)
            cursor = conn.cursor()
            
            # Listar tabelas
            cursor.execute("SELECT name FROM sqlite_master WHERE type='table';")
            tables = cursor.fetchall()
            
            for table in tables[:5]:  # Limitar a 5 tabelas
                table_name = table[0]
                
                # Contar registros
                start_time = time.time()
                cursor.execute(f"SELECT COUNT(*) FROM {table_name}")
                count = cursor.fetchone()[0]
                query_time = (time.time() - start_time) * 1000
                
                if query_time > self.thresholds['database_query_time']:
                    metric = PerformanceMetric(
                        name=f"Database Query: COUNT {table_name}",
                        value=query_time,
                        unit="ms",
                        timestamp=datetime.now(),
                        category="database",
                        severity="HIGH" if query_time > 2000 else "MEDIUM",
                        description=f"COUNT query on {table_name} took {query_time:.2f}ms ({count} records)"
                    )
                    self.metrics.append(metric)
                
                # Analisar tamanho da tabela
                if count > 10000:  # Tabela grande
                    metric = PerformanceMetric(
                        name=f"Large Table: {table_name}",
                        value=count,
                        unit="records",
                        timestamp=datetime.now(),
                        category="database",
                        severity="MEDIUM",
                        description=f"Table {table_name} has {count} records - consider indexing"
                    )
                    self.metrics.append(metric)
            
            conn.close()
        
        except Exception as e:
            logger.warning(f"‚ö†Ô∏è Erro ao analisar banco {db_file}: {e}")
    
    def _analyze_api_performance(self):
        """Analisa performance de APIs"""
        logger.info("üåê Analisando performance de APIs...")
        
        # Verificar se h√° servidor rodando
        try:
            # Testar endpoints comuns
            endpoints = [
                "http://localhost:8000/",
                "http://localhost:5000/",
                "http://localhost:3000/",
                "http://127.0.0.1:8000/",
                "http://127.0.0.1:5000/"
            ]
            
            for endpoint in endpoints:
                try:
                    start_time = time.time()
                    response = requests.get(endpoint, timeout=5)
                    response_time = (time.time() - start_time) * 1000
                    
                    if response_time > self.thresholds['api_response_time']:
                        metric = PerformanceMetric(
                            name=f"API Response: {endpoint}",
                            value=response_time,
                            unit="ms",
                            timestamp=datetime.now(),
                            category="api",
                            severity="HIGH" if response_time > 1000 else "MEDIUM",
                            description=f"API {endpoint} responded in {response_time:.2f}ms (status: {response.status_code})"
                        )
                        self.metrics.append(metric)
                
                except requests.exceptions.RequestException:
                    continue  # Endpoint n√£o dispon√≠vel
        
        except Exception as e:
            logger.warning(f"‚ö†Ô∏è Erro ao analisar APIs: {e}")
    
    def _run_load_tests(self):
        """Executa testes de carga"""
        logger.info("‚ö° Executando testes de carga...")
        
        # Simular carga de CPU
        def cpu_load():
            start_time = time.time()
            while time.time() - start_time < self.load_test_duration:
                # Opera√ß√£o intensiva de CPU
                sum(range(1000000))
        
        # Simular carga de mem√≥ria
        def memory_load():
            start_time = time.time()
            data = []
            while time.time() - start_time < self.load_test_duration:
                data.append([0] * 10000)  # 10KB por itera√ß√£o
                if len(data) > 1000:  # Limitar uso de mem√≥ria
                    data = data[-100:]
        
        # Executar testes em paralelo
        with ThreadPoolExecutor(max_workers=self.concurrent_users) as executor:
            futures = []
            
            # CPU load tests
            for _ in range(self.concurrent_users // 2):
                futures.append(executor.submit(cpu_load))
            
            # Memory load tests
            for _ in range(self.concurrent_users // 2):
                futures.append(executor.submit(memory_load))
            
            # Aguardar conclus√£o
            for future in as_completed(futures):
                try:
                    future.result()
                except Exception as e:
                    logger.warning(f"‚ö†Ô∏è Erro no teste de carga: {e}")
        
        # Coletar m√©tricas durante teste de carga
        self._analyze_system_resources()
    
    def _analyze_bottlenecks(self):
        """Analisa gargalos identificados"""
        logger.info("üîç Analisando gargalos de performance...")
        
        # Agrupar m√©tricas por categoria
        for metric in self.metrics:
            if metric.severity in ["HIGH", "CRITICAL"]:
                self.bottlenecks[metric.category].append(metric)
        
        # An√°lise espec√≠fica por categoria
        self._analyze_cpu_bottlenecks()
        self._analyze_memory_bottlenecks()
        self._analyze_disk_bottlenecks()
        self._analyze_network_bottlenecks()
        self._analyze_database_bottlenecks()
        self._analyze_api_bottlenecks()
    
    def _analyze_cpu_bottlenecks(self):
        """Analisa gargalos de CPU"""
        cpu_metrics = self.bottlenecks.get("cpu", [])
        if cpu_metrics:
            avg_cpu = statistics.mean([m.value for m in cpu_metrics])
            if avg_cpu > 90:
                metric = PerformanceMetric(
                    name="Critical CPU Bottleneck",
                    value=avg_cpu,
                    unit="%",
                    timestamp=datetime.now(),
                    category="cpu",
                    severity="CRITICAL",
                    description=f"Average CPU usage during benchmark: {avg_cpu:.1f}%"
                )
                self.metrics.append(metric)
    
    def _analyze_memory_bottlenecks(self):
        """Analisa gargalos de mem√≥ria"""
        memory_metrics = self.bottlenecks.get("memory", [])
        if memory_metrics:
            avg_memory = statistics.mean([m.value for m in memory_metrics])
            if avg_memory > 95:
                metric = PerformanceMetric(
                    name="Critical Memory Bottleneck",
                    value=avg_memory,
                    unit="%",
                    timestamp=datetime.now(),
                    category="memory",
                    severity="CRITICAL",
                    description=f"Average memory usage during benchmark: {avg_memory:.1f}%"
                )
                self.metrics.append(metric)
    
    def _analyze_disk_bottlenecks(self):
        """Analisa gargalos de disco"""
        disk_metrics = self.bottlenecks.get("disk", [])
        if disk_metrics:
            avg_disk = statistics.mean([m.value for m in disk_metrics])
            if avg_disk > 95:
                metric = PerformanceMetric(
                    name="Critical Disk Bottleneck",
                    value=avg_disk,
                    unit="%",
                    timestamp=datetime.now(),
                    category="disk",
                    severity="CRITICAL",
                    description=f"Average disk usage during benchmark: {avg_disk:.1f}%"
                )
                self.metrics.append(metric)
    
    def _analyze_network_bottlenecks(self):
        """Analisa gargalos de rede"""
        network_metrics = self.bottlenecks.get("network", [])
        if network_metrics:
            avg_latency = statistics.mean([m.value for m in network_metrics])
            if avg_latency > 200:
                metric = PerformanceMetric(
                    name="Critical Network Bottleneck",
                    value=avg_latency,
                    unit="ms",
                    timestamp=datetime.now(),
                    category="network",
                    severity="CRITICAL",
                    description=f"Average network latency during benchmark: {avg_latency:.2f}ms"
                )
                self.metrics.append(metric)
    
    def _analyze_database_bottlenecks(self):
        """Analisa gargalos de banco de dados"""
        db_metrics = self.bottlenecks.get("database", [])
        if db_metrics:
            avg_query_time = statistics.mean([m.value for m in db_metrics])
            if avg_query_time > 2000:
                metric = PerformanceMetric(
                    name="Critical Database Bottleneck",
                    value=avg_query_time,
                    unit="ms",
                    timestamp=datetime.now(),
                    category="database",
                    severity="CRITICAL",
                    description=f"Average query time during benchmark: {avg_query_time:.2f}ms"
                )
                self.metrics.append(metric)
    
    def _analyze_api_bottlenecks(self):
        """Analisa gargalos de API"""
        api_metrics = self.bottlenecks.get("api", [])
        if api_metrics:
            avg_response_time = statistics.mean([m.value for m in api_metrics])
            if avg_response_time > 1000:
                metric = PerformanceMetric(
                    name="Critical API Bottleneck",
                    value=avg_response_time,
                    unit="ms",
                    timestamp=datetime.now(),
                    category="api",
                    severity="CRITICAL",
                    description=f"Average API response time during benchmark: {avg_response_time:.2f}ms"
                )
                self.metrics.append(metric)
    
    def _calculate_performance_score(self):
        """Calcula score geral de performance"""
        logger.info("üìä Calculando score de performance...")
        
        if not self.metrics:
            self.performance_score = 1.0
            return
        
        # Contar problemas por severidade
        critical_count = len([m for m in self.metrics if m.severity == "CRITICAL"])
        high_count = len([m for m in self.metrics if m.severity == "HIGH"])
        medium_count = len([m for m in self.metrics if m.severity == "MEDIUM"])
        low_count = len([m for m in self.metrics if m.severity == "LOW"])
        
        # Calcular score (0.0 = pior, 1.0 = melhor)
        total_issues = critical_count + high_count + medium_count + low_count
        if total_issues == 0:
            self.performance_score = 1.0
        else:
            # Peso por severidade
            weighted_score = (
                critical_count * 0.0 +  # Cr√≠tico = 0 pontos
                high_count * 0.3 +      # Alto = 0.3 pontos
                medium_count * 0.6 +    # M√©dio = 0.6 pontos
                low_count * 0.9         # Baixo = 0.9 pontos
            )
            self.performance_score = 1.0 - (weighted_score / total_issues)
    
    def _generate_benchmark_result(self) -> PerformanceResult:
        """Gera resultado do benchmark"""
        # Estat√≠sticas
        total_metrics = len(self.metrics)
        critical_issues = len([m for m in self.metrics if m.severity == "CRITICAL"])
        high_priority_issues = len([m for m in self.metrics if m.severity in ["CRITICAL", "HIGH"]])
        
        # Por categoria
        cpu_bottlenecks = len(self.bottlenecks.get("cpu", []))
        memory_bottlenecks = len(self.bottlenecks.get("memory", []))
        disk_bottlenecks = len(self.bottlenecks.get("disk", []))
        network_bottlenecks = len(self.bottlenecks.get("network", []))
        database_bottlenecks = len(self.bottlenecks.get("database", []))
        api_bottlenecks = len(self.bottlenecks.get("api", []))
        
        # Recomenda√ß√µes
        recommendations = self._generate_recommendations()
        
        # Potencial de otimiza√ß√£o
        optimization_potential = 1.0 - self.performance_score
        
        return PerformanceResult(
            total_metrics=total_metrics,
            critical_issues=critical_issues,
            high_priority_issues=high_priority_issues,
            cpu_bottlenecks=cpu_bottlenecks,
            memory_bottlenecks=memory_bottlenecks,
            disk_bottlenecks=disk_bottlenecks,
            network_bottlenecks=network_bottlenecks,
            database_bottlenecks=database_bottlenecks,
            api_bottlenecks=api_bottlenecks,
            recommendations=recommendations,
            performance_score=self.performance_score,
            bottleneck_analysis=dict(self.bottlenecks),
            resource_usage=dict(self.resource_usage),
            optimization_potential=optimization_potential
        )
    
    def _generate_recommendations(self) -> List[str]:
        """Gera recomenda√ß√µes baseadas na an√°lise"""
        recommendations = []
        
        # Por criticidade
        critical_issues = [m for m in self.metrics if m.severity == "CRITICAL"]
        if critical_issues:
            recommendations.append(f"üö® {len(critical_issues)} problemas cr√≠ticos identificados - a√ß√£o imediata necess√°ria")
        
        # Por categoria
        cpu_issues = [m for m in self.metrics if m.category == "cpu" and m.severity in ["HIGH", "CRITICAL"]]
        if cpu_issues:
            recommendations.append(f"‚ö° {len(cpu_issues)} gargalos de CPU - considerar otimiza√ß√£o de algoritmos")
        
        memory_issues = [m for m in self.metrics if m.category == "memory" and m.severity in ["HIGH", "CRITICAL"]]
        if memory_issues:
            recommendations.append(f"üíæ {len(memory_issues)} gargalos de mem√≥ria - considerar garbage collection e otimiza√ß√£o")
        
        disk_issues = [m for m in self.metrics if m.category == "disk" and m.severity in ["HIGH", "CRITICAL"]]
        if disk_issues:
            recommendations.append(f"üíø {len(disk_issues)} gargalos de disco - considerar SSD e otimiza√ß√£o de I/O")
        
        database_issues = [m for m in self.metrics if m.category == "database" and m.severity in ["HIGH", "CRITICAL"]]
        if database_issues:
            recommendations.append(f"üóÑÔ∏è {len(database_issues)} gargalos de banco - considerar √≠ndices e otimiza√ß√£o de queries")
        
        api_issues = [m for m in self.metrics if m.category == "api" and m.severity in ["HIGH", "CRITICAL"]]
        if api_issues:
            recommendations.append(f"üåê {len(api_issues)} gargalos de API - considerar cache e otimiza√ß√£o de endpoints")
        
        # Por score de performance
        if self.performance_score < 0.5:
            recommendations.append("‚ö†Ô∏è Score de performance baixo - revis√£o completa necess√°ria")
        elif self.performance_score < 0.8:
            recommendations.append("üìà Score de performance moderado - otimiza√ß√µes recomendadas")
        else:
            recommendations.append("‚úÖ Score de performance bom - manter monitoramento")
        
        return recommendations
    
    def save_benchmark_results(self, result: PerformanceResult, output_file: str):
        """Salva resultados do benchmark"""
        logger.info(f"üíæ Salvando resultados em {output_file}...")
        
        # Converter para formato serializ√°vel
        result_data = {
            "metadata": {
                "tracing_id": "PERFORMANCE_BENCHMARK_20250127_001",
                "timestamp": datetime.now().isoformat(),
                "version": "1.0.0",
                "project": "Omni Keywords Finder"
            },
            "summary": {
                "total_metrics": result.total_metrics,
                "critical_issues": result.critical_issues,
                "high_priority_issues": result.high_priority_issues,
                "performance_score": result.performance_score,
                "optimization_potential": result.optimization_potential
            },
            "bottlenecks": {
                "cpu": result.cpu_bottlenecks,
                "memory": result.memory_bottlenecks,
                "disk": result.disk_bottlenecks,
                "network": result.network_bottlenecks,
                "database": result.database_bottlenecks,
                "api": result.api_bottlenecks
            },
            "resource_usage": result.resource_usage,
            "critical_metrics": [
                {
                    "name": metric.name,
                    "value": metric.value,
                    "unit": metric.unit,
                    "category": metric.category,
                    "severity": metric.severity,
                    "description": metric.description
                }
                for metric in self.metrics
                if metric.severity in ["HIGH", "CRITICAL"]
            ],
            "recommendations": result.recommendations
        }
        
        with open(output_file, 'w', encoding='utf-8') as f:
            json.dump(result_data, f, indent=2, ensure_ascii=False)
        
        logger.info(f"‚úÖ Resultados salvos em {output_file}")

def main():
    """
    üéØ Fun√ß√£o principal do benchmark
    
    Executa benchmark completo seguindo abordagens:
    - CoCoT: An√°lise fundamentada
    - ToT: M√∫ltiplas estrat√©gias
    - ReAct: Simula√ß√£o de impactos
    - Visual: Representa√ß√µes claras
    """
    logger.info("üöÄ Iniciando Benchmark Completo de Performance")
    logger.info("üìê CoCoT: An√°lise fundamentada em boas pr√°ticas")
    logger.info("üå≤ ToT: M√∫ltiplas estrat√©gias de an√°lise")
    logger.info("‚ôªÔ∏è ReAct: Simula√ß√£o de impactos e riscos")
    logger.info("üñºÔ∏è Visual: Representa√ß√µes claras dos resultados")
    
    # Configura√ß√£o
    project_root = os.getcwd()
    output_file = "performance_benchmark_results.json"
    
    try:
        # Inicializar benchmark
        benchmark = PerformanceBenchmark(project_root)
        
        # Executar benchmark
        logger.info("üîç Etapa 1: Executando benchmark completo...")
        result = benchmark.run_complete_benchmark()
        
        # Salvar resultados
        logger.info("üíæ Etapa 2: Salvando resultados...")
        benchmark.save_benchmark_results(result, output_file)
        
        # Exibir resumo
        logger.info("üéØ RESUMO DO BENCHMARK:")
        logger.info(f"   üìä Total de m√©tricas: {result.total_metrics}")
        logger.info(f"   üö® Problemas cr√≠ticos: {result.critical_issues}")
        logger.info(f"   ‚ö†Ô∏è Problemas de alta prioridade: {result.high_priority_issues}")
        logger.info(f"   ‚ö° Gargalos de CPU: {result.cpu_bottlenecks}")
        logger.info(f"   üíæ Gargalos de mem√≥ria: {result.memory_bottlenecks}")
        logger.info(f"   üíø Gargalos de disco: {result.disk_bottlenecks}")
        logger.info(f"   üåê Gargalos de rede: {result.network_bottlenecks}")
        logger.info(f"   üóÑÔ∏è Gargalos de banco: {result.database_bottlenecks}")
        logger.info(f"   üåê Gargalos de API: {result.api_bottlenecks}")
        logger.info(f"   üìà Score de performance: {result.performance_score:.2f}")
        logger.info(f"   üéØ Potencial de otimiza√ß√£o: {result.optimization_potential:.1%}")
        logger.info(f"   üìã Recomenda√ß√µes: {len(result.recommendations)}")
        
        logger.info("‚úÖ Benchmark de performance conclu√≠do com sucesso!")
        return 0
        
    except Exception as e:
        logger.error(f"‚ùå Erro durante benchmark: {e}")
        return 1

if __name__ == "__main__":
    sys.exit(main()) 