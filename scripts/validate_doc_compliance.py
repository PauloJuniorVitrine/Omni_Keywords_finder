#!/usr/bin/env python3
"""
üîç Documenta√ß√£o Compliance Validator Script
üéØ Objetivo: Validar conformidade de documenta√ß√£o enterprise
üìä Tracing ID: DOC_COMPLIANCE_VALIDATOR_20250127_001
üìÖ Data: 2025-01-27
üîß Status: Implementa√ß√£o Inicial
"""

import os
import sys
import json
import logging
import argparse
from pathlib import Path
from datetime import datetime
from typing import Dict, List, Any, Optional, Tuple
import traceback
from dataclasses import dataclass, asdict

# Adicionar diret√≥rio raiz ao path
sys.path.insert(0, str(Path(__file__).parent.parent))

from shared.doc_generation_metrics import DocGenerationMetrics, MetricsCollector, MetricsAnalyzer
from infrastructure.validation.doc_quality_score import DocQualityAnalyzer
from infrastructure.security.advanced_security_system import SensitiveDataDetector
from shared.trigger_config_validator import TriggerConfigValidator
from infrastructure.ml.semantic_embeddings import SemanticEmbeddingService
from shared.compliance_validator import ComplianceValidator
from infrastructure.backup.rollback_system import RollbackSystem


@dataclass
class ComplianceReport:
    """Relat√≥rio de conformidade de documenta√ß√£o"""
    timestamp: str
    tracing_id: str
    overall_score: float
    quality_score: float
    security_score: float
    compliance_score: float
    performance_score: float
    issues: List[Dict[str, Any]]
    recommendations: List[str]
    status: str  # PASS, FAIL, WARNING


class DocComplianceValidator:
    """
    Validador principal de conformidade de documenta√ß√£o enterprise
    """
    
    def __init__(self, config_path: str = "docs/trigger_config.json"):
        self.tracing_id = f"DOC_COMPLIANCE_{datetime.now().strftime('%Y%m%d_%H%M%S')}"
        self.config_path = config_path
        self.setup_logging()
        
        # Inicializar servi√ßos
        self.quality_analyzer = DocQualityAnalyzer()
        self.security_detector = SensitiveDataDetector()
        self.metrics_collector = MetricsCollector()
        self.trigger_validator = TriggerConfigValidator()
        self.compliance_validator = ComplianceValidator()
        self.rollback_system = RollbackSystem()
        self.semantic_service = SemanticEmbeddingService()
        
        # Configura√ß√µes
        self.quality_threshold = 0.85
        self.security_threshold = 1.0  # Zero toler√¢ncia
        self.compliance_threshold = 0.90
        self.performance_threshold = 0.80
        
        self.logger.info(f"[{self.tracing_id}] Inicializado validador de conformidade")
    
    def setup_logging(self):
        """Configurar sistema de logging"""
        self.logger = logging.getLogger(f"DocComplianceValidator_{self.tracing_id}")
        self.logger.setLevel(logging.INFO)
        
        # Handler para console
        console_handler = logging.StreamHandler()
        console_handler.setLevel(logging.INFO)
        
        # Formato
        formatter = logging.Formatter(
            '[%(levelname)string_data] [%(name)string_data] %(message)string_data - %(asctime)string_data'
        )
        console_handler.setFormatter(formatter)
        
        self.logger.addHandler(console_handler)
    
    def validate_quality(self, docs_path: str = "docs/") -> Tuple[float, List[Dict]]:
        """
        Validar qualidade da documenta√ß√£o
        """
        self.logger.info(f"[{self.tracing_id}] Iniciando valida√ß√£o de qualidade")
        
        issues = []
        total_score = 0.0
        doc_count = 0
        
        docs_dir = Path(docs_path)
        if not docs_dir.exists():
            issues.append({
                "type": "CRITICAL",
                "message": f"Diret√≥rio de documenta√ß√£o n√£o encontrado: {docs_path}",
                "file": docs_path
            })
            return 0.0, issues
        
        # Analisar cada arquivo de documenta√ß√£o
        for doc_file in docs_dir.rglob("*.md"):
            try:
                with open(doc_file, 'r', encoding='utf-8') as f:
                    content = f.read()
                
                # Calcular qualidade
                quality_score = self.quality_analyzer.calculate_doc_quality_score(content)
                total_score += quality_score
                doc_count += 1
                
                if quality_score < self.quality_threshold:
                    issues.append({
                        "type": "WARNING",
                        "message": f"Qualidade abaixo do threshold: {quality_score:.2f}",
                        "file": str(doc_file),
                        "score": quality_score,
                        "threshold": self.quality_threshold
                    })
                
                self.logger.info(f"[{self.tracing_id}] {doc_file}: {quality_score:.2f}")
                
            except Exception as e:
                issues.append({
                    "type": "ERROR",
                    "message": f"Erro ao analisar arquivo: {str(e)}",
                    "file": str(doc_file)
                })
        
        avg_quality = total_score / doc_count if doc_count > 0 else 0.0
        self.logger.info(f"[{self.tracing_id}] Qualidade m√©dia: {avg_quality:.2f}")
        
        return avg_quality, issues
    
    def validate_security(self, docs_path: str = "docs/") -> Tuple[float, List[Dict]]:
        """
        Validar seguran√ßa da documenta√ß√£o
        """
        self.logger.info(f"[{self.tracing_id}] Iniciando valida√ß√£o de seguran√ßa")
        
        issues = []
        security_score = 1.0  # Come√ßa com score perfeito
        
        docs_dir = Path(docs_path)
        if not docs_dir.exists():
            return 0.0, issues
        
        # Escanear documenta√ß√£o
        for doc_file in docs_dir.rglob("*.md"):
            try:
                with open(doc_file, 'r', encoding='utf-8') as f:
                    content = f.read()
                
                # Detectar dados sens√≠veis
                sensitive_data = self.security_detector.scan_documentation(content)
                
                if sensitive_data:
                    security_score = 0.0  # Falha cr√≠tica
                    for data in sensitive_data:
                        issues.append({
                            "type": "CRITICAL",
                            "message": f"Dados sens√≠veis detectados: {data['type']}",
                            "file": str(doc_file),
                            "line": data.get('line', 'N/A'),
                            "pattern": data.get('pattern', 'N/A')
                        })
                
            except Exception as e:
                issues.append({
                    "type": "ERROR",
                    "message": f"Erro ao escanear arquivo: {str(e)}",
                    "file": str(doc_file)
                })
        
        self.logger.info(f"[{self.tracing_id}] Score de seguran√ßa: {security_score:.2f}")
        return security_score, issues
    
    def validate_compliance(self) -> Tuple[float, List[Dict]]:
        """
        Validar compliance (PCI-DSS, LGPD, etc.)
        """
        self.logger.info(f"[{self.tracing_id}] Iniciando valida√ß√£o de compliance")
        
        issues = []
        
        try:
            # Validar configura√ß√µes de compliance
            compliance_result = self.compliance_validator.validate_all_standards()
            compliance_score = compliance_result.get('overall_score', 0.0)
            
            # Verificar viola√ß√µes
            violations = compliance_result.get('violations', [])
            for violation in violations:
                issues.append({
                    "type": "WARNING" if violation['severity'] == 'medium' else "CRITICAL",
                    "message": f"Violation de compliance: {violation['description']}",
                    "standard": violation.get('standard', 'N/A'),
                    "severity": violation.get('severity', 'unknown')
                })
            
            self.logger.info(f"[{self.tracing_id}] Score de compliance: {compliance_score:.2f}")
            return compliance_score, issues
            
        except Exception as e:
            issues.append({
                "type": "ERROR",
                "message": f"Erro na valida√ß√£o de compliance: {str(e)}"
            })
            return 0.0, issues
    
    def validate_performance(self) -> Tuple[float, List[Dict]]:
        """
        Validar m√©tricas de performance
        """
        self.logger.info(f"[{self.tracing_id}] Iniciando valida√ß√£o de performance")
        
        issues = []
        performance_score = 1.0
        
        try:
            # Coletar m√©tricas
            metrics = self.metrics_collector.collect_all_metrics()
            
            # Validar tempo de gera√ß√£o (< 5 minutos)
            generation_time = metrics.get('avg_generation_time', 0)
            if generation_time > 300:  # 5 minutos
                performance_score -= 0.2
                issues.append({
                    "type": "WARNING",
                    "message": f"Tempo de gera√ß√£o alto: {generation_time:.2f}string_data",
                    "threshold": 300
                })
            
            # Validar tokens consumidos (< 10.000)
            tokens_used = metrics.get('avg_tokens_used', 0)
            if tokens_used > 10000:
                performance_score -= 0.2
                issues.append({
                    "type": "WARNING",
                    "message": f"Tokens consumidos alto: {tokens_used}",
                    "threshold": 10000
                })
            
            # Validar cobertura de documenta√ß√£o (> 95%)
            doc_coverage = metrics.get('documentation_coverage', 0)
            if doc_coverage < 0.95:
                performance_score -= 0.3
                issues.append({
                    "type": "WARNING",
                    "message": f"Cobertura de documenta√ß√£o baixa: {doc_coverage:.2%}",
                    "threshold": "95%"
                })
            
            # Garantir score m√≠nimo
            performance_score = max(0.0, performance_score)
            
            self.logger.info(f"[{self.tracing_id}] Score de performance: {performance_score:.2f}")
            return performance_score, issues
            
        except Exception as e:
            issues.append({
                "type": "ERROR",
                "message": f"Erro na valida√ß√£o de performance: {str(e)}"
            })
            return 0.0, issues
    
    def validate_trigger_config(self) -> Tuple[float, List[Dict]]:
        """
        Validar configura√ß√£o de triggers
        """
        self.logger.info(f"[{self.tracing_id}] Iniciando valida√ß√£o de trigger config")
        
        issues = []
        config_score = 1.0
        
        try:
            # Validar arquivo de configura√ß√£o
            validation_result = self.trigger_validator.validate_config_file(self.config_path)
            
            if not validation_result['is_valid']:
                config_score = 0.0
                for error in validation_result.get('errors', []):
                    issues.append({
                        "type": "CRITICAL",
                        "message": f"Erro na configura√ß√£o: {error}",
                        "config_file": self.config_path
                    })
            
            self.logger.info(f"[{self.tracing_id}] Score de configura√ß√£o: {config_score:.2f}")
            return config_score, issues
            
        except Exception as e:
            issues.append({
                "type": "ERROR",
                "message": f"Erro na valida√ß√£o de configura√ß√£o: {str(e)}"
            })
            return 0.0, issues
    
    def generate_recommendations(self, issues: List[Dict]) -> List[str]:
        """
        Gerar recomenda√ß√µes baseadas nos problemas encontrados
        """
        recommendations = []
        
        # Agrupar por tipo
        critical_issues = [index for index in issues if index['type'] == 'CRITICAL']
        warning_issues = [index for index in issues if index['type'] == 'WARNING']
        
        if critical_issues:
            recommendations.append("üî¥ CR√çTICO: Corrija imediatamente os problemas de seguran√ßa e compliance")
        
        if warning_issues:
            recommendations.append("üü° ATEN√á√ÉO: Melhore a qualidade e performance da documenta√ß√£o")
        
        # Recomenda√ß√µes espec√≠ficas
        for issue in issues:
            if "dados sens√≠veis" in issue.get('message', '').lower():
                recommendations.append("üîê Remova dados sens√≠veis da documenta√ß√£o")
            
            if "qualidade" in issue.get('message', '').lower():
                recommendations.append("üìù Melhore a qualidade da documenta√ß√£o")
            
            if "performance" in issue.get('message', '').lower():
                recommendations.append("‚ö° Otimize a performance da gera√ß√£o de documenta√ß√£o")
        
        return list(set(recommendations))  # Remover duplicatas
    
    def determine_status(self, scores: Dict[str, float]) -> str:
        """
        Determinar status geral baseado nos scores
        """
        if scores['security'] < self.security_threshold:
            return "FAIL"  # Falha cr√≠tica de seguran√ßa
        
        if scores['compliance'] < self.compliance_threshold:
            return "FAIL"  # Falha de compliance
        
        if scores['quality'] < self.quality_threshold:
            return "WARNING"  # Aviso de qualidade
        
        if scores['performance'] < self.performance_threshold:
            return "WARNING"  # Aviso de performance
        
        return "PASS"  # Tudo OK
    
    def run_validation(self, docs_path: str = "docs/") -> ComplianceReport:
        """
        Executar valida√ß√£o completa
        """
        self.logger.info(f"[{self.tracing_id}] Iniciando valida√ß√£o completa")
        
        start_time = datetime.now()
        
        # Executar todas as valida√ß√µes
        quality_score, quality_issues = self.validate_quality(docs_path)
        security_score, security_issues = self.validate_security(docs_path)
        compliance_score, compliance_issues = self.validate_compliance()
        performance_score, performance_issues = self.validate_performance()
        config_score, config_issues = self.validate_trigger_config()
        
        # Consolidar issues
        all_issues = quality_issues + security_issues + compliance_issues + performance_issues + config_issues
        
        # Calcular score geral (m√©dia ponderada)
        scores = {
            'quality': quality_score,
            'security': security_score,
            'compliance': compliance_score,
            'performance': performance_score,
            'config': config_score
        }
        
        overall_score = (
            quality_score * 0.3 +
            security_score * 0.3 +
            compliance_score * 0.25 +
            performance_score * 0.1 +
            config_score * 0.05
        )
        
        # Determinar status
        status = self.determine_status(scores)
        
        # Gerar recomenda√ß√µes
        recommendations = self.generate_recommendations(all_issues)
        
        # Criar relat√≥rio
        report = ComplianceReport(
            timestamp=datetime.now().isoformat(),
            tracing_id=self.tracing_id,
            overall_score=overall_score,
            quality_score=quality_score,
            security_score=security_score,
            compliance_score=compliance_score,
            performance_score=performance_score,
            issues=all_issues,
            recommendations=recommendations,
            status=status
        )
        
        end_time = datetime.now()
        duration = (end_time - start_time).total_seconds()
        
        self.logger.info(f"[{self.tracing_id}] Valida√ß√£o conclu√≠da em {duration:.2f}string_data")
        self.logger.info(f"[{self.tracing_id}] Status: {status}")
        self.logger.info(f"[{self.tracing_id}] Score geral: {overall_score:.2f}")
        
        return report
    
    def save_report(self, report: ComplianceReport, output_path: str = "logs/compliance_report.json"):
        """
        Salvar relat√≥rio em arquivo JSON
        """
        try:
            # Criar diret√≥rio se n√£o existir
            output_dir = Path(output_path).parent
            output_dir.mkdir(parents=True, exist_ok=True)
            
            # Converter para dict
            report_dict = asdict(report)
            
            # Salvar arquivo
            with open(output_path, 'w', encoding='utf-8') as f:
                json.dump(report_dict, f, indent=2, ensure_ascii=False)
            
            self.logger.info(f"[{self.tracing_id}] Relat√≥rio salvo em: {output_path}")
            
        except Exception as e:
            self.logger.error(f"[{self.tracing_id}] Erro ao salvar relat√≥rio: {str(e)}")
    
    def print_summary(self, report: ComplianceReport):
        """
        Imprimir resumo do relat√≥rio
        """
        print("\n" + "="*80)
        print("üìã RELAT√ìRIO DE CONFORMIDADE DE DOCUMENTA√á√ÉO ENTERPRISE")
        print("="*80)
        print(f"Tracing ID: {report.tracing_id}")
        print(f"Timestamp: {report.timestamp}")
        print(f"Status: {report.status}")
        print(f"Score Geral: {report.overall_score:.2f}")
        print("-"*80)
        print("üìä SCORES DETALHADOS:")
        print(f"  Qualidade: {report.quality_score:.2f}")
        print(f"  Seguran√ßa: {report.security_score:.2f}")
        print(f"  Compliance: {report.compliance_score:.2f}")
        print(f"  Performance: {report.performance_score:.2f}")
        print("-"*80)
        
        if report.issues:
            print("üö® PROBLEMAS ENCONTRADOS:")
            for index, issue in enumerate(report.issues, 1):
                print(f"  {index}. [{issue['type']}] {issue['message']}")
                if 'file' in issue:
                    print(f"     Arquivo: {issue['file']}")
        else:
            print("‚úÖ Nenhum problema encontrado!")
        
        if report.recommendations:
            print("-"*80)
            print("üí° RECOMENDA√á√ïES:")
            for index, rec in enumerate(report.recommendations, 1):
                print(f"  {index}. {rec}")
        
        print("="*80)


def main():
    """
    Fun√ß√£o principal do script
    """
    parser = argparse.ArgumentParser(
        description="Validador de Conformidade de Documenta√ß√£o Enterprise"
    )
    parser.add_argument(
        "--docs-path",
        default="docs/",
        help="Caminho para a documenta√ß√£o (padr√£o: docs/)"
    )
    parser.add_argument(
        "--config-path",
        default="docs/trigger_config.json",
        help="Caminho para configura√ß√£o de triggers (padr√£o: docs/trigger_config.json)"
    )
    parser.add_argument(
        "--output",
        default="logs/compliance_report.json",
        help="Caminho para salvar relat√≥rio (padr√£o: logs/compliance_report.json)"
    )
    parser.add_argument(
        "--verbose",
        action="store_true",
        help="Modo verbose com logs detalhados"
    )
    
    args = parser.parse_args()
    
    # Configurar logging
    if args.verbose:
        logging.basicConfig(level=logging.DEBUG)
    
    try:
        # Criar validador
        validator = DocComplianceValidator(args.config_path)
        
        # Executar valida√ß√£o
        report = validator.run_validation(args.docs_path)
        
        # Salvar relat√≥rio
        validator.save_report(report, args.output)
        
        # Imprimir resumo
        validator.print_summary(report)
        
        # Retornar c√≥digo de sa√≠da baseado no status
        if report.status == "FAIL":
            sys.exit(1)
        elif report.status == "WARNING":
            sys.exit(2)
        else:
            sys.exit(0)
            
    except Exception as e:
        print(f"‚ùå Erro cr√≠tico: {str(e)}")
        sys.exit(1)


if __name__ == "__main__":
    main() 